{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"F-PCMCI - Filtered PCMCI","text":"<p>Extension of the state-of-the-art causal discovery method PCMCI augmented with a feature-selection method based on Transfer Entropy. The algorithm, starting from a prefixed set of variables, identifies the correct subset of features and possible links between them which describe the observed process. Then, from the selected features and links, a causal model is built.</p>"},{"location":"#useful-links","title":"Useful links","text":"<ul> <li>Documentation</li> <li>Tutorials</li> </ul>"},{"location":"#why-f-pcmci","title":"Why F-PCMCI?","text":"<p>Current state-of-the-art causal discovery approaches suffer in terms of speed and accuracy of the causal analysis when the process to be analysed is composed by a large number of features. F-PCMCI is able to select the most meaningful features from a set of variables and build a causal model from such selection. To this end, the causal analysis results faster and more accurate.</p> <p>In the following it is presented an example showing a comparison between causal models obtained by PCMCI and F-PCMCI causal discovery algorithms on the same data. The latter have been created by defining a 6-variables system defined as follows:</p> <pre><code>min_lag = 1\nmax_lag = 1\nnp.random.seed(1)\nnsample = 1500\nnfeature = 6\n\nd = np.random.random(size = (nsample, feature))\nfor t in range(max_lag, nsample):\n  d[t, 0] += 2 * d[t-1, 1] + 3 * d[t-1, 3]\n  d[t, 2] += 1.1 * d[t-1, 1]**2\n  d[t, 3] += d[t-1, 3] * d[t-1, 2]\n  d[t, 4] += d[t-1, 4] + d[t-1, 5] * d[t-1, 0]\n</code></pre> Causal Model by PCMCI Causal Model by F-PCMCI Execution time ~ 6min 50sec Execution time ~ 2min 45sec <p>The causal analysis performed by the F-PCMCI results not only faster but also more accurate. Indeed, the causal model derived by the F-PCMCI agrees with the structure of the system of equations, instead the one derived by the PCMCI presents spurious links: * $X_2$ \u2192 $X_4$ * $X_2$ \u2192 $X_5$</p> <p>Note that, since all the 6 variables were involved in the evolution of the system, the F-PCMCI did not remove any of them. In the following example instead, we added a new variable in the system which is defined just by the noise component (as $X_1$ and $X_5$) and does not appear in any other equation, defined as follows: $X_6(t) = \\eta_6(t)$. In the following the comparison between PCMCI and F-PCMCI with this new system configuration:</p> Causal Model by PCMCI Causal Model by F-PCMCI Execution time ~ 8min 40sec Execution time ~ 3min 00sec <p>In this case the F-PCMCI removes the $X_6$ variable from the causal graph leading to generate exactly the same causal model as in the previous example, with comparable executional time. Instead, the PCMCI suffers the presence of $X_6$ in terms of time and accuracy of the causal structure. Indeed, a spurious link $X_6$ \u2192 $X_5$ appears in the causal graph derived by the PCMCI.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you found this useful for your work, please cite this papers:</p> <pre><code>@inproceedings{castri2023fpcmci,\n    title={Enhancing Causal Discovery from Robot Sensor Data in Dynamic Scenarios},\n    author={Castri, Luca and Mghames, Sariah and Hanheide, Marc and Bellotto, Nicola},\n    booktitle={Conference on Causal Learning and Reasoning (CLeaR)},\n    year={2023},\n}\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>tigramite&gt;=5.1.0.3</li> <li>pandas&gt;=1.5.2</li> <li>netgraph&gt;=4.10.2</li> <li>networkx&gt;=2.8.6</li> <li>ruptures&gt;=1.1.7</li> <li>scikit_learn&gt;=1.1.3</li> <li>torch&gt;=1.11.0</li> <li>gpytorch&gt;=1.4</li> <li>dcor&gt;=0.5.3</li> <li>h5py&gt;=3.7.0    </li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Before installing the F-PCMCI package, you need to install Java and the IDTxl package used for the feature-selection process, following the guide described here. Once complete, you can install the current release of <code>F-PCMCI</code> with:</p> <pre><code>pip install fpcmci\n</code></pre> <p>For a complete installation Java - IDTxl - F-PCMCI, follow the following procedure.</p>"},{"location":"#1-java-installation","title":"1 - Java installation","text":"<p>Verify that you have not already installed Java:</p> <pre><code>java -version\n</code></pre> <p>if the latter returns <code>Command 'java' not found, ...</code>, you can install Java by the following commands, otherwise you can jump to IDTxl installation.</p> <pre><code># Java\nsudo apt-get update\nsudo apt install default-jdk\n</code></pre> <p>Then, you need to add JAVA_HOME to the environment</p> <pre><code>sudo nano /etc/environment\nJAVA_HOME=\"/lib/jvm/java-11-openjdk-amd64/bin/java\" # Paste the JAVA_HOME assignment at the bottom of the file\nsource /etc/environment\n</code></pre>"},{"location":"#2-idtxl-installation","title":"2 - IDTxl installation","text":"<pre><code># IDTxl\ngit clone https://github.com/pwollstadt/IDTxl.git\nconda create --name fpcmci python=3.8 pip matplotlib h5py scipy networkx\nconda activate fpcmci\nconda install -c conda-forge jpype1    # required by CPU JIDT estimators\nconda install -c conda-forge pyopencl  # required by GPU OpenCL estimators\nconda install -c anaconda ecos         # required by Tartu PID estimator\nconda install numba                    # required by NumbaCuda estimators\nconda install cudatoolkit              # required by NumbaCuda estimators\nconda install mpmath\n\ncd IDTxl\npip install -e .\n</code></pre>"},{"location":"#3-f-pcmci-installation","title":"3 - F-PCMCI installation","text":"<pre><code>pip install fpcmci\n</code></pre>"},{"location":"#recent-changes","title":"Recent changes","text":"Version Changes 4.4.0 neglect autodependent nodes fix 4.3.3 sourcelist method signature fixed in Node.py 4.3.2 documentation improved 4.3.1 README and index.md updated 4.3.0 alpha level fix in PCMCItimeseries_dag fixadaptation to DAG structure 4.2.1 fixed dependency error in setup.py 4.2.0 causal model with only selected features fixadapted to tigramite 5.2get_causal_matrix FPCMCI method addedf_alpha and pcmci_alpha instead of alpharequirements changedtutorials adapted to new version 4.1.2 tutorials adapted to 4.1.1 and get_SCM method added in FPCMCI 4.1.1 PCMCI dependencies fix: FPCMCI causal model field added, FPCMCI.run() and .run_pcmci() outputs the selected variables and the corresponding causal model 4.1.0 FSelector and FValidator turned into FPCMCI and PCMCIshow_edge_label removed and dag optimizednew package included in the setup.pyadded tutorialsnew example in README.md 4.0.1 online documentation and paths fixes 4.0.0 package published"},{"location":"DAG/","title":"DAG","text":"Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>class DAG():\n    def __init__(self, var_names, min_lag, max_lag, neglect_autodep = False, scm = None):\n\"\"\"\n        DAG constructor\n\n        Args:\n            var_names (list): _description_\n            min_lag (int): _description_\n            max_lag (int): _description_\n            neglect_autodep (bool, optional): _description_. Defaults to False.\n            scm (dict, optional): _description_. Defaults to None.\n        \"\"\"\n        self.g = {var: Node(var, neglect_autodep) for var in var_names}\n        self.neglect_autodep = neglect_autodep\n        self.sys_context = dict()\n        self.min_lag = min_lag\n        self.max_lag = max_lag\n\n        if scm is not None:\n            for t in scm:\n                for s in scm[t]: self.add_source(t, s[0], 0.3, 0, s[1])\n\n\n    @property\n    def features(self) -&gt; list:\n\"\"\"\n        Features list\n\n        Returns:\n            list: Features list\n        \"\"\"\n        return list(self.g)\n\n\n    @property\n    def autodep_nodes(self) -&gt; list:\n\"\"\"\n        Autodependent nodes list\n\n        Returns:\n            list: Autodependent nodes list\n        \"\"\"\n        autodeps = list()\n        for t in self.g:\n            # NOTE: I commented this because I want to check all the auto-dep nodes with obs data\n            # if self.g[t].is_autodependent and self.g[t].intervention_node: autodeps.append(t)\n            if self.g[t].is_autodependent: autodeps.append(t)\n        return autodeps\n\n\n    @property\n    def interventions_links(self) -&gt; list:\n\"\"\"\n        Intervention links list\n\n        Returns:\n            list: Intervention link list\n        \"\"\"\n        int_links = list()\n        for t in self.g:\n            for s in self.g[t].sources:\n                if self.g[s[0]].intervention_node:\n                    int_links.append((s[0], s[1], t))\n        return int_links\n\n\n    def fully_connected_dag(self):\n\"\"\"\n        Build a fully connected DAG\n        \"\"\"\n        for t in self.g:\n            for s in self.g:\n                for l in range(1, self.max_lag + 1): self.add_source(t, s, 1, 0, l)\n\n\n    def add_source(self, t, s, score, pval, lag):\n\"\"\"\n        Adds source node to a target node\n\n        Args:\n            t (str): target node name\n            s (str): source node name\n            score (float): dependency score\n            pval (float): dependency p-value\n            lag (int): dependency lag\n        \"\"\"\n        self.g[t].sources[(s, abs(lag))] = {SCORE: score, PVAL: pval}\n        self.g[s].children.append(t)\n\n\n    def del_source(self, t, s, lag):\n\"\"\"\n        Removes source node from a target node\n\n        Args:\n            t (str): target node name\n            s (str): source node name\n            lag (int): dependency lag\n        \"\"\"\n        del self.g[t].sources[(s, lag)]\n        self.g[s].children.remove(t)\n\n\n    def remove_unneeded_features(self):\n\"\"\"\n        Removes isolated nodes\n        \"\"\"\n        tmp = copy.deepcopy(self.g)\n        for t in self.g.keys():\n            if self.g[t].is_isolated: \n                if self.g[t].intervention_node: del tmp[self.g[t].associated_context] # FIXME: last edit to be tested\n                del tmp[t]\n        self.g = tmp\n\n\n    def add_context(self):\n\"\"\"\n        Adds context variables\n        \"\"\"\n        for sys_var, context_var in self.sys_context.items():\n            if sys_var in self.features:\n\n                # Adding context var to the graph\n                self.g[context_var] = Node(context_var, self.neglect_autodep)\n\n                # Adding context var to sys var\n                self.g[sys_var].intervention_node = True\n                self.g[sys_var].associated_context = context_var\n                self.add_source(sys_var, context_var, 1, 0, 1)\n\n\n    def remove_context(self):\n\"\"\"\n        Remove context variables\n        \"\"\"\n        for sys_var, context_var in self.sys_context.items():\n            if sys_var in self.g:\n\n                # Removing context var from sys var\n                # self.g[sys_var].intervention_node = False\n                self.g[sys_var].associated_context = None\n                self.del_source(sys_var, context_var, 1)\n\n                # Removing context var from dag\n                del self.g[context_var]\n\n\n    def get_link_assumptions(self, autodep_ok = False) -&gt; dict:\n\"\"\"\n        Returnes link assumption dictionary\n\n        Args:\n            autodep_ok (bool, optional): If true, autodependecy link assumption = --&gt;. Otherwise -?&gt;. Defaults to False.\n\n        Returns:\n            dict: link assumption dictionary\n        \"\"\"\n        link_assump = {self.features.index(f): dict() for f in self.features}\n        for t in self.g:\n            for s in self.g[t].sources:\n                if autodep_ok and s[0] == t: # NOTE: new condition added in order to not control twice the autodependency links\n                    link_assump[self.features.index(t)][(self.features.index(s[0]), -abs(s[1]))] = '--&gt;'\n\n                elif s[0] not in list(self.sys_context.values()):\n                    link_assump[self.features.index(t)][(self.features.index(s[0]), -abs(s[1]))] = '-?&gt;'\n\n                elif t in self.sys_context.keys() and s[0] == self.sys_context[t]:\n                    link_assump[self.features.index(t)][(self.features.index(s[0]), -abs(s[1]))] = '--&gt;'\n\n        return link_assump\n\n\n    def get_SCM(self) -&gt; dict:   \n\"\"\"\n        Returns SCM\n\n        Returns:\n            dict: SCM\n        \"\"\"\n        scm = {v: list() for v in self.features}\n        for t in self.g:\n            for s in self.g[t].sources:\n                scm[t].append((s[0], -abs(s[1]))) \n        return scm\n\n\n    def get_parents(self) -&gt; dict:\n\"\"\"\n        Returns Parents dict\n\n        Returns:\n            dict: Parents dict\n        \"\"\"\n        scm = {self.features.index(v): list() for v in self.features}\n        for t in self.g:\n            for s in self.g[t].sources:\n                scm[self.features.index(t)].append((self.features.index(s[0]), -abs(s[1]))) \n        return scm\n\n\n    def make_pretty(self) -&gt; dict:\n\"\"\"\n        Makes variables' names pretty, i.e. $ varname $\n\n        Returns:\n            dict: pretty DAG\n        \"\"\"\n        pretty = dict()\n        for t in self.g:\n            p_t = '$' + t + '$'\n            pretty[p_t] = copy.deepcopy(self.g[t])\n            pretty[p_t].name = p_t\n            pretty[p_t].children = ['$' + c + '$' for c in self.g[t].children]\n            for s in self.g[t].sources:\n                del pretty[p_t].sources[s]\n                p_s = '$' + s[0] + '$'\n                pretty[p_t].sources[(p_s, s[1])] = {SCORE: self.g[t].sources[s][SCORE], PVAL: self.g[t].sources[s][PVAL]}\n        return pretty\n\n\n    def dag(self,\n            node_layout = 'dot',\n            min_width = 1, max_width = 5,\n            min_score = 0, max_score = 1,\n            node_size = 8, node_color = 'orange',\n            edge_color = 'grey',\n            bundle_parallel_edges = True,\n            font_size = 12,\n            label_type = LabelType.Lag,\n            save_name = None,\n            img_extention = ImageExt.PNG):\n\"\"\"\n        build a dag\n\n        Args:\n            node_layout (str, optional): Node layout. Defaults to 'dot'.\n            min_width (int, optional): minimum linewidth. Defaults to 1.\n            max_width (int, optional): maximum linewidth. Defaults to 5.\n            min_score (int, optional): minimum score range. Defaults to 0.\n            max_score (int, optional): maximum score range. Defaults to 1.\n            node_size (int, optional): node size. Defaults to 8.\n            node_color (str, optional): node color. Defaults to 'orange'.\n            edge_color (str, optional): edge color. Defaults to 'grey'.\n            bundle_parallel_edges (str, optional): bundle parallel edge bit. Defaults to True.\n            font_size (int, optional): font size. Defaults to 12.\n            label_type (LabelType, optional): enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.\n            save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None.\n        \"\"\"\n        r = copy.deepcopy(self)\n        r.g = r.make_pretty()\n\n        G = nx.DiGraph()\n\n        # NODES DEFINITION\n        G.add_nodes_from(r.g.keys())\n\n        # BORDER LINE\n        border = dict()\n        for t in r.g:\n            border[t] = 0\n            if r.g[t].is_autodependent:\n                autodep = r.g[t].get_max_autodependent\n                border[t] = max(self.__scale(r.g[t].sources[autodep][SCORE], min_width, max_width, min_score, max_score), border[t])\n\n        # BORDER LABEL\n        node_label = None\n        if label_type == LabelType.Lag or label_type == LabelType.Score:\n            node_label = {t: [] for t in r.g.keys()}\n            for t in r.g:\n                if r.g[t].is_autodependent:\n                    for s in r.g[t].sources:\n                        if s[0] == t:\n                            if label_type == LabelType.Lag:\n                                node_label[t].append(s[1])\n                            elif label_type == LabelType.Score:\n                                node_label[t].append(round(r.g[t].sources[s][SCORE], 2))\n                node_label[t] = \",\".join(str(s) for s in node_label[t])\n\n\n        # EDGE DEFINITION\n        edges = [(s[0], t) for t in r.g for s in r.g[t].sources if t != s[0]]\n        G.add_edges_from(edges)\n\n        # EDGE LINE\n        edge_width = {(s[0], t): 0 for t in r.g for s in r.g[t].sources if t != s[0]}\n        for t in r.g:\n            for s in r.g[t].sources:\n                if t != s[0]:\n                    edge_width[(s[0], t)] = max(self.__scale(r.g[t].sources[s][SCORE], min_width, max_width, min_score, max_score), edge_width[(s[0], t)])\n\n        # EDGE LABEL\n        edge_label = None\n        if label_type == LabelType.Lag or label_type == LabelType.Score:\n            edge_label = {(s[0], t): [] for t in r.g for s in r.g[t].sources if t != s[0]}\n            for t in r.g:\n                for s in r.g[t].sources:\n                    if t != s[0]:\n                        if label_type == LabelType.Lag:\n                            edge_label[(s[0], t)].append(s[1])\n                        elif label_type == LabelType.Score:\n                            edge_label[(s[0], t)].append(round(r.g[t].sources[s][SCORE], 3))\n            for k in edge_label.keys():\n                edge_label[k] = \",\".join(str(s) for s in edge_label[k])\n\n        fig, ax = plt.subplots(figsize=(8,6))\n\n        if edges:\n            a = Graph(G, \n                    node_layout = node_layout,\n                    node_size = node_size,\n                    node_color = node_color,\n                    node_labels = node_label,\n                    node_edge_width = border,\n                    node_label_fontdict = dict(size=font_size),\n                    node_edge_color = edge_color,\n                    node_label_offset = 0.1,\n                    node_alpha = 1,\n\n                    arrows = True,\n                    edge_layout = 'curved',\n                    edge_label = label_type != LabelType.NoLabels,\n                    edge_labels = edge_label,\n                    edge_label_fontdict = dict(size=font_size),\n                    edge_color = edge_color, \n                    edge_width = edge_width,\n                    edge_alpha = 1,\n                    edge_zorder = 1,\n                    edge_label_position = 0.35,\n                    edge_layout_kwargs = dict(bundle_parallel_edges = bundle_parallel_edges, k = 0.05))\n\n            nx.draw_networkx_labels(G, \n                                    pos = a.node_positions,\n                                    labels = {n: n for n in G},\n                                    font_size = font_size)\n\n        if save_name is not None:\n            plt.savefig(save_name + img_extention.value, dpi = 300)\n        else:\n            plt.show()\n\n\n    def ts_dag(self,\n               tau,\n               min_width = 1, max_width = 5,\n               min_score = 0, max_score = 1,\n               node_size = 8,\n               node_proximity = 2,\n               node_color = 'orange',\n               edge_color = 'grey',\n               font_size = 12,\n               save_name = None,\n               img_extention = ImageExt.PNG):\n\"\"\"\n        build a timeseries dag\n\n        Args:\n            tau (int): max time lag\n            min_width (int, optional): minimum linewidth. Defaults to 1.\n            max_width (int, optional): maximum linewidth. Defaults to 5.\n            min_score (int, optional): minimum score range. Defaults to 0.\n            max_score (int, optional): maximum score range. Defaults to 1.\n            node_size (int, optional): node size. Defaults to 8.\n            node_proximity (int, optional): node proximity. Defaults to 2.\n            node_color (str, optional): node color. Defaults to 'orange'.\n            edge_color (str, optional): edge color. Defaults to 'grey'.\n            font_size (int, optional): font size. Defaults to 12.\n            save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None.\n        \"\"\"\n\n        r = copy.deepcopy(self)\n        r.g = r.make_pretty()\n\n        # add nodes\n        G = nx.grid_2d_graph(tau + 1, len(r.g.keys()))\n        pos = {n : (n[0], n[1]/node_proximity) for n in G.nodes()}\n        scale = max(pos.values())\n        G.remove_edges_from(G.edges())\n\n        # Nodes color definition\n        # node_c = ['tab:blue', 'tab:orange','tab:red', 'tab:purple']\n        # node_c = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n        # node_color = dict()\n        # tmpG = nx.grid_2d_graph(self.max_lag + 1, len(r.g.keys()))\n        # for n in tmpG.nodes():\n        #     node_color[n] = node_c[abs(n[1] - (len(r.g.keys()) - 1))]\n\n        # edges definition\n        edges = list()\n        edge_width = dict()\n        for t in r.g:\n            for s in r.g[t].sources:\n                s_index = len(r.g.keys())-1 - list(r.g.keys()).index(s[0])\n                t_index = len(r.g.keys())-1 - list(r.g.keys()).index(t)\n\n                s_lag = tau - s[1]\n                t_lag = tau\n                while s_lag &gt;= 0:\n                    s_node = (s_lag, s_index)\n                    t_node = (t_lag, t_index)\n                    edges.append((s_node, t_node))\n                    edge_width[(s_node, t_node)] = self.__scale(r.g[t].sources[s][SCORE], min_width, max_width, min_score, max_score)\n                    s_lag -= s[1]\n                    t_lag -= s[1]\n\n        G.add_edges_from(edges)\n\n        # label definition\n        labeldict = {}\n        for n in G.nodes():\n            if n[0] == 0:\n                labeldict[n] = list(r.g.keys())[len(r.g.keys()) - 1 - n[1]]\n\n        fig, ax = plt.subplots(figsize=(8,6))\n\n        # time line text drawing\n        pos_tau = set([pos[p][0] for p in pos])\n        max_y = max([pos[p][1] for p in pos])\n        for p in pos_tau:\n            if abs(int(p) - tau) == 0:\n                ax.text(p, max_y + .3, r\"$t$\", horizontalalignment='center', fontsize=font_size)\n            else:\n                ax.text(p, max_y + .3, r\"$t-\" + str(abs(int(p) - tau)) + \"$\", horizontalalignment='center', fontsize=font_size)\n\n        Graph(G,\n            node_layout = {p : np.array(pos[p]) for p in pos},\n            node_size = node_size,\n            node_color = node_color,\n            node_labels = labeldict,\n            node_label_offset = 0,\n            node_edge_width = 0,\n            node_label_fontdict = dict(size=font_size),\n            node_alpha = 1,\n\n            arrows = True,\n            edge_layout = 'curved',\n            edge_label = False,\n            edge_color = edge_color, \n            edge_width = edge_width,\n            edge_alpha = 1,\n            edge_zorder = 1,\n            scale = (scale[0] + 2, scale[1] + 2))\n\n        if save_name is not None:\n            plt.savefig(save_name + img_extention.value, dpi = 300)\n        else:\n            plt.show()\n\n\n    def __scale(self, score, min_width, max_width, min_score = 0, max_score = 1):\n\"\"\"\n        Scales the score of the cause-effect relationship strength to a linewitdth\n\n        Args:\n            score (float): score to scale\n            min_width (float): minimum linewidth\n            max_width (float): maximum linewidth\n            min_score (int, optional): minimum score range. Defaults to 0.\n            max_score (int, optional): maximum score range. Defaults to 1.\n\n        Returns:\n            (float): scaled score\n        \"\"\"\n        return ((score - min_score) / (max_score - min_score)) * (max_width - min_width) + min_width\n\n\n    def get_skeleton(self) -&gt; np.array:\n\"\"\"\n        Returns skeleton matrix.\n        Skeleton matrix is composed by 0 and 1.\n        1 &lt;- if there is a link from source to target \n        0 &lt;- if there is not a link from source to target \n\n        Returns:\n            np.array: skeleton matrix\n        \"\"\"\n        r = [np.zeros(shape=(len(self.features), len(self.features)), dtype = np.int32) for _ in range(self.min_lag, self.max_lag + 1)]\n        for l in range(self.min_lag, self.max_lag + 1):\n            for t in self.g.keys():\n                for s in self.g[t].sources:\n                    if s[1] == l: r[l - self.min_lag][self.features.index(t), self.features.index(s[0])] = 1\n        return np.array(r)\n\n\n    def get_val_matrix(self) -&gt; np.array:\n\"\"\"\n        Returns val matrix.\n        val matrix contains information about the strength of the links componing the causal model.\n\n        Returns:\n            np.array: val matrix\n        \"\"\"\n        r = [np.zeros(shape=(len(self.features), len(self.features))) for _ in range(self.min_lag, self.max_lag + 1)]\n        for l in range(self.min_lag, self.max_lag + 1):\n            for t in self.g.keys():\n                for s, info in self.g[t].sources.items():\n                    if s[1] == l: r[l - self.min_lag][self.features.index(t), self.features.index(s[0])] = info['score']\n        return np.array(r)\n\n\n    def get_pval_matrix(self) -&gt; np.array:\n\"\"\"\n        Returns pval matrix.\n        pval matrix contains information about the pval of the links componing the causal model.\n\n        Returns:\n            np.array: pval matrix\n        \"\"\"\n        r = [np.zeros(shape=(len(self.features), len(self.features))) for _ in range(self.min_lag, self.max_lag + 1)]\n        for l in range(self.min_lag, self.max_lag + 1):\n            for t in self.g.keys():\n                for s, info in self.g[t].sources.items():\n                    if s[1] == l: r[l - self.min_lag][self.features.index(t), self.features.index(s[0])] = info['pval']\n        return np.array(r)\n</code></pre> Source code in <code>fpcmci/graph/Node.py</code> <pre><code>class Node():\n\n    def __init__(self, name, neglect_autodep):\n\"\"\"\n        Node class contructer\n\n        Args:\n            name (str): node name\n            neglect_autodep (bool): flag to decide whether to to skip the node if it is only auto-dependent\n        \"\"\"\n        self.name = name\n        self.sources = dict()\n        self.children = list()\n        self.neglect_autodep = neglect_autodep\n        self.intervention_node = False        \n        self.associated_context = None        \n\n\n    @property\n    def is_autodependent(self) -&gt; bool:\n\"\"\"\n        Returns True if the node is autodependent\n\n        Returns:\n            bool: Returns True if the node is autodependent. Otherwise False\n        \"\"\"\n        return self.name in self.sourcelist\n\n\n    @property\n    def is_isolated(self) -&gt; bool:\n\"\"\"\n        Returns True if the node is isolated\n\n        Returns:\n            bool: Returns True if the node is isolated. Otherwise False\n        \"\"\"\n        if self.neglect_autodep:\n            return (self.is_exogenous or self.is_only_autodep or self.is_only_autodep_context) and not self.has_child\n\n        return (self.is_exogenous or self.has_only_context) and not self.has_child\n\n\n    @property\n    def is_only_autodep(self) -&gt; bool:\n\"\"\"\n        Returns True if the node is ONLY auto-dependent\n\n        Returns:\n            bool: Returns True if the node is ONLY auto-dependent. Otherwise False\n        \"\"\"\n        return len(self.sources) == 1 and self.name in self.sourcelist\n\n\n    @property\n    def has_only_context(self) -&gt; bool:\n\"\"\"\n        Returns True if the node has ONLY the context variable as parent\n\n        Returns:\n            bool: Returns True if the node has ONLY the context variable as parent. Otherwise False\n        \"\"\"\n        return len(self.sources) == 1 and self.associated_context in self.sourcelist\n\n\n    @property\n    def is_only_autodep_context(self) -&gt; bool:\n\"\"\"\n        Returns True if the node has ONLY the context variable and itself as parent\n\n        Returns:\n            bool: Returns True if the node has ONLY the context variable and itself as parent. Otherwise False\n        \"\"\"\n        return len(self.sources) == 2 and self.name in self.sourcelist and self.associated_context in self.sourcelist\n\n\n    @property\n    def is_exogenous(self) -&gt; bool:\n\"\"\"\n        Returns True if the node has no parents\n\n        Returns:\n            bool: Returns True if the node has no parents. Otherwise False\n        \"\"\"\n        return len(self.sources) == 0\n\n\n    @property\n    def has_child(self) -&gt; bool:\n\"\"\"\n        Returns True if the node has at least one child\n\n        Returns:\n            bool: Returns True if the node has at least one child. Otherwise False\n        \"\"\"\n        tmp = copy.deepcopy(self.children)\n        if self.name in tmp:\n            tmp.remove(self.name)\n        return len(tmp) &gt; 0\n\n\n    @property\n    def sourcelist(self) -&gt; list:\n\"\"\"\n        Returns list of source names\n\n        Returns:\n            list: Returns list of source names\n        \"\"\"\n        return [s[0] for s in self.sources]\n\n\n    @property\n    def autodependency_links(self) -&gt; list:\n\"\"\"\n        Returns list of autodependency links\n\n        Returns:\n            list: Returns list of autodependency links\n\n        \"\"\"\n        autodep_links = list()\n        if self.is_autodependent:\n            for s in self.sources: \n                if s[0] == self.name: \n                    autodep_links.append(s)\n        return autodep_links\n\n\n    @property\n    def get_max_autodependent(self) -&gt; float:\n\"\"\"\n        Returns max score of autodependent link\n\n        Returns:\n            float: Returns max score of autodependent link\n        \"\"\"\n        max_score = 0\n        max_s = None\n        if self.is_autodependent:\n            for s in self.sources: \n                if s[0] == self.name:\n                    if self.sources[s][SCORE] &gt; max_score: max_s = s\n        return max_s\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.autodep_nodes","title":"<code>autodep_nodes: list</code>  <code>property</code>","text":"<p>Autodependent nodes list</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Autodependent nodes list</p>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.features","title":"<code>features: list</code>  <code>property</code>","text":"<p>Features list</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Features list</p>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.interventions_links","title":"<code>interventions_links: list</code>  <code>property</code>","text":"<p>Intervention links list</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Intervention link list</p>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.__init__","title":"<code>__init__(var_names, min_lag, max_lag, neglect_autodep=False, scm=None)</code>","text":"<p>DAG constructor</p> <p>Parameters:</p> Name Type Description Default <code>var_names</code> <code>list</code> <p>description</p> required <code>min_lag</code> <code>int</code> <p>description</p> required <code>max_lag</code> <code>int</code> <p>description</p> required <code>neglect_autodep</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> <code>scm</code> <code>dict</code> <p>description. Defaults to None.</p> <code>None</code> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def __init__(self, var_names, min_lag, max_lag, neglect_autodep = False, scm = None):\n\"\"\"\n    DAG constructor\n\n    Args:\n        var_names (list): _description_\n        min_lag (int): _description_\n        max_lag (int): _description_\n        neglect_autodep (bool, optional): _description_. Defaults to False.\n        scm (dict, optional): _description_. Defaults to None.\n    \"\"\"\n    self.g = {var: Node(var, neglect_autodep) for var in var_names}\n    self.neglect_autodep = neglect_autodep\n    self.sys_context = dict()\n    self.min_lag = min_lag\n    self.max_lag = max_lag\n\n    if scm is not None:\n        for t in scm:\n            for s in scm[t]: self.add_source(t, s[0], 0.3, 0, s[1])\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.__scale","title":"<code>__scale(score, min_width, max_width, min_score=0, max_score=1)</code>","text":"<p>Scales the score of the cause-effect relationship strength to a linewitdth</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>float</code> <p>score to scale</p> required <code>min_width</code> <code>float</code> <p>minimum linewidth</p> required <code>max_width</code> <code>float</code> <p>maximum linewidth</p> required <code>min_score</code> <code>int</code> <p>minimum score range. Defaults to 0.</p> <code>0</code> <code>max_score</code> <code>int</code> <p>maximum score range. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>float</code> <p>scaled score</p> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def __scale(self, score, min_width, max_width, min_score = 0, max_score = 1):\n\"\"\"\n    Scales the score of the cause-effect relationship strength to a linewitdth\n\n    Args:\n        score (float): score to scale\n        min_width (float): minimum linewidth\n        max_width (float): maximum linewidth\n        min_score (int, optional): minimum score range. Defaults to 0.\n        max_score (int, optional): maximum score range. Defaults to 1.\n\n    Returns:\n        (float): scaled score\n    \"\"\"\n    return ((score - min_score) / (max_score - min_score)) * (max_width - min_width) + min_width\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.add_context","title":"<code>add_context()</code>","text":"<p>Adds context variables</p> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def add_context(self):\n\"\"\"\n    Adds context variables\n    \"\"\"\n    for sys_var, context_var in self.sys_context.items():\n        if sys_var in self.features:\n\n            # Adding context var to the graph\n            self.g[context_var] = Node(context_var, self.neglect_autodep)\n\n            # Adding context var to sys var\n            self.g[sys_var].intervention_node = True\n            self.g[sys_var].associated_context = context_var\n            self.add_source(sys_var, context_var, 1, 0, 1)\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.add_source","title":"<code>add_source(t, s, score, pval, lag)</code>","text":"<p>Adds source node to a target node</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>str</code> <p>target node name</p> required <code>s</code> <code>str</code> <p>source node name</p> required <code>score</code> <code>float</code> <p>dependency score</p> required <code>pval</code> <code>float</code> <p>dependency p-value</p> required <code>lag</code> <code>int</code> <p>dependency lag</p> required Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def add_source(self, t, s, score, pval, lag):\n\"\"\"\n    Adds source node to a target node\n\n    Args:\n        t (str): target node name\n        s (str): source node name\n        score (float): dependency score\n        pval (float): dependency p-value\n        lag (int): dependency lag\n    \"\"\"\n    self.g[t].sources[(s, abs(lag))] = {SCORE: score, PVAL: pval}\n    self.g[s].children.append(t)\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.dag","title":"<code>dag(node_layout='dot', min_width=1, max_width=5, min_score=0, max_score=1, node_size=8, node_color='orange', edge_color='grey', bundle_parallel_edges=True, font_size=12, label_type=LabelType.Lag, save_name=None, img_extention=ImageExt.PNG)</code>","text":"<p>build a dag</p> <p>Parameters:</p> Name Type Description Default <code>node_layout</code> <code>str</code> <p>Node layout. Defaults to 'dot'.</p> <code>'dot'</code> <code>min_width</code> <code>int</code> <p>minimum linewidth. Defaults to 1.</p> <code>1</code> <code>max_width</code> <code>int</code> <p>maximum linewidth. Defaults to 5.</p> <code>5</code> <code>min_score</code> <code>int</code> <p>minimum score range. Defaults to 0.</p> <code>0</code> <code>max_score</code> <code>int</code> <p>maximum score range. Defaults to 1.</p> <code>1</code> <code>node_size</code> <code>int</code> <p>node size. Defaults to 8.</p> <code>8</code> <code>node_color</code> <code>str</code> <p>node color. Defaults to 'orange'.</p> <code>'orange'</code> <code>edge_color</code> <code>str</code> <p>edge color. Defaults to 'grey'.</p> <code>'grey'</code> <code>bundle_parallel_edges</code> <code>str</code> <p>bundle parallel edge bit. Defaults to True.</p> <code>True</code> <code>font_size</code> <code>int</code> <p>font size. Defaults to 12.</p> <code>12</code> <code>label_type</code> <code>LabelType</code> <p>enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.</p> <code>LabelType.Lag</code> <code>save_name</code> <code>str</code> <p>Filename path. If None, plot is shown and not saved. Defaults to None.</p> <code>None</code> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def dag(self,\n        node_layout = 'dot',\n        min_width = 1, max_width = 5,\n        min_score = 0, max_score = 1,\n        node_size = 8, node_color = 'orange',\n        edge_color = 'grey',\n        bundle_parallel_edges = True,\n        font_size = 12,\n        label_type = LabelType.Lag,\n        save_name = None,\n        img_extention = ImageExt.PNG):\n\"\"\"\n    build a dag\n\n    Args:\n        node_layout (str, optional): Node layout. Defaults to 'dot'.\n        min_width (int, optional): minimum linewidth. Defaults to 1.\n        max_width (int, optional): maximum linewidth. Defaults to 5.\n        min_score (int, optional): minimum score range. Defaults to 0.\n        max_score (int, optional): maximum score range. Defaults to 1.\n        node_size (int, optional): node size. Defaults to 8.\n        node_color (str, optional): node color. Defaults to 'orange'.\n        edge_color (str, optional): edge color. Defaults to 'grey'.\n        bundle_parallel_edges (str, optional): bundle parallel edge bit. Defaults to True.\n        font_size (int, optional): font size. Defaults to 12.\n        label_type (LabelType, optional): enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.\n        save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None.\n    \"\"\"\n    r = copy.deepcopy(self)\n    r.g = r.make_pretty()\n\n    G = nx.DiGraph()\n\n    # NODES DEFINITION\n    G.add_nodes_from(r.g.keys())\n\n    # BORDER LINE\n    border = dict()\n    for t in r.g:\n        border[t] = 0\n        if r.g[t].is_autodependent:\n            autodep = r.g[t].get_max_autodependent\n            border[t] = max(self.__scale(r.g[t].sources[autodep][SCORE], min_width, max_width, min_score, max_score), border[t])\n\n    # BORDER LABEL\n    node_label = None\n    if label_type == LabelType.Lag or label_type == LabelType.Score:\n        node_label = {t: [] for t in r.g.keys()}\n        for t in r.g:\n            if r.g[t].is_autodependent:\n                for s in r.g[t].sources:\n                    if s[0] == t:\n                        if label_type == LabelType.Lag:\n                            node_label[t].append(s[1])\n                        elif label_type == LabelType.Score:\n                            node_label[t].append(round(r.g[t].sources[s][SCORE], 2))\n            node_label[t] = \",\".join(str(s) for s in node_label[t])\n\n\n    # EDGE DEFINITION\n    edges = [(s[0], t) for t in r.g for s in r.g[t].sources if t != s[0]]\n    G.add_edges_from(edges)\n\n    # EDGE LINE\n    edge_width = {(s[0], t): 0 for t in r.g for s in r.g[t].sources if t != s[0]}\n    for t in r.g:\n        for s in r.g[t].sources:\n            if t != s[0]:\n                edge_width[(s[0], t)] = max(self.__scale(r.g[t].sources[s][SCORE], min_width, max_width, min_score, max_score), edge_width[(s[0], t)])\n\n    # EDGE LABEL\n    edge_label = None\n    if label_type == LabelType.Lag or label_type == LabelType.Score:\n        edge_label = {(s[0], t): [] for t in r.g for s in r.g[t].sources if t != s[0]}\n        for t in r.g:\n            for s in r.g[t].sources:\n                if t != s[0]:\n                    if label_type == LabelType.Lag:\n                        edge_label[(s[0], t)].append(s[1])\n                    elif label_type == LabelType.Score:\n                        edge_label[(s[0], t)].append(round(r.g[t].sources[s][SCORE], 3))\n        for k in edge_label.keys():\n            edge_label[k] = \",\".join(str(s) for s in edge_label[k])\n\n    fig, ax = plt.subplots(figsize=(8,6))\n\n    if edges:\n        a = Graph(G, \n                node_layout = node_layout,\n                node_size = node_size,\n                node_color = node_color,\n                node_labels = node_label,\n                node_edge_width = border,\n                node_label_fontdict = dict(size=font_size),\n                node_edge_color = edge_color,\n                node_label_offset = 0.1,\n                node_alpha = 1,\n\n                arrows = True,\n                edge_layout = 'curved',\n                edge_label = label_type != LabelType.NoLabels,\n                edge_labels = edge_label,\n                edge_label_fontdict = dict(size=font_size),\n                edge_color = edge_color, \n                edge_width = edge_width,\n                edge_alpha = 1,\n                edge_zorder = 1,\n                edge_label_position = 0.35,\n                edge_layout_kwargs = dict(bundle_parallel_edges = bundle_parallel_edges, k = 0.05))\n\n        nx.draw_networkx_labels(G, \n                                pos = a.node_positions,\n                                labels = {n: n for n in G},\n                                font_size = font_size)\n\n    if save_name is not None:\n        plt.savefig(save_name + img_extention.value, dpi = 300)\n    else:\n        plt.show()\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.del_source","title":"<code>del_source(t, s, lag)</code>","text":"<p>Removes source node from a target node</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>str</code> <p>target node name</p> required <code>s</code> <code>str</code> <p>source node name</p> required <code>lag</code> <code>int</code> <p>dependency lag</p> required Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def del_source(self, t, s, lag):\n\"\"\"\n    Removes source node from a target node\n\n    Args:\n        t (str): target node name\n        s (str): source node name\n        lag (int): dependency lag\n    \"\"\"\n    del self.g[t].sources[(s, lag)]\n    self.g[s].children.remove(t)\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.fully_connected_dag","title":"<code>fully_connected_dag()</code>","text":"<p>Build a fully connected DAG</p> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def fully_connected_dag(self):\n\"\"\"\n    Build a fully connected DAG\n    \"\"\"\n    for t in self.g:\n        for s in self.g:\n            for l in range(1, self.max_lag + 1): self.add_source(t, s, 1, 0, l)\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.get_SCM","title":"<code>get_SCM()</code>","text":"<p>Returns SCM</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>SCM</p> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def get_SCM(self) -&gt; dict:   \n\"\"\"\n    Returns SCM\n\n    Returns:\n        dict: SCM\n    \"\"\"\n    scm = {v: list() for v in self.features}\n    for t in self.g:\n        for s in self.g[t].sources:\n            scm[t].append((s[0], -abs(s[1]))) \n    return scm\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.get_link_assumptions","title":"<code>get_link_assumptions(autodep_ok=False)</code>","text":"<p>Returnes link assumption dictionary</p> <p>Parameters:</p> Name Type Description Default <code>autodep_ok</code> <code>bool</code> <p>If true, autodependecy link assumption = --&gt;. Otherwise -?&gt;. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>link assumption dictionary</p> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def get_link_assumptions(self, autodep_ok = False) -&gt; dict:\n\"\"\"\n    Returnes link assumption dictionary\n\n    Args:\n        autodep_ok (bool, optional): If true, autodependecy link assumption = --&gt;. Otherwise -?&gt;. Defaults to False.\n\n    Returns:\n        dict: link assumption dictionary\n    \"\"\"\n    link_assump = {self.features.index(f): dict() for f in self.features}\n    for t in self.g:\n        for s in self.g[t].sources:\n            if autodep_ok and s[0] == t: # NOTE: new condition added in order to not control twice the autodependency links\n                link_assump[self.features.index(t)][(self.features.index(s[0]), -abs(s[1]))] = '--&gt;'\n\n            elif s[0] not in list(self.sys_context.values()):\n                link_assump[self.features.index(t)][(self.features.index(s[0]), -abs(s[1]))] = '-?&gt;'\n\n            elif t in self.sys_context.keys() and s[0] == self.sys_context[t]:\n                link_assump[self.features.index(t)][(self.features.index(s[0]), -abs(s[1]))] = '--&gt;'\n\n    return link_assump\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.get_parents","title":"<code>get_parents()</code>","text":"<p>Returns Parents dict</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Parents dict</p> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def get_parents(self) -&gt; dict:\n\"\"\"\n    Returns Parents dict\n\n    Returns:\n        dict: Parents dict\n    \"\"\"\n    scm = {self.features.index(v): list() for v in self.features}\n    for t in self.g:\n        for s in self.g[t].sources:\n            scm[self.features.index(t)].append((self.features.index(s[0]), -abs(s[1]))) \n    return scm\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.get_pval_matrix","title":"<code>get_pval_matrix()</code>","text":"<p>Returns pval matrix. pval matrix contains information about the pval of the links componing the causal model.</p> <p>Returns:</p> Type Description <code>np.array</code> <p>np.array: pval matrix</p> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def get_pval_matrix(self) -&gt; np.array:\n\"\"\"\n    Returns pval matrix.\n    pval matrix contains information about the pval of the links componing the causal model.\n\n    Returns:\n        np.array: pval matrix\n    \"\"\"\n    r = [np.zeros(shape=(len(self.features), len(self.features))) for _ in range(self.min_lag, self.max_lag + 1)]\n    for l in range(self.min_lag, self.max_lag + 1):\n        for t in self.g.keys():\n            for s, info in self.g[t].sources.items():\n                if s[1] == l: r[l - self.min_lag][self.features.index(t), self.features.index(s[0])] = info['pval']\n    return np.array(r)\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.get_skeleton","title":"<code>get_skeleton()</code>","text":"<p>Returns skeleton matrix. Skeleton matrix is composed by 0 and 1. 1 &lt;- if there is a link from source to target  0 &lt;- if there is not a link from source to target </p> <p>Returns:</p> Type Description <code>np.array</code> <p>np.array: skeleton matrix</p> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def get_skeleton(self) -&gt; np.array:\n\"\"\"\n    Returns skeleton matrix.\n    Skeleton matrix is composed by 0 and 1.\n    1 &lt;- if there is a link from source to target \n    0 &lt;- if there is not a link from source to target \n\n    Returns:\n        np.array: skeleton matrix\n    \"\"\"\n    r = [np.zeros(shape=(len(self.features), len(self.features)), dtype = np.int32) for _ in range(self.min_lag, self.max_lag + 1)]\n    for l in range(self.min_lag, self.max_lag + 1):\n        for t in self.g.keys():\n            for s in self.g[t].sources:\n                if s[1] == l: r[l - self.min_lag][self.features.index(t), self.features.index(s[0])] = 1\n    return np.array(r)\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.get_val_matrix","title":"<code>get_val_matrix()</code>","text":"<p>Returns val matrix. val matrix contains information about the strength of the links componing the causal model.</p> <p>Returns:</p> Type Description <code>np.array</code> <p>np.array: val matrix</p> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def get_val_matrix(self) -&gt; np.array:\n\"\"\"\n    Returns val matrix.\n    val matrix contains information about the strength of the links componing the causal model.\n\n    Returns:\n        np.array: val matrix\n    \"\"\"\n    r = [np.zeros(shape=(len(self.features), len(self.features))) for _ in range(self.min_lag, self.max_lag + 1)]\n    for l in range(self.min_lag, self.max_lag + 1):\n        for t in self.g.keys():\n            for s, info in self.g[t].sources.items():\n                if s[1] == l: r[l - self.min_lag][self.features.index(t), self.features.index(s[0])] = info['score']\n    return np.array(r)\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.make_pretty","title":"<code>make_pretty()</code>","text":"<p>Makes variables' names pretty, i.e. $ varname $</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>pretty DAG</p> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def make_pretty(self) -&gt; dict:\n\"\"\"\n    Makes variables' names pretty, i.e. $ varname $\n\n    Returns:\n        dict: pretty DAG\n    \"\"\"\n    pretty = dict()\n    for t in self.g:\n        p_t = '$' + t + '$'\n        pretty[p_t] = copy.deepcopy(self.g[t])\n        pretty[p_t].name = p_t\n        pretty[p_t].children = ['$' + c + '$' for c in self.g[t].children]\n        for s in self.g[t].sources:\n            del pretty[p_t].sources[s]\n            p_s = '$' + s[0] + '$'\n            pretty[p_t].sources[(p_s, s[1])] = {SCORE: self.g[t].sources[s][SCORE], PVAL: self.g[t].sources[s][PVAL]}\n    return pretty\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.remove_context","title":"<code>remove_context()</code>","text":"<p>Remove context variables</p> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def remove_context(self):\n\"\"\"\n    Remove context variables\n    \"\"\"\n    for sys_var, context_var in self.sys_context.items():\n        if sys_var in self.g:\n\n            # Removing context var from sys var\n            # self.g[sys_var].intervention_node = False\n            self.g[sys_var].associated_context = None\n            self.del_source(sys_var, context_var, 1)\n\n            # Removing context var from dag\n            del self.g[context_var]\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.remove_unneeded_features","title":"<code>remove_unneeded_features()</code>","text":"<p>Removes isolated nodes</p> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def remove_unneeded_features(self):\n\"\"\"\n    Removes isolated nodes\n    \"\"\"\n    tmp = copy.deepcopy(self.g)\n    for t in self.g.keys():\n        if self.g[t].is_isolated: \n            if self.g[t].intervention_node: del tmp[self.g[t].associated_context] # FIXME: last edit to be tested\n            del tmp[t]\n    self.g = tmp\n</code></pre>"},{"location":"DAG/#fpcmci.graph.DAG.DAG.ts_dag","title":"<code>ts_dag(tau, min_width=1, max_width=5, min_score=0, max_score=1, node_size=8, node_proximity=2, node_color='orange', edge_color='grey', font_size=12, save_name=None, img_extention=ImageExt.PNG)</code>","text":"<p>build a timeseries dag</p> <p>Parameters:</p> Name Type Description Default <code>tau</code> <code>int</code> <p>max time lag</p> required <code>min_width</code> <code>int</code> <p>minimum linewidth. Defaults to 1.</p> <code>1</code> <code>max_width</code> <code>int</code> <p>maximum linewidth. Defaults to 5.</p> <code>5</code> <code>min_score</code> <code>int</code> <p>minimum score range. Defaults to 0.</p> <code>0</code> <code>max_score</code> <code>int</code> <p>maximum score range. Defaults to 1.</p> <code>1</code> <code>node_size</code> <code>int</code> <p>node size. Defaults to 8.</p> <code>8</code> <code>node_proximity</code> <code>int</code> <p>node proximity. Defaults to 2.</p> <code>2</code> <code>node_color</code> <code>str</code> <p>node color. Defaults to 'orange'.</p> <code>'orange'</code> <code>edge_color</code> <code>str</code> <p>edge color. Defaults to 'grey'.</p> <code>'grey'</code> <code>font_size</code> <code>int</code> <p>font size. Defaults to 12.</p> <code>12</code> <code>save_name</code> <code>str</code> <p>Filename path. If None, plot is shown and not saved. Defaults to None.</p> <code>None</code> Source code in <code>fpcmci/graph/DAG.py</code> <pre><code>def ts_dag(self,\n           tau,\n           min_width = 1, max_width = 5,\n           min_score = 0, max_score = 1,\n           node_size = 8,\n           node_proximity = 2,\n           node_color = 'orange',\n           edge_color = 'grey',\n           font_size = 12,\n           save_name = None,\n           img_extention = ImageExt.PNG):\n\"\"\"\n    build a timeseries dag\n\n    Args:\n        tau (int): max time lag\n        min_width (int, optional): minimum linewidth. Defaults to 1.\n        max_width (int, optional): maximum linewidth. Defaults to 5.\n        min_score (int, optional): minimum score range. Defaults to 0.\n        max_score (int, optional): maximum score range. Defaults to 1.\n        node_size (int, optional): node size. Defaults to 8.\n        node_proximity (int, optional): node proximity. Defaults to 2.\n        node_color (str, optional): node color. Defaults to 'orange'.\n        edge_color (str, optional): edge color. Defaults to 'grey'.\n        font_size (int, optional): font size. Defaults to 12.\n        save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None.\n    \"\"\"\n\n    r = copy.deepcopy(self)\n    r.g = r.make_pretty()\n\n    # add nodes\n    G = nx.grid_2d_graph(tau + 1, len(r.g.keys()))\n    pos = {n : (n[0], n[1]/node_proximity) for n in G.nodes()}\n    scale = max(pos.values())\n    G.remove_edges_from(G.edges())\n\n    # Nodes color definition\n    # node_c = ['tab:blue', 'tab:orange','tab:red', 'tab:purple']\n    # node_c = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\n    # node_color = dict()\n    # tmpG = nx.grid_2d_graph(self.max_lag + 1, len(r.g.keys()))\n    # for n in tmpG.nodes():\n    #     node_color[n] = node_c[abs(n[1] - (len(r.g.keys()) - 1))]\n\n    # edges definition\n    edges = list()\n    edge_width = dict()\n    for t in r.g:\n        for s in r.g[t].sources:\n            s_index = len(r.g.keys())-1 - list(r.g.keys()).index(s[0])\n            t_index = len(r.g.keys())-1 - list(r.g.keys()).index(t)\n\n            s_lag = tau - s[1]\n            t_lag = tau\n            while s_lag &gt;= 0:\n                s_node = (s_lag, s_index)\n                t_node = (t_lag, t_index)\n                edges.append((s_node, t_node))\n                edge_width[(s_node, t_node)] = self.__scale(r.g[t].sources[s][SCORE], min_width, max_width, min_score, max_score)\n                s_lag -= s[1]\n                t_lag -= s[1]\n\n    G.add_edges_from(edges)\n\n    # label definition\n    labeldict = {}\n    for n in G.nodes():\n        if n[0] == 0:\n            labeldict[n] = list(r.g.keys())[len(r.g.keys()) - 1 - n[1]]\n\n    fig, ax = plt.subplots(figsize=(8,6))\n\n    # time line text drawing\n    pos_tau = set([pos[p][0] for p in pos])\n    max_y = max([pos[p][1] for p in pos])\n    for p in pos_tau:\n        if abs(int(p) - tau) == 0:\n            ax.text(p, max_y + .3, r\"$t$\", horizontalalignment='center', fontsize=font_size)\n        else:\n            ax.text(p, max_y + .3, r\"$t-\" + str(abs(int(p) - tau)) + \"$\", horizontalalignment='center', fontsize=font_size)\n\n    Graph(G,\n        node_layout = {p : np.array(pos[p]) for p in pos},\n        node_size = node_size,\n        node_color = node_color,\n        node_labels = labeldict,\n        node_label_offset = 0,\n        node_edge_width = 0,\n        node_label_fontdict = dict(size=font_size),\n        node_alpha = 1,\n\n        arrows = True,\n        edge_layout = 'curved',\n        edge_label = False,\n        edge_color = edge_color, \n        edge_width = edge_width,\n        edge_alpha = 1,\n        edge_zorder = 1,\n        scale = (scale[0] + 2, scale[1] + 2))\n\n    if save_name is not None:\n        plt.savefig(save_name + img_extention.value, dpi = 300)\n    else:\n        plt.show()\n</code></pre>"},{"location":"DAG/#fpcmci.graph.Node.Node.autodependency_links","title":"<code>autodependency_links: list</code>  <code>property</code>","text":"<p>Returns list of autodependency links</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Returns list of autodependency links</p>"},{"location":"DAG/#fpcmci.graph.Node.Node.get_max_autodependent","title":"<code>get_max_autodependent: float</code>  <code>property</code>","text":"<p>Returns max score of autodependent link</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Returns max score of autodependent link</p>"},{"location":"DAG/#fpcmci.graph.Node.Node.has_child","title":"<code>has_child: bool</code>  <code>property</code>","text":"<p>Returns True if the node has at least one child</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node has at least one child. Otherwise False</p>"},{"location":"DAG/#fpcmci.graph.Node.Node.has_only_context","title":"<code>has_only_context: bool</code>  <code>property</code>","text":"<p>Returns True if the node has ONLY the context variable as parent</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node has ONLY the context variable as parent. Otherwise False</p>"},{"location":"DAG/#fpcmci.graph.Node.Node.is_autodependent","title":"<code>is_autodependent: bool</code>  <code>property</code>","text":"<p>Returns True if the node is autodependent</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node is autodependent. Otherwise False</p>"},{"location":"DAG/#fpcmci.graph.Node.Node.is_exogenous","title":"<code>is_exogenous: bool</code>  <code>property</code>","text":"<p>Returns True if the node has no parents</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node has no parents. Otherwise False</p>"},{"location":"DAG/#fpcmci.graph.Node.Node.is_isolated","title":"<code>is_isolated: bool</code>  <code>property</code>","text":"<p>Returns True if the node is isolated</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node is isolated. Otherwise False</p>"},{"location":"DAG/#fpcmci.graph.Node.Node.is_only_autodep","title":"<code>is_only_autodep: bool</code>  <code>property</code>","text":"<p>Returns True if the node is ONLY auto-dependent</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node is ONLY auto-dependent. Otherwise False</p>"},{"location":"DAG/#fpcmci.graph.Node.Node.is_only_autodep_context","title":"<code>is_only_autodep_context: bool</code>  <code>property</code>","text":"<p>Returns True if the node has ONLY the context variable and itself as parent</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Returns True if the node has ONLY the context variable and itself as parent. Otherwise False</p>"},{"location":"DAG/#fpcmci.graph.Node.Node.sourcelist","title":"<code>sourcelist: list</code>  <code>property</code>","text":"<p>Returns list of source names</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Returns list of source names</p>"},{"location":"DAG/#fpcmci.graph.Node.Node.__init__","title":"<code>__init__(name, neglect_autodep)</code>","text":"<p>Node class contructer</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>node name</p> required <code>neglect_autodep</code> <code>bool</code> <p>flag to decide whether to to skip the node if it is only auto-dependent</p> required Source code in <code>fpcmci/graph/Node.py</code> <pre><code>def __init__(self, name, neglect_autodep):\n\"\"\"\n    Node class contructer\n\n    Args:\n        name (str): node name\n        neglect_autodep (bool): flag to decide whether to to skip the node if it is only auto-dependent\n    \"\"\"\n    self.name = name\n    self.sources = dict()\n    self.children = list()\n    self.neglect_autodep = neglect_autodep\n    self.intervention_node = False        \n    self.associated_context = None        \n</code></pre>"},{"location":"feature_selection_method/","title":"Feature Selection Methods","text":""},{"location":"feature_selection_method/#fpcmci.selection_methods.SelectionMethod.SelectionMethod","title":"<code>SelectionMethod</code>","text":"<p>             Bases: <code>ABC</code></p> <p>SelectionMethod abstract class</p> Source code in <code>fpcmci/selection_methods/SelectionMethod.py</code> <pre><code>class SelectionMethod(ABC):\n\"\"\"\n    SelectionMethod abstract class\n    \"\"\"\n    def __init__(self, ctest):\n        self.ctest = ctest\n        self.data = None\n        self.alpha = None\n        self.min_lag = None\n        self.max_lag = None\n        self.result = None\n\n\n    @property\n    def name(self):\n\"\"\"\n        Returns Selection Method name\n\n        Returns:\n            (str): Selection Method name\n        \"\"\"\n        return self.ctest.value\n\n\n    # def initialise(self, data: Data, alpha, min_lag, max_lag):\n    def initialise(self, data: Data, alpha, min_lag, max_lag, graph):\n\"\"\"\n        Initialises the selection method\n\n        Args:\n            data (Data): Data\n            alpha (float): significance threshold\n            min_lag (int): min lag time\n            max_lag (int): max lag time\n        \"\"\"\n        self.data = data\n        self.alpha = alpha\n        self.min_lag = min_lag\n        self.max_lag = max_lag\n        self.result = graph\n\n\n    @abstractmethod\n    def compute_dependencies(self) -&gt; DAG:\n\"\"\"\n        abstract method\n        \"\"\"\n        pass\n\n\n    def _prepare_ts(self, target, lag, apply_lag = True, consider_autodep = True):\n\"\"\"\n        prepare the dataframe to the analysis\n\n        Args:\n            target (str): name target var\n            lag (int): lag time to apply\n            apply_lag (bool, optional): True if you want to apply the lag, False otherwise. Defaults to True.\n\n        Returns:\n            tuple(DataFrame, DataFrame): source and target dataframe\n        \"\"\"\n        if not consider_autodep:\n            if apply_lag:\n                Y = self.data.d[target][lag:]\n                X = self.data.d.loc[:, self.data.d.columns != target][:-lag]\n            else:\n                Y = self.data.d[target]\n                X = self.data.d.loc[:, self.data.d.columns != target]\n        else:\n            if apply_lag:\n                Y = self.data.d[target][lag:]\n                X = self.data.d[:-lag]\n            else:\n                Y = self.data.d[target]\n                X = self.data.d\n        return X, Y\n\n\n    def _add_dependecy(self, t, s, score, pval, lag):\n\"\"\"\n        Adds found dependency from source (s) to target (t) specifying the \n        score, pval and the lag\n\n        Args:\n            t (str): target feature name\n            s (str): source feature name\n            score (float): selection method score\n            pval (float): pval associated to the dependency\n            lag (int): lag time of the dependency\n        \"\"\"\n        self.result.add_source(t, s, score, pval, lag)\n\n        str_s = \"(\" + s + \" -\" + str(lag) + \")\"\n        str_t = \"(\" + t + \")\"\n\n        CP.info(\"\\tlink: \" + str_s + \" -?&gt; \" + str_t)\n        CP.info(\"\\t|val = \" + str(round(score,3)) + \" |pval = \" + str(str(round(pval,3))))\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.SelectionMethod.SelectionMethod.name","title":"<code>name</code>  <code>property</code>","text":"<p>Returns Selection Method name</p> <p>Returns:</p> Type Description <code>str</code> <p>Selection Method name</p>"},{"location":"feature_selection_method/#fpcmci.selection_methods.SelectionMethod.SelectionMethod.compute_dependencies","title":"<code>compute_dependencies()</code>  <code>abstractmethod</code>","text":"<p>abstract method</p> Source code in <code>fpcmci/selection_methods/SelectionMethod.py</code> <pre><code>@abstractmethod\ndef compute_dependencies(self) -&gt; DAG:\n\"\"\"\n    abstract method\n    \"\"\"\n    pass\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.SelectionMethod.SelectionMethod.initialise","title":"<code>initialise(data, alpha, min_lag, max_lag, graph)</code>","text":"<p>Initialises the selection method</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>Data</p> required <code>alpha</code> <code>float</code> <p>significance threshold</p> required <code>min_lag</code> <code>int</code> <p>min lag time</p> required <code>max_lag</code> <code>int</code> <p>max lag time</p> required Source code in <code>fpcmci/selection_methods/SelectionMethod.py</code> <pre><code>def initialise(self, data: Data, alpha, min_lag, max_lag, graph):\n\"\"\"\n    Initialises the selection method\n\n    Args:\n        data (Data): Data\n        alpha (float): significance threshold\n        min_lag (int): min lag time\n        max_lag (int): max lag time\n    \"\"\"\n    self.data = data\n    self.alpha = alpha\n    self.min_lag = min_lag\n    self.max_lag = max_lag\n    self.result = graph\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.Corr.Corr","title":"<code>Corr</code>","text":"<p>             Bases: <code>SelectionMethod</code></p> <p>Feature selection method based on Correlation analysis</p> Source code in <code>fpcmci/selection_methods/Corr.py</code> <pre><code>class Corr(SelectionMethod):\n\"\"\"\n    Feature selection method based on Correlation analysis\n    \"\"\"\n    def __init__(self):\n\"\"\"\n        Corr contructor class\n        \"\"\"\n        super().__init__(CTest.Corr)\n\n\n    def compute_dependencies(self):\n\"\"\"\n        compute list of dependencies for each target by correlation analysis\n\n        Returns:\n            (dict): dictonary(TARGET: list SOURCES)\n        \"\"\"\n        CP.info(\"\\n##\")\n        CP.info(\"## \" + self.name + \" analysis\")\n        CP.info(\"##\")\n\n        for lag in range(self.min_lag, self.max_lag + 1):\n            for target in self.data.features:\n                CP.info(\"\\n## Target variable: \" + target)\n\n                X, Y = self._prepare_ts(target, lag)\n                scores, pval = f_regression(X, Y)\n\n                # Filter on pvalue\n                f = pval &lt; self.alpha\n\n                # Result of the selection\n                sel_sources, sel_sources_score, sel_sources_pval = X.columns[f].tolist(), scores[f].tolist(), pval[f].tolist()\n\n                for s, score, pval in zip(sel_sources, sel_sources_score, sel_sources_pval):\n                    self._add_dependecy(target, s, score, pval, lag)\n\n        return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.Corr.Corr.__init__","title":"<code>__init__()</code>","text":"<p>Corr contructor class</p> Source code in <code>fpcmci/selection_methods/Corr.py</code> <pre><code>def __init__(self):\n\"\"\"\n    Corr contructor class\n    \"\"\"\n    super().__init__(CTest.Corr)\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.Corr.Corr.compute_dependencies","title":"<code>compute_dependencies()</code>","text":"<p>compute list of dependencies for each target by correlation analysis</p> <p>Returns:</p> Type Description <code>dict</code> <p>dictonary(TARGET: list SOURCES)</p> Source code in <code>fpcmci/selection_methods/Corr.py</code> <pre><code>def compute_dependencies(self):\n\"\"\"\n    compute list of dependencies for each target by correlation analysis\n\n    Returns:\n        (dict): dictonary(TARGET: list SOURCES)\n    \"\"\"\n    CP.info(\"\\n##\")\n    CP.info(\"## \" + self.name + \" analysis\")\n    CP.info(\"##\")\n\n    for lag in range(self.min_lag, self.max_lag + 1):\n        for target in self.data.features:\n            CP.info(\"\\n## Target variable: \" + target)\n\n            X, Y = self._prepare_ts(target, lag)\n            scores, pval = f_regression(X, Y)\n\n            # Filter on pvalue\n            f = pval &lt; self.alpha\n\n            # Result of the selection\n            sel_sources, sel_sources_score, sel_sources_pval = X.columns[f].tolist(), scores[f].tolist(), pval[f].tolist()\n\n            for s, score, pval in zip(sel_sources, sel_sources_score, sel_sources_pval):\n                self._add_dependecy(target, s, score, pval, lag)\n\n    return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr","title":"<code>ParCorr</code>","text":"<p>             Bases: <code>SelectionMethod</code></p> <p>Feature selection method based on Partial Correlation analysis</p> Source code in <code>fpcmci/selection_methods/ParCorr.py</code> <pre><code>class ParCorr(SelectionMethod):\n\"\"\"\n    Feature selection method based on Partial Correlation analysis\n    \"\"\"\n    def __init__(self):\n\"\"\"\n        ParCorr class contructor\n        \"\"\"\n        super().__init__(CTest.Corr)\n\n\n    def get_residual(self, covar, target):\n\"\"\"\n        Calculate residual of the target variable obtaining conditioning on the covar variables\n\n        Args:\n            covar (np.array): conditioning variables\n            target (np.array): target variable\n\n        Returns:\n            (np.array): residual\n        \"\"\"\n        beta = np.linalg.lstsq(covar, target, rcond=None)[0]\n        return target - np.dot(covar, beta)\n\n\n    def partial_corr(self, X, Y, Z):\n\"\"\"\n        Calculate Partial correlation between X and Y conditioning on Z\n\n        Args:\n            X (np.array): source candidate variable\n            Y (np.array): target variable\n            Z (np.array): conditioning variable\n\n        Returns:\n            (float, float): partial correlation, p-value\n        \"\"\"\n\n        pcorr, pval = stats.pearsonr(self.get_residual(Z, X), self.get_residual(Z, Y))\n\n        return pcorr, pval\n\n    def compute_dependencies(self):\n\"\"\"\n        compute list of dependencies for each target by partial correlation analysis\n\n        Returns:\n            (dict): dictonary(TARGET: list SOURCES)\n        \"\"\"\n        CP.info(\"\\n##\")\n        CP.info(\"## \" + self.name + \" analysis\")\n        CP.info(\"##\")\n\n        for lag in range(self.min_lag, self.max_lag + 1):\n            for target in self.data.features:\n                CP.info(\"\\n## Target variable: \" + target)\n                candidates = self.data.features\n\n                Y = np.array(self.data.d[target][lag:])\n\n                while candidates:\n                    tmp_res = None\n                    covars = self._get_sources(target)\n                    Z = np.array(self.data.d[covars][:-lag])\n\n                    for candidate in candidates:\n                        X = np.array(self.data.d[candidate][:-lag])\n                        score, pval = self.partial_corr(X, Y, Z)\n                        if pval &lt; self.alpha and (tmp_res is None or abs(tmp_res[1]) &lt; abs(score)):\n                            tmp_res = (candidate, score, pval)\n\n                    if tmp_res is not None: \n                        self._add_dependecy(target, tmp_res[0], tmp_res[1], tmp_res[2], lag)\n                        candidates.remove(tmp_res[0])\n                    else:\n                        break\n        return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr.__init__","title":"<code>__init__()</code>","text":"<p>ParCorr class contructor</p> Source code in <code>fpcmci/selection_methods/ParCorr.py</code> <pre><code>def __init__(self):\n\"\"\"\n    ParCorr class contructor\n    \"\"\"\n    super().__init__(CTest.Corr)\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr.compute_dependencies","title":"<code>compute_dependencies()</code>","text":"<p>compute list of dependencies for each target by partial correlation analysis</p> <p>Returns:</p> Type Description <code>dict</code> <p>dictonary(TARGET: list SOURCES)</p> Source code in <code>fpcmci/selection_methods/ParCorr.py</code> <pre><code>def compute_dependencies(self):\n\"\"\"\n    compute list of dependencies for each target by partial correlation analysis\n\n    Returns:\n        (dict): dictonary(TARGET: list SOURCES)\n    \"\"\"\n    CP.info(\"\\n##\")\n    CP.info(\"## \" + self.name + \" analysis\")\n    CP.info(\"##\")\n\n    for lag in range(self.min_lag, self.max_lag + 1):\n        for target in self.data.features:\n            CP.info(\"\\n## Target variable: \" + target)\n            candidates = self.data.features\n\n            Y = np.array(self.data.d[target][lag:])\n\n            while candidates:\n                tmp_res = None\n                covars = self._get_sources(target)\n                Z = np.array(self.data.d[covars][:-lag])\n\n                for candidate in candidates:\n                    X = np.array(self.data.d[candidate][:-lag])\n                    score, pval = self.partial_corr(X, Y, Z)\n                    if pval &lt; self.alpha and (tmp_res is None or abs(tmp_res[1]) &lt; abs(score)):\n                        tmp_res = (candidate, score, pval)\n\n                if tmp_res is not None: \n                    self._add_dependecy(target, tmp_res[0], tmp_res[1], tmp_res[2], lag)\n                    candidates.remove(tmp_res[0])\n                else:\n                    break\n    return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr.get_residual","title":"<code>get_residual(covar, target)</code>","text":"<p>Calculate residual of the target variable obtaining conditioning on the covar variables</p> <p>Parameters:</p> Name Type Description Default <code>covar</code> <code>np.array</code> <p>conditioning variables</p> required <code>target</code> <code>np.array</code> <p>target variable</p> required <p>Returns:</p> Type Description <code>np.array</code> <p>residual</p> Source code in <code>fpcmci/selection_methods/ParCorr.py</code> <pre><code>def get_residual(self, covar, target):\n\"\"\"\n    Calculate residual of the target variable obtaining conditioning on the covar variables\n\n    Args:\n        covar (np.array): conditioning variables\n        target (np.array): target variable\n\n    Returns:\n        (np.array): residual\n    \"\"\"\n    beta = np.linalg.lstsq(covar, target, rcond=None)[0]\n    return target - np.dot(covar, beta)\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr.partial_corr","title":"<code>partial_corr(X, Y, Z)</code>","text":"<p>Calculate Partial correlation between X and Y conditioning on Z</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.array</code> <p>source candidate variable</p> required <code>Y</code> <code>np.array</code> <p>target variable</p> required <code>Z</code> <code>np.array</code> <p>conditioning variable</p> required <p>Returns:</p> Type Description <code>(float, float)</code> <p>partial correlation, p-value</p> Source code in <code>fpcmci/selection_methods/ParCorr.py</code> <pre><code>def partial_corr(self, X, Y, Z):\n\"\"\"\n    Calculate Partial correlation between X and Y conditioning on Z\n\n    Args:\n        X (np.array): source candidate variable\n        Y (np.array): target variable\n        Z (np.array): conditioning variable\n\n    Returns:\n        (float, float): partial correlation, p-value\n    \"\"\"\n\n    pcorr, pval = stats.pearsonr(self.get_residual(Z, X), self.get_residual(Z, Y))\n\n    return pcorr, pval\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.MI.MI","title":"<code>MI</code>","text":"<p>             Bases: <code>SelectionMethod</code></p> <p>Feature selection method based on Mutual Information analysis</p> Source code in <code>fpcmci/selection_methods/MI.py</code> <pre><code>class MI(SelectionMethod):\n\"\"\"\n    Feature selection method based on Mutual Information analysis\n    \"\"\"\n    def __init__(self, estimator: MIestimator):\n\"\"\"\n        MI class contructor\n\n        Args:\n            estimator (MIestimator): Gaussian/Kraskov\n        \"\"\"\n        super().__init__(CTest.MI)\n        self.estimator = estimator\n\n    def compute_dependencies(self):\n\"\"\"\n        compute list of dependencies for each target by mutual information analysis\n\n        Returns:\n            (dict): dictonary(TARGET: list SOURCES)\n        \"\"\"\n        with _suppress_stdout():\n            data = Data(self.d.values, dim_order='sp') # sp = samples(row) x processes(col)\n\n            network_analysis = MultivariateMI()\n            settings = {'cmi_estimator': self.estimator.value,\n                        'max_lag_sources': self.max_lag,\n                        'min_lag_sources': self.min_lag,\n                        'alpha_max_stats': self.alpha,\n                        'alpha_min_stats': self.alpha,\n                        'alpha_omnibus': self.alpha,\n                        'alpha_max_seq': self.alpha,\n                        'verbose': False}\n            results = network_analysis.analyse_network(settings=settings, data=data)\n\n        for t in results._single_target.keys():\n            sel_sources = [s[0] for s in results._single_target[t]['selected_vars_sources']]\n            if sel_sources:\n                sel_sources_lag = [s[1] for s in results._single_target[t]['selected_vars_sources']]\n                sel_sources_score = results._single_target[t]['selected_sources_mi']\n                sel_sources_pval = results._single_target[t]['selected_sources_pval']\n                for s, score, pval, lag in zip(sel_sources, sel_sources_score, sel_sources_pval, sel_sources_lag):\n                    self._add_dependecy(self.features[t], self.features[s], score, pval, lag)\n\n        return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.MI.MI.__init__","title":"<code>__init__(estimator)</code>","text":"<p>MI class contructor</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>MIestimator</code> <p>Gaussian/Kraskov</p> required Source code in <code>fpcmci/selection_methods/MI.py</code> <pre><code>def __init__(self, estimator: MIestimator):\n\"\"\"\n    MI class contructor\n\n    Args:\n        estimator (MIestimator): Gaussian/Kraskov\n    \"\"\"\n    super().__init__(CTest.MI)\n    self.estimator = estimator\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.MI.MI.compute_dependencies","title":"<code>compute_dependencies()</code>","text":"<p>compute list of dependencies for each target by mutual information analysis</p> <p>Returns:</p> Type Description <code>dict</code> <p>dictonary(TARGET: list SOURCES)</p> Source code in <code>fpcmci/selection_methods/MI.py</code> <pre><code>def compute_dependencies(self):\n\"\"\"\n    compute list of dependencies for each target by mutual information analysis\n\n    Returns:\n        (dict): dictonary(TARGET: list SOURCES)\n    \"\"\"\n    with _suppress_stdout():\n        data = Data(self.d.values, dim_order='sp') # sp = samples(row) x processes(col)\n\n        network_analysis = MultivariateMI()\n        settings = {'cmi_estimator': self.estimator.value,\n                    'max_lag_sources': self.max_lag,\n                    'min_lag_sources': self.min_lag,\n                    'alpha_max_stats': self.alpha,\n                    'alpha_min_stats': self.alpha,\n                    'alpha_omnibus': self.alpha,\n                    'alpha_max_seq': self.alpha,\n                    'verbose': False}\n        results = network_analysis.analyse_network(settings=settings, data=data)\n\n    for t in results._single_target.keys():\n        sel_sources = [s[0] for s in results._single_target[t]['selected_vars_sources']]\n        if sel_sources:\n            sel_sources_lag = [s[1] for s in results._single_target[t]['selected_vars_sources']]\n            sel_sources_score = results._single_target[t]['selected_sources_mi']\n            sel_sources_pval = results._single_target[t]['selected_sources_pval']\n            for s, score, pval, lag in zip(sel_sources, sel_sources_score, sel_sources_pval, sel_sources_lag):\n                self._add_dependecy(self.features[t], self.features[s], score, pval, lag)\n\n    return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.TE.TE","title":"<code>TE</code>","text":"<p>             Bases: <code>SelectionMethod</code></p> <p>Feature selection method based on Trasfer Entropy analysis</p> Source code in <code>fpcmci/selection_methods/TE.py</code> <pre><code>class TE(SelectionMethod):\n\"\"\"\n    Feature selection method based on Trasfer Entropy analysis\n    \"\"\"\n    def __init__(self, estimator: TEestimator):\n\"\"\"\n        TE class contructor\n\n        Args:\n            estimator (TEestimator): Gaussian/Kraskov\n        \"\"\"\n        super().__init__(CTest.TE)\n        self.estimator = estimator\n\n\n    def compute_dependencies(self):\n\"\"\"\n        compute list of dependencies for each target by transfer entropy analysis\n\n        Returns:\n            (dict): dictonary(TARGET: list SOURCES)\n        \"\"\"\n        multi_network_analysis = MultivariateTE()\n        bi_network_analysis = BivariateMI()\n        settings = {'cmi_estimator': self.estimator.value,\n                    'max_lag_sources': self.max_lag,\n                    'min_lag_sources': self.min_lag,\n                    'max_lag_target': self.max_lag,\n                    'min_lag_target': self.min_lag,\n                    'alpha_max_stats': self.alpha,\n                    'alpha_min_stats': self.alpha,\n                    'alpha_omnibus': self.alpha,\n                    'alpha_max_seq': self.alpha,\n                    'verbose': False}\n\n        CP.info(\"\\n##\")\n        CP.info(\"## \" + self.name + \" analysis\")\n        CP.info(\"##\")\n        for target in self.data.features:\n            CP.info(\"\\n## Target variable: \" + target)\n            with _suppress_stdout():\n                t = self.data.features.index(target)\n\n                # Check auto-dependency\n                tmp_d = np.c_[self.data.d.values[:, t], self.data.d.values[:, t]]\n                data = Data(tmp_d, dim_order='sp') # sp = samples(row) x processes(col)\n                res_auto = bi_network_analysis.analyse_single_target(settings = settings, data = data, target = 0, sources = 1)\n\n                # Check cross-dependencies\n                data = Data(self.data.d.values, dim_order='sp') # sp = samples(row) x processes(col)\n                res_cross = multi_network_analysis.analyse_single_target(settings = settings, data = data, target = t)\n\n            # Auto-dependency handling\n            auto_lag = [s[1] for s in res_auto._single_target[0]['selected_vars_sources']]\n            auto_score = res_auto._single_target[0]['selected_sources_mi']\n            auto_pval = res_auto._single_target[0]['selected_sources_pval']\n            if auto_score is not None:\n                for score, pval, lag in zip(auto_score, auto_pval, auto_lag):\n                    self._add_dependecy(self.data.features[t], self.data.features[t], score, pval, lag)\n\n            # Cross-dependencies handling    \n            sel_sources = [s[0] for s in res_cross._single_target[t]['selected_vars_sources']]\n            if sel_sources:\n                sel_sources_lag = [s[1] for s in res_cross._single_target[t]['selected_vars_sources']]\n                sel_sources_score = res_cross._single_target[t]['selected_sources_te']\n                sel_sources_pval = res_cross._single_target[t]['selected_sources_pval']\n                for s, score, pval, lag in zip(sel_sources, sel_sources_score, sel_sources_pval, sel_sources_lag):\n                    self._add_dependecy(self.data.features[t], self.data.features[s], score, pval, lag)\n\n            if auto_score is None and not sel_sources:\n                CP.info(\"\\tno sources selected\")\n\n        return self.result\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.TE.TE.__init__","title":"<code>__init__(estimator)</code>","text":"<p>TE class contructor</p> <p>Parameters:</p> Name Type Description Default <code>estimator</code> <code>TEestimator</code> <p>Gaussian/Kraskov</p> required Source code in <code>fpcmci/selection_methods/TE.py</code> <pre><code>def __init__(self, estimator: TEestimator):\n\"\"\"\n    TE class contructor\n\n    Args:\n        estimator (TEestimator): Gaussian/Kraskov\n    \"\"\"\n    super().__init__(CTest.TE)\n    self.estimator = estimator\n</code></pre>"},{"location":"feature_selection_method/#fpcmci.selection_methods.TE.TE.compute_dependencies","title":"<code>compute_dependencies()</code>","text":"<p>compute list of dependencies for each target by transfer entropy analysis</p> <p>Returns:</p> Type Description <code>dict</code> <p>dictonary(TARGET: list SOURCES)</p> Source code in <code>fpcmci/selection_methods/TE.py</code> <pre><code>def compute_dependencies(self):\n\"\"\"\n    compute list of dependencies for each target by transfer entropy analysis\n\n    Returns:\n        (dict): dictonary(TARGET: list SOURCES)\n    \"\"\"\n    multi_network_analysis = MultivariateTE()\n    bi_network_analysis = BivariateMI()\n    settings = {'cmi_estimator': self.estimator.value,\n                'max_lag_sources': self.max_lag,\n                'min_lag_sources': self.min_lag,\n                'max_lag_target': self.max_lag,\n                'min_lag_target': self.min_lag,\n                'alpha_max_stats': self.alpha,\n                'alpha_min_stats': self.alpha,\n                'alpha_omnibus': self.alpha,\n                'alpha_max_seq': self.alpha,\n                'verbose': False}\n\n    CP.info(\"\\n##\")\n    CP.info(\"## \" + self.name + \" analysis\")\n    CP.info(\"##\")\n    for target in self.data.features:\n        CP.info(\"\\n## Target variable: \" + target)\n        with _suppress_stdout():\n            t = self.data.features.index(target)\n\n            # Check auto-dependency\n            tmp_d = np.c_[self.data.d.values[:, t], self.data.d.values[:, t]]\n            data = Data(tmp_d, dim_order='sp') # sp = samples(row) x processes(col)\n            res_auto = bi_network_analysis.analyse_single_target(settings = settings, data = data, target = 0, sources = 1)\n\n            # Check cross-dependencies\n            data = Data(self.data.d.values, dim_order='sp') # sp = samples(row) x processes(col)\n            res_cross = multi_network_analysis.analyse_single_target(settings = settings, data = data, target = t)\n\n        # Auto-dependency handling\n        auto_lag = [s[1] for s in res_auto._single_target[0]['selected_vars_sources']]\n        auto_score = res_auto._single_target[0]['selected_sources_mi']\n        auto_pval = res_auto._single_target[0]['selected_sources_pval']\n        if auto_score is not None:\n            for score, pval, lag in zip(auto_score, auto_pval, auto_lag):\n                self._add_dependecy(self.data.features[t], self.data.features[t], score, pval, lag)\n\n        # Cross-dependencies handling    \n        sel_sources = [s[0] for s in res_cross._single_target[t]['selected_vars_sources']]\n        if sel_sources:\n            sel_sources_lag = [s[1] for s in res_cross._single_target[t]['selected_vars_sources']]\n            sel_sources_score = res_cross._single_target[t]['selected_sources_te']\n            sel_sources_pval = res_cross._single_target[t]['selected_sources_pval']\n            for s, score, pval, lag in zip(sel_sources, sel_sources_score, sel_sources_pval, sel_sources_lag):\n                self._add_dependecy(self.data.features[t], self.data.features[s], score, pval, lag)\n\n        if auto_score is None and not sel_sources:\n            CP.info(\"\\tno sources selected\")\n\n    return self.result\n</code></pre>"},{"location":"fpcmci/","title":"FPCMCI","text":""},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI","title":"<code>FPCMCI</code>","text":"<p>FPCMCI class.</p> <p>FPCMCI is a causal feature selector framework for large-scale time series datasets. Sarting from a Data object and it selects the main features responsible for the evolution of the analysed system. Based on the selected features, the framework outputs a causal model.</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>class FPCMCI():\n\"\"\"\n    FPCMCI class.\n\n    FPCMCI is a causal feature selector framework for large-scale time series\n    datasets. Sarting from a Data object and it selects the main features\n    responsible for the evolution of the analysed system. Based on the selected features,\n    the framework outputs a causal model.\n    \"\"\"\n\n    def __init__(self, \n                 data: Data, \n                 min_lag, max_lag, \n                 sel_method: SelectionMethod, val_condtest: CondIndTest, \n                 verbosity: CPLevel, \n                 clean_cls = True, \n                 f_alpha = 0.05, \n                 pcmci_alpha = 0.05, \n                 resfolder = None,\n                 neglect_only_autodep = False):\n\"\"\"\n        FPCMCI class contructor\n\n        Args:\n            data (Data): data to analyse\n            min_lag (int): minimum time lag\n            max_lag (int): maximum time lag\n            sel_method (SelectionMethod): selection method\n            val_condtest (CondIndTest): validation method\n            verbosity (CPLevel): verbosity level\n            clean_cls (bool): Clean console bit. Default to True.\n            f_alpha (float, optional): filter significance level. Defaults to 0.05.\n            pcmci_alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n            resfolder (string, optional): result folder to create. Defaults to None.\n            neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n        \"\"\"\n\n        self.data = data\n        self.f_alpha = f_alpha\n        self.pcmci_alpha = pcmci_alpha\n        self.min_lag = min_lag\n        self.max_lag = max_lag\n        self.sel_method = sel_method\n        self.CM = DAG(self.data.features, min_lag, max_lag, neglect_only_autodep)\n        self.neglect_only_autodep = neglect_only_autodep\n\n        self.respath, self.dag_path, self.ts_dag_path = None, None, None\n        if resfolder is not None:\n            logpath, self.respath, self.dag_path, self.ts_dag_path = utils.get_selectorpath(resfolder)  \n            sys.stdout = Logger(logpath, clean_cls)\n\n        self.validator = PCMCI(self.pcmci_alpha, min_lag, max_lag, neglect_only_autodep, val_condtest, verbosity)       \n        CP.set_verbosity(verbosity)\n\n\n    @ignore_warnings(category=ConvergenceWarning)\n    def run_filter(self):\n\"\"\"\n        Run filter method\n        \"\"\"\n        CP.info(\"\\n\")\n        CP.info(DASH)\n        CP.info(\"Selecting relevant features among: \" + str(self.data.features))\n        CP.info(\"Selection method: \" + self.sel_method.name)\n        CP.info(\"Significance level: \" + str(self.f_alpha))\n        CP.info(\"Max lag time: \" + str(self.max_lag))\n        CP.info(\"Min lag time: \" + str(self.min_lag))\n        CP.info(\"Data length: \" + str(self.data.T))\n\n        self.sel_method.initialise(self.data, self.f_alpha, self.min_lag, self.max_lag, self.CM)\n        self.CM = self.sel_method.compute_dependencies()  \n\n\n    @ignore_warnings(category=ConvergenceWarning)\n    def run_pcmci(self):\n\"\"\"\n        Run PCMCI\n\n        Returns:\n            list(str): list of selected variable names\n            dict(str:list(tuple)): causal model\n        \"\"\"\n        CP.info(\"Significance level: \" + str(self.pcmci_alpha))\n        CP.info(\"Max lag time: \" + str(self.max_lag))\n        CP.info(\"Min lag time: \" + str(self.min_lag))\n        CP.info(\"Data length: \" + str(self.data.T))\n\n        # calculate dependencies on selected links\n        self.CM = self.validator.run(self.data)\n\n        # list of selected features based on validator dependencies\n        self.CM.remove_unneeded_features()\n\n        # Saving final causal model\n        self.save()\n\n        return self.CM.features, self.CM\n\n\n    @ignore_warnings(category=ConvergenceWarning)\n    def run(self):\n\"\"\"\n        Run Selector and Validator\n\n        Returns:\n            list(str): list of selected variable names\n            dict(str,TargetDep): causal model\n        \"\"\"\n\n        ## 1. FILTER\n        self.run_filter()\n\n        # list of selected features based on filter dependencies\n        self.CM.remove_unneeded_features()\n        if not self.CM.features: return None, None\n\n        ## 2. VALIDATOR\n        # shrink dataframe d by using the filter result\n        self.data.shrink(self.CM.features)\n\n        # selected links to check by the validator\n        link_assumptions = self.CM.get_link_assumptions()\n\n        # calculate dependencies on selected links\n        f_dag = copy.deepcopy(self.CM)\n        self.CM = self.validator.run(self.data, link_assumptions)\n\n        # list of selected features based on validator dependencies\n        self.CM.remove_unneeded_features()\n\n        # Saving final causal model\n        self.__print_differences(f_dag, self.CM)\n        self.save()\n\n        return self.CM.features, self.CM\n\n\n    def dag(self,\n            node_layout = 'dot',\n            min_width = 1,\n            max_width = 5,\n            min_score = 0,\n            max_score = 1,\n            node_size = 8,\n            node_color = 'orange',\n            edge_color = 'grey',\n            bundle_parallel_edges = True,\n            font_size = 12,\n            label_type = LabelType.Lag,\n            save_name = None,\n            img_ext = ImageExt.PNG):\n\"\"\"\n        Saves dag plot if resfolder has been set otherwise it shows the figure\n\n        Args:\n            node_layout (str, optional): Node layout. Defaults to 'dot'.\n            min_width (int, optional): minimum linewidth. Defaults to 1.\n            max_width (int, optional): maximum linewidth. Defaults to 5.\n            min_score (int, optional): minimum score range. Defaults to 0.\n            max_score (int, optional): maximum score range. Defaults to 1.\n            node_size (int, optional): node size. Defaults to 8.\n            node_color (str, optional): node color. Defaults to 'orange'.\n            edge_color (str, optional): edge color. Defaults to 'grey'.\n            bundle_parallel_edges (str, optional): bundle parallel edge bit. Defaults to True.\n            font_size (int, optional): font size. Defaults to 12.\n            label_type (LabelType, optional): enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.\n            img_ext (ImageExt, optional): dag image extention (.png, .pdf, ..). Default ImageExt.PNG.\n        \"\"\"\n\n        if self.CM:\n            if save_name is None: save_name = self.dag_path\n            try:\n                self.CM.dag(node_layout, min_width, \n                            max_width, min_score, max_score,\n                            node_size, node_color, edge_color,\n                            bundle_parallel_edges, font_size, \n                            label_type, save_name, img_ext)\n            except:\n                CP.warning(\"node_layout = \" + node_layout + \" generates error. node_layout = circular used.\")\n                self.CM.dag(\"circular\", min_width, \n                            max_width, min_score, max_score,\n                            node_size, node_color, edge_color,\n                            bundle_parallel_edges, font_size, \n                            label_type, save_name, img_ext)\n        else:\n            CP.warning(\"Dag impossible to create: causal model not estimated yet\")\n\n\n    def timeseries_dag(self,\n                       min_width = 1,\n                       max_width = 5,\n                       min_score = 0,\n                       max_score = 1,\n                       node_size = 8,\n                       node_proximity = 2,\n                       font_size = 12,\n                       node_color = 'orange',\n                       edge_color = 'grey',\n                       save_name = None,\n                       img_ext = ImageExt.PNG):\n\"\"\"\n        Saves timeseries dag plot if resfolder has been set otherwise it shows the figure\n\n        Args:\n            min_width (int, optional): minimum linewidth. Defaults to 1.\n            max_width (int, optional): maximum linewidth. Defaults to 5.\n            min_score (int, optional): minimum score range. Defaults to 0.\n            max_score (int, optional): maximum score range. Defaults to 1.\n            node_size (int, optional): node size. Defaults to 8.\n            node_proximity (int, optional): node proximity. Defaults to 2.\n            node_color (str, optional): node color. Defaults to 'orange'.\n            edge_color (str, optional): edge color. Defaults to 'grey'.\n            font_size (int, optional): font size. Defaults to 12.\n            img_ext (ImageExt, optional): dag image extention (.png, .pdf, ..). Default ImageExt.PNG.\n        \"\"\"\n\n        if self.CM:\n            if save_name is None: save_name = self.ts_dag_path\n            self.CM.ts_dag(self.max_lag, min_width,\n                           max_width, min_score, max_score,\n                           node_size, node_proximity, node_color, edge_color,\n                           font_size, save_name, img_ext)\n        else:\n            CP.warning(\"Timeseries dag impossible to create: causal model not estimated yet\")\n\n\n    def load(self, res_path):\n\"\"\"\n        Loads previously estimated result \n\n        Args:\n            res_path (str): pickle file path\n        \"\"\"\n        with open(res_path, 'rb') as f:\n            r = pickle.load(f)\n            self.CM = r['causal_model']\n            self.f_alpha = r['filter_alpha']\n            self.pcmci_alpha = r['pcmci_alpha']\n            self.dag_path = r['dag_path']\n            self.ts_dag_path = r['ts_dag_path']\n\n\n    def save(self):\n\"\"\"\n        Save causal discovery result as pickle file if resfolder is set\n        \"\"\"\n        if self.respath is not None:\n            if self.CM:\n                res = dict()\n                res['causal_model'] = copy.deepcopy(self.CM)\n                res['features'] = copy.deepcopy(self.CM.features)\n                res['filter_alpha'] = self.f_alpha\n                res['pcmci_alpha'] = self.pcmci_alpha\n                res['dag_path'] = self.dag_path\n                res['ts_dag_path'] = self.ts_dag_path\n                with open(self.respath, 'wb') as resfile:\n                    pickle.dump(res, resfile)\n            else:\n                CP.warning(\"Causal model impossible to save\")\n\n\n    def __print_differences(self, old_dag : DAG, new_dag : DAG):\n\"\"\"\n        Print difference between old and new dependencies\n\n        Args:\n            old_dag (DAG): old dag\n            new_dag (DAG): new dag\n        \"\"\"\n        # Check difference(s) between validator and filter dependencies\n        list_diffs = list()\n        tmp = copy.deepcopy(old_dag)\n        for t in tmp.g:\n            if t not in new_dag.g:\n                list_diffs.append(t)\n                continue\n\n            for s in tmp.g[t].sources:\n                if s not in new_dag.g[t].sources:\n                    list_diffs.append((s[0], s[1], t))\n\n        if list_diffs:\n            CP.info(\"\\n\")\n            CP.info(DASH)\n            CP.info(\"Difference(s):\")\n            for diff in list_diffs: \n                if type(diff) is tuple:\n                    CP.info(\"Removed (\" + str(diff[0]) + \" -\" + str(diff[1]) +\") --&gt; (\" + str(diff[2]) + \")\")\n                else:\n                    CP.info(diff + \" removed\")\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.__init__","title":"<code>__init__(data, min_lag, max_lag, sel_method, val_condtest, verbosity, clean_cls=True, f_alpha=0.05, pcmci_alpha=0.05, resfolder=None, neglect_only_autodep=False)</code>","text":"<p>FPCMCI class contructor</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>data to analyse</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag</p> required <code>sel_method</code> <code>SelectionMethod</code> <p>selection method</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level</p> required <code>clean_cls</code> <code>bool</code> <p>Clean console bit. Default to True.</p> <code>True</code> <code>f_alpha</code> <code>float</code> <p>filter significance level. Defaults to 0.05.</p> <code>0.05</code> <code>pcmci_alpha</code> <code>float</code> <p>PCMCI significance level. Defaults to 0.05.</p> <code>0.05</code> <code>resfolder</code> <code>string</code> <p>result folder to create. Defaults to None.</p> <code>None</code> <code>neglect_only_autodep</code> <code>bool</code> <p>Bit for neglecting variables with only autodependency. Defaults to False.</p> <code>False</code> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def __init__(self, \n             data: Data, \n             min_lag, max_lag, \n             sel_method: SelectionMethod, val_condtest: CondIndTest, \n             verbosity: CPLevel, \n             clean_cls = True, \n             f_alpha = 0.05, \n             pcmci_alpha = 0.05, \n             resfolder = None,\n             neglect_only_autodep = False):\n\"\"\"\n    FPCMCI class contructor\n\n    Args:\n        data (Data): data to analyse\n        min_lag (int): minimum time lag\n        max_lag (int): maximum time lag\n        sel_method (SelectionMethod): selection method\n        val_condtest (CondIndTest): validation method\n        verbosity (CPLevel): verbosity level\n        clean_cls (bool): Clean console bit. Default to True.\n        f_alpha (float, optional): filter significance level. Defaults to 0.05.\n        pcmci_alpha (float, optional): PCMCI significance level. Defaults to 0.05.\n        resfolder (string, optional): result folder to create. Defaults to None.\n        neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False.\n    \"\"\"\n\n    self.data = data\n    self.f_alpha = f_alpha\n    self.pcmci_alpha = pcmci_alpha\n    self.min_lag = min_lag\n    self.max_lag = max_lag\n    self.sel_method = sel_method\n    self.CM = DAG(self.data.features, min_lag, max_lag, neglect_only_autodep)\n    self.neglect_only_autodep = neglect_only_autodep\n\n    self.respath, self.dag_path, self.ts_dag_path = None, None, None\n    if resfolder is not None:\n        logpath, self.respath, self.dag_path, self.ts_dag_path = utils.get_selectorpath(resfolder)  \n        sys.stdout = Logger(logpath, clean_cls)\n\n    self.validator = PCMCI(self.pcmci_alpha, min_lag, max_lag, neglect_only_autodep, val_condtest, verbosity)       \n    CP.set_verbosity(verbosity)\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.__print_differences","title":"<code>__print_differences(old_dag, new_dag)</code>","text":"<p>Print difference between old and new dependencies</p> <p>Parameters:</p> Name Type Description Default <code>old_dag</code> <code>DAG</code> <p>old dag</p> required <code>new_dag</code> <code>DAG</code> <p>new dag</p> required Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def __print_differences(self, old_dag : DAG, new_dag : DAG):\n\"\"\"\n    Print difference between old and new dependencies\n\n    Args:\n        old_dag (DAG): old dag\n        new_dag (DAG): new dag\n    \"\"\"\n    # Check difference(s) between validator and filter dependencies\n    list_diffs = list()\n    tmp = copy.deepcopy(old_dag)\n    for t in tmp.g:\n        if t not in new_dag.g:\n            list_diffs.append(t)\n            continue\n\n        for s in tmp.g[t].sources:\n            if s not in new_dag.g[t].sources:\n                list_diffs.append((s[0], s[1], t))\n\n    if list_diffs:\n        CP.info(\"\\n\")\n        CP.info(DASH)\n        CP.info(\"Difference(s):\")\n        for diff in list_diffs: \n            if type(diff) is tuple:\n                CP.info(\"Removed (\" + str(diff[0]) + \" -\" + str(diff[1]) +\") --&gt; (\" + str(diff[2]) + \")\")\n            else:\n                CP.info(diff + \" removed\")\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.dag","title":"<code>dag(node_layout='dot', min_width=1, max_width=5, min_score=0, max_score=1, node_size=8, node_color='orange', edge_color='grey', bundle_parallel_edges=True, font_size=12, label_type=LabelType.Lag, save_name=None, img_ext=ImageExt.PNG)</code>","text":"<p>Saves dag plot if resfolder has been set otherwise it shows the figure</p> <p>Parameters:</p> Name Type Description Default <code>node_layout</code> <code>str</code> <p>Node layout. Defaults to 'dot'.</p> <code>'dot'</code> <code>min_width</code> <code>int</code> <p>minimum linewidth. Defaults to 1.</p> <code>1</code> <code>max_width</code> <code>int</code> <p>maximum linewidth. Defaults to 5.</p> <code>5</code> <code>min_score</code> <code>int</code> <p>minimum score range. Defaults to 0.</p> <code>0</code> <code>max_score</code> <code>int</code> <p>maximum score range. Defaults to 1.</p> <code>1</code> <code>node_size</code> <code>int</code> <p>node size. Defaults to 8.</p> <code>8</code> <code>node_color</code> <code>str</code> <p>node color. Defaults to 'orange'.</p> <code>'orange'</code> <code>edge_color</code> <code>str</code> <p>edge color. Defaults to 'grey'.</p> <code>'grey'</code> <code>bundle_parallel_edges</code> <code>str</code> <p>bundle parallel edge bit. Defaults to True.</p> <code>True</code> <code>font_size</code> <code>int</code> <p>font size. Defaults to 12.</p> <code>12</code> <code>label_type</code> <code>LabelType</code> <p>enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.</p> <code>LabelType.Lag</code> <code>img_ext</code> <code>ImageExt</code> <p>dag image extention (.png, .pdf, ..). Default ImageExt.PNG.</p> <code>ImageExt.PNG</code> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def dag(self,\n        node_layout = 'dot',\n        min_width = 1,\n        max_width = 5,\n        min_score = 0,\n        max_score = 1,\n        node_size = 8,\n        node_color = 'orange',\n        edge_color = 'grey',\n        bundle_parallel_edges = True,\n        font_size = 12,\n        label_type = LabelType.Lag,\n        save_name = None,\n        img_ext = ImageExt.PNG):\n\"\"\"\n    Saves dag plot if resfolder has been set otherwise it shows the figure\n\n    Args:\n        node_layout (str, optional): Node layout. Defaults to 'dot'.\n        min_width (int, optional): minimum linewidth. Defaults to 1.\n        max_width (int, optional): maximum linewidth. Defaults to 5.\n        min_score (int, optional): minimum score range. Defaults to 0.\n        max_score (int, optional): maximum score range. Defaults to 1.\n        node_size (int, optional): node size. Defaults to 8.\n        node_color (str, optional): node color. Defaults to 'orange'.\n        edge_color (str, optional): edge color. Defaults to 'grey'.\n        bundle_parallel_edges (str, optional): bundle parallel edge bit. Defaults to True.\n        font_size (int, optional): font size. Defaults to 12.\n        label_type (LabelType, optional): enum to set whether to show the lag time (LabelType.Lag) or the strength (LabelType.Score) of the dependencies on each link/node or not showing the labels (LabelType.NoLabels). Default LabelType.Lag.\n        img_ext (ImageExt, optional): dag image extention (.png, .pdf, ..). Default ImageExt.PNG.\n    \"\"\"\n\n    if self.CM:\n        if save_name is None: save_name = self.dag_path\n        try:\n            self.CM.dag(node_layout, min_width, \n                        max_width, min_score, max_score,\n                        node_size, node_color, edge_color,\n                        bundle_parallel_edges, font_size, \n                        label_type, save_name, img_ext)\n        except:\n            CP.warning(\"node_layout = \" + node_layout + \" generates error. node_layout = circular used.\")\n            self.CM.dag(\"circular\", min_width, \n                        max_width, min_score, max_score,\n                        node_size, node_color, edge_color,\n                        bundle_parallel_edges, font_size, \n                        label_type, save_name, img_ext)\n    else:\n        CP.warning(\"Dag impossible to create: causal model not estimated yet\")\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.load","title":"<code>load(res_path)</code>","text":"<p>Loads previously estimated result </p> <p>Parameters:</p> Name Type Description Default <code>res_path</code> <code>str</code> <p>pickle file path</p> required Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def load(self, res_path):\n\"\"\"\n    Loads previously estimated result \n\n    Args:\n        res_path (str): pickle file path\n    \"\"\"\n    with open(res_path, 'rb') as f:\n        r = pickle.load(f)\n        self.CM = r['causal_model']\n        self.f_alpha = r['filter_alpha']\n        self.pcmci_alpha = r['pcmci_alpha']\n        self.dag_path = r['dag_path']\n        self.ts_dag_path = r['ts_dag_path']\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.run","title":"<code>run()</code>","text":"<p>Run Selector and Validator</p> <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>list of selected variable names</p> <code>dict</code> <code>(str, TargetDep)</code> <p>causal model</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>@ignore_warnings(category=ConvergenceWarning)\ndef run(self):\n\"\"\"\n    Run Selector and Validator\n\n    Returns:\n        list(str): list of selected variable names\n        dict(str,TargetDep): causal model\n    \"\"\"\n\n    ## 1. FILTER\n    self.run_filter()\n\n    # list of selected features based on filter dependencies\n    self.CM.remove_unneeded_features()\n    if not self.CM.features: return None, None\n\n    ## 2. VALIDATOR\n    # shrink dataframe d by using the filter result\n    self.data.shrink(self.CM.features)\n\n    # selected links to check by the validator\n    link_assumptions = self.CM.get_link_assumptions()\n\n    # calculate dependencies on selected links\n    f_dag = copy.deepcopy(self.CM)\n    self.CM = self.validator.run(self.data, link_assumptions)\n\n    # list of selected features based on validator dependencies\n    self.CM.remove_unneeded_features()\n\n    # Saving final causal model\n    self.__print_differences(f_dag, self.CM)\n    self.save()\n\n    return self.CM.features, self.CM\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.run_filter","title":"<code>run_filter()</code>","text":"<p>Run filter method</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>@ignore_warnings(category=ConvergenceWarning)\ndef run_filter(self):\n\"\"\"\n    Run filter method\n    \"\"\"\n    CP.info(\"\\n\")\n    CP.info(DASH)\n    CP.info(\"Selecting relevant features among: \" + str(self.data.features))\n    CP.info(\"Selection method: \" + self.sel_method.name)\n    CP.info(\"Significance level: \" + str(self.f_alpha))\n    CP.info(\"Max lag time: \" + str(self.max_lag))\n    CP.info(\"Min lag time: \" + str(self.min_lag))\n    CP.info(\"Data length: \" + str(self.data.T))\n\n    self.sel_method.initialise(self.data, self.f_alpha, self.min_lag, self.max_lag, self.CM)\n    self.CM = self.sel_method.compute_dependencies()  \n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.run_pcmci","title":"<code>run_pcmci()</code>","text":"<p>Run PCMCI</p> <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>list of selected variable names</p> <code>dict</code> <code>str:list(tuple)</code> <p>causal model</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>@ignore_warnings(category=ConvergenceWarning)\ndef run_pcmci(self):\n\"\"\"\n    Run PCMCI\n\n    Returns:\n        list(str): list of selected variable names\n        dict(str:list(tuple)): causal model\n    \"\"\"\n    CP.info(\"Significance level: \" + str(self.pcmci_alpha))\n    CP.info(\"Max lag time: \" + str(self.max_lag))\n    CP.info(\"Min lag time: \" + str(self.min_lag))\n    CP.info(\"Data length: \" + str(self.data.T))\n\n    # calculate dependencies on selected links\n    self.CM = self.validator.run(self.data)\n\n    # list of selected features based on validator dependencies\n    self.CM.remove_unneeded_features()\n\n    # Saving final causal model\n    self.save()\n\n    return self.CM.features, self.CM\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.save","title":"<code>save()</code>","text":"<p>Save causal discovery result as pickle file if resfolder is set</p> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def save(self):\n\"\"\"\n    Save causal discovery result as pickle file if resfolder is set\n    \"\"\"\n    if self.respath is not None:\n        if self.CM:\n            res = dict()\n            res['causal_model'] = copy.deepcopy(self.CM)\n            res['features'] = copy.deepcopy(self.CM.features)\n            res['filter_alpha'] = self.f_alpha\n            res['pcmci_alpha'] = self.pcmci_alpha\n            res['dag_path'] = self.dag_path\n            res['ts_dag_path'] = self.ts_dag_path\n            with open(self.respath, 'wb') as resfile:\n                pickle.dump(res, resfile)\n        else:\n            CP.warning(\"Causal model impossible to save\")\n</code></pre>"},{"location":"fpcmci/#fpcmci.FPCMCI.FPCMCI.timeseries_dag","title":"<code>timeseries_dag(min_width=1, max_width=5, min_score=0, max_score=1, node_size=8, node_proximity=2, font_size=12, node_color='orange', edge_color='grey', save_name=None, img_ext=ImageExt.PNG)</code>","text":"<p>Saves timeseries dag plot if resfolder has been set otherwise it shows the figure</p> <p>Parameters:</p> Name Type Description Default <code>min_width</code> <code>int</code> <p>minimum linewidth. Defaults to 1.</p> <code>1</code> <code>max_width</code> <code>int</code> <p>maximum linewidth. Defaults to 5.</p> <code>5</code> <code>min_score</code> <code>int</code> <p>minimum score range. Defaults to 0.</p> <code>0</code> <code>max_score</code> <code>int</code> <p>maximum score range. Defaults to 1.</p> <code>1</code> <code>node_size</code> <code>int</code> <p>node size. Defaults to 8.</p> <code>8</code> <code>node_proximity</code> <code>int</code> <p>node proximity. Defaults to 2.</p> <code>2</code> <code>node_color</code> <code>str</code> <p>node color. Defaults to 'orange'.</p> <code>'orange'</code> <code>edge_color</code> <code>str</code> <p>edge color. Defaults to 'grey'.</p> <code>'grey'</code> <code>font_size</code> <code>int</code> <p>font size. Defaults to 12.</p> <code>12</code> <code>img_ext</code> <code>ImageExt</code> <p>dag image extention (.png, .pdf, ..). Default ImageExt.PNG.</p> <code>ImageExt.PNG</code> Source code in <code>fpcmci/FPCMCI.py</code> <pre><code>def timeseries_dag(self,\n                   min_width = 1,\n                   max_width = 5,\n                   min_score = 0,\n                   max_score = 1,\n                   node_size = 8,\n                   node_proximity = 2,\n                   font_size = 12,\n                   node_color = 'orange',\n                   edge_color = 'grey',\n                   save_name = None,\n                   img_ext = ImageExt.PNG):\n\"\"\"\n    Saves timeseries dag plot if resfolder has been set otherwise it shows the figure\n\n    Args:\n        min_width (int, optional): minimum linewidth. Defaults to 1.\n        max_width (int, optional): maximum linewidth. Defaults to 5.\n        min_score (int, optional): minimum score range. Defaults to 0.\n        max_score (int, optional): maximum score range. Defaults to 1.\n        node_size (int, optional): node size. Defaults to 8.\n        node_proximity (int, optional): node proximity. Defaults to 2.\n        node_color (str, optional): node color. Defaults to 'orange'.\n        edge_color (str, optional): edge color. Defaults to 'grey'.\n        font_size (int, optional): font size. Defaults to 12.\n        img_ext (ImageExt, optional): dag image extention (.png, .pdf, ..). Default ImageExt.PNG.\n    \"\"\"\n\n    if self.CM:\n        if save_name is None: save_name = self.ts_dag_path\n        self.CM.ts_dag(self.max_lag, min_width,\n                       max_width, min_score, max_score,\n                       node_size, node_proximity, node_color, edge_color,\n                       font_size, save_name, img_ext)\n    else:\n        CP.warning(\"Timeseries dag impossible to create: causal model not estimated yet\")\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI","title":"<code>PCMCI</code>","text":"<p>PCMCI class.</p> <p>PCMCI works with FSelector in order to find the causal  model starting from a prefixed set of variables and links.</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>class PCMCI():\n\"\"\"\n    PCMCI class.\n\n    PCMCI works with FSelector in order to find the causal \n    model starting from a prefixed set of variables and links.\n    \"\"\"\n    def __init__(self, alpha, min_lag, max_lag, neglect_only_autodep, val_condtest: CondIndTest, verbosity: CPLevel, sys_context = dict()):\n\"\"\"\n        PCMCI class constructor\n\n        Args:\n            alpha (float): significance level\n            min_lag (int): minimum time lag\n            max_lag (int): maximum time lag\n            val_condtest (CondIndTest): validation method\n            verbosity (CPLevel): verbosity level\n        \"\"\"\n        self.alpha = alpha\n        self.min_lag = min_lag\n        self.max_lag = max_lag\n        self.neglect_only_autodep = neglect_only_autodep\n        self.result = None\n        self.dependencies = None\n        self.val_method = None\n        self.val_condtest = val_condtest\n        self.verbosity = verbosity.value\n        self.sys_context = sys_context\n\n\n    def run(self, data: Data, link_assumptions = None):\n\"\"\"\n        Run causal discovery algorithm\n\n        Args:\n            data (Data): Data obj to analyse\n            link_assumptions (dict, optional): prior assumptions on causal model links. Defaults to None.\n\n        Returns:\n            (DAG): estimated causal model\n        \"\"\"\n\n        CP.info('\\n')\n        CP.info(DASH)\n        CP.info(\"Running Causal Discovery Algorithm\")\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        d = vector(data.d)\n        dataframe = pp.DataFrame(data = d, var_names = data.features)\n\n        # init and run pcmci\n        self.val_method = VAL(dataframe = dataframe,\n                              cond_ind_test = self.val_condtest,\n                              verbosity = self.verbosity)\n\n        self.result = self.val_method.run_pcmci(link_assumptions = link_assumptions,\n                                                tau_max = self.max_lag,\n                                                tau_min = self.min_lag,\n                                                alpha_level = self.alpha,\n                                                # pc_alpha = self.alpha\n                                                )\n\n        self.result['var_names'] = data.features\n        self.result['pretty_var_names'] = data.pretty_features\n\n        self.dependencies = self.__PCMCI_to_DAG()\n        return self.dependencies\n\n\n    def run_pc(self, data: Data, link_assumptions = None):\n\"\"\"\n        Run PC causal discovery algorithm\n\n        Args:\n            data (Data): Data obj to analyse\n            link_assumptions (dict, optional): prior assumptions on causal model links. Defaults to None.\n\n        Returns:\n            (DAG): estimated parents\n        \"\"\"\n\n        CP.info('\\n')\n        CP.info(DASH)\n        CP.info(\"Running Causal Discovery Algorithm\")\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        d = vector(data.d)\n        dataframe = pp.DataFrame(data = d, var_names = data.features)\n\n        # init and run pcmci\n        self.val_method = VAL(dataframe = dataframe,\n                              cond_ind_test = self.val_condtest,\n                              verbosity = self.verbosity)\n\n        parents = self.val_method.run_pc_stable(link_assumptions = link_assumptions,\n                                                tau_max = self.max_lag,\n                                                tau_min = self.min_lag,\n                                                # pc_alpha = self.alpha,\n                                                )\n\n        return self.__PC_to_DAG(parents, data.features)\n\n\n    def __my_mci(self, autodep_dag: DAG):\n\"\"\"\n        Performs MCI test\n\n        Args:\n            autodep_dag (DAG): current DAG to check via MCI\n\n        Returns:\n            (dict): MCI result\n        \"\"\"\n        _int_link_assumptions = self.val_method._set_link_assumptions(autodep_dag.get_link_assumptions(autodep_ok = True), \n                                                                      self.min_lag, self.max_lag)\n\n        # Set the maximum condition dimension for Y and X\n        max_conds_py = self.val_method._set_max_condition_dim(None, self.min_lag, self.max_lag)\n        max_conds_px = self.val_method._set_max_condition_dim(None, self.min_lag, self.max_lag)\n\n        # Get the parents that will be checked\n        _int_parents = self.val_method._get_int_parents(autodep_dag.get_parents())\n\n        # Initialize the return values\n        val_matrix = np.zeros((self.val_method.dataframe.N, self.val_method.dataframe.N, self.max_lag + 1))\n        p_matrix = np.ones((self.val_method.dataframe.N, self.val_method.dataframe.N, self.max_lag + 1))\n        # Initialize the optional return of the confidance matrix\n        conf_matrix = None\n        if self.val_method.cond_ind_test.confidence is not None:\n            conf_matrix = np.zeros((self.val_method.dataframe.N, self.val_method.dataframe.N, self.max_lag + 1, 2))\n\n        # Get the conditions as implied by the input arguments\n        for j, i, tau, Z in self.val_method._iter_indep_conds(_int_parents,\n                                                              _int_link_assumptions,\n                                                              max_conds_py,\n                                                              max_conds_px):\n            # Set X and Y (for clarity of code)\n            X = [(i, tau)]\n            Y = [(j, 0)]\n\n            if ((i, -abs(tau)) in _int_link_assumptions[j] and _int_link_assumptions[j][(i, -abs(tau))] in ['--&gt;', 'o-o']):\n                if autodep_dag.g[autodep_dag.features[j]].is_autodependent:\n                    val = autodep_dag.g[autodep_dag.features[j]].sources[(autodep_dag.features[i], abs(tau))][SCORE]\n                    pval = autodep_dag.g[autodep_dag.features[j]].sources[(autodep_dag.features[i], abs(tau))][PVAL]\n                else:\n                    val = 1. \n                    pval = 0.\n            else:\n                val, pval = self.val_method.cond_ind_test.run_test(X, Y, Z=Z, tau_max=self.max_lag)\n            val_matrix[i, j, abs(tau)] = val\n            p_matrix[i, j, abs(tau)] = pval\n            CP.info(\"\\t|val = \" + str(round(val,3)) + \" |pval = \" + str(str(round(pval,3))))\n\n\n            # Get the confidence value, returns None if cond_ind_test.confidence\n            # is False\n            conf = self.val_method.cond_ind_test.get_confidence(X, Y, Z=Z, tau_max=self.max_lag)\n            # Record the value if the conditional independence requires it\n            if self.val_method.cond_ind_test.confidence:\n                conf_matrix[i, j, abs(tau)] = conf\n\n        # Threshold p_matrix to get graph\n        final_graph = p_matrix &lt;= self.alpha\n\n        # Convert to string graph representation\n        graph = self.val_method.convert_to_string_graph(final_graph)\n\n        # Symmetrize p_matrix and val_matrix\n        symmetrized_results = self.val_method.symmetrize_p_and_val_matrix(\n                            p_matrix=p_matrix, \n                            val_matrix=val_matrix, \n                            link_assumptions=_int_link_assumptions,\n                            conf_matrix=conf_matrix)\n\n        if self.verbosity &gt; 0:\n            self.val_method.print_significant_links(graph = graph,\n                                                    p_matrix = symmetrized_results['p_matrix'], \n                                                    val_matrix = symmetrized_results['val_matrix'],\n                                                    conf_matrix = symmetrized_results['conf_matrix'],\n                                                    alpha_level = self.alpha)\n\n        # Return the values as a dictionary and store in class\n        results = {\n            'graph': graph,\n            'p_matrix': symmetrized_results['p_matrix'],\n            'val_matrix': symmetrized_results['val_matrix'],\n            'conf_matrix': symmetrized_results['conf_matrix'],\n                   }\n        self.results = results\n        return results\n\n\n    def check_autodependency(self, data: Data, dag:DAG, link_assumptions) -&gt; DAG:\n\"\"\"\n        Run MCI test on observational data using the causal structure computed by the validator \n\n        Args:\n            data (Data): Data obj to analyse\n            dag (DAG): causal model\n            link_assumptions (dict): prior assumptions on causal model links. Defaults to None.\n\n        Returns:\n            (DAG): estimated causal model\n        \"\"\"\n\n        CP.info(\"\\n##\")\n        CP.info(\"## Auto-dependency check on observational data\")\n        CP.info(\"##\")\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        d = vector(data.d)\n        dataframe = pp.DataFrame(data = d, var_names = data.features)\n\n        # init and run pcmci\n        self.val_method = VAL(dataframe = dataframe,\n                              cond_ind_test = self.val_condtest,\n                              verbosity = 0)\n\n        _int_link_assumptions = self.val_method._set_link_assumptions(link_assumptions, self.min_lag, self.max_lag)\n\n        # Set the maximum condition dimension for Y and X\n        max_conds_py = self.val_method._set_max_condition_dim(None, self.min_lag, self.max_lag)\n        max_conds_px = self.val_method._set_max_condition_dim(None, self.min_lag, self.max_lag)\n\n        # Get the parents that will be checked\n        _int_parents = self.val_method._get_int_parents(dag.get_parents())\n\n        # Get the conditions as implied by the input arguments\n        links_tocheck = self.val_method._iter_indep_conds(_int_parents, _int_link_assumptions, max_conds_py, max_conds_px)\n        for j, i, tau, Z in links_tocheck:\n            if data.features[j] not in dag.autodep_nodes or j != i: continue\n            else:\n                # Set X and Y (for clarity of code)\n                X = [(i, tau)]\n                Y = [(j, 0)]\n\n                CP.info(\"\\tlink: (\" + data.features[i] + \" \" + str(tau) + \") -?&gt; (\" + data.features[j] + \"):\")\n                # Run the independence tests and record the results\n                val, pval = self.val_method.cond_ind_test.run_test(X, Y, Z = Z, tau_max = self.max_lag)\n                if pval &gt; self.alpha:\n                    dag.del_source(data.features[j], data.features[j], abs(tau))\n                    CP.info(\"\\t|val = \" + str(round(val,3)) + \" |pval = \" + str(str(round(pval,3))) + \" -- removed\")\n\n                else:\n                    dag.g[data.features[j]].sources[(data.features[i], abs(tau))][SCORE] = val\n                    dag.g[data.features[j]].sources[(data.features[i], abs(tau))][PVAL] = pval\n                    CP.info(\"\\t|val = \" + str(round(val,3)) + \" |pval = \" + str(str(round(pval,3))) + \" -- ok\")\n\n        return dag\n\n\n    def run_mci(self, data: Data, autodep_dag:DAG):#, link_assumptions, parents):\n\"\"\"\n        Run MCI test on observational data using the causal structure computed by the validator \n\n        Args:\n            data (Data): Data obj to analyse\n            autodep_dag (DAG): current DAG to check via MCI\n\n        Returns:\n            (DAG): estimated causal model\n        \"\"\"\n\n        CP.info(\"\\n##\")\n        CP.info(\"## MCI test analysis\")\n        CP.info(\"##\")\n\n        # build tigramite dataset\n        vector = np.vectorize(float)\n        d = vector(data.d)\n        dataframe = pp.DataFrame(data = d, var_names = data.features)\n\n        # init and run pcmci\n        self.val_method = VAL(dataframe = dataframe,\n                              cond_ind_test = self.val_condtest,\n                              verbosity = self.verbosity)\n\n        self.result = self.__my_mci(autodep_dag)\n\n        self.result['var_names'] = data.features\n        self.result['pretty_var_names'] = data.pretty_features\n\n        self.dependencies = self.__PCMCI_to_DAG()\n        return self.dependencies\n\n\n    def __PC_to_DAG(self, parents, features):\n\"\"\"\n        Re-elaborates the PC result in a dag\n\n        Args:\n            parents (dict): causal structure\n            features (list): feature list\n\n        Returns:\n            (DAG): pc result re-elaborated\n        \"\"\"\n        tmp_dag = DAG(features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = self.sys_context\n        for t in parents:\n            for s in parents[t]:\n                if features[t] in self.sys_context.keys() and features[s[0]] == self.sys_context[features[t]]:\n                    tmp_dag.g[features[t]].intervention_node = True\n                    tmp_dag.g[features[t]].associated_context = features[s[0]]\n                tmp_dag.add_source(features[t], features[s[0]], 1.0, 0.0, s[1])\n        return tmp_dag\n\n\n    def __PCMCI_to_DAG(self):\n\"\"\"\n        Re-elaborates the PCMCI result in a new dictionary\n\n        Returns:\n            (DAG): pcmci result re-elaborated\n        \"\"\"\n        vars = self.result['var_names']\n        tmp_dag = DAG(vars, self.min_lag, self.max_lag, self.neglect_only_autodep)\n        tmp_dag.sys_context = self.sys_context\n        N, lags = self.result['graph'][0].shape\n        for s in range(len(self.result['graph'])):\n            for t in range(N):\n                for lag in range(lags):\n                    if self.result['graph'][s][t,lag] == '--&gt;':\n                        if vars[t] in self.sys_context.keys() and vars[s] == self.sys_context[vars[t]]:\n                            tmp_dag.g[vars[t]].intervention_node = True\n                            tmp_dag.g[vars[t]].associated_context = vars[s]\n                        tmp_dag.add_source(vars[t], \n                                           vars[s],\n                                           self.result['val_matrix'][s][t,lag],\n                                           self.result['p_matrix'][s][t,lag],\n                                           lag)\n        return tmp_dag\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.__PCMCI_to_DAG","title":"<code>__PCMCI_to_DAG()</code>","text":"<p>Re-elaborates the PCMCI result in a new dictionary</p> <p>Returns:</p> Type Description <code>DAG</code> <p>pcmci result re-elaborated</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def __PCMCI_to_DAG(self):\n\"\"\"\n    Re-elaborates the PCMCI result in a new dictionary\n\n    Returns:\n        (DAG): pcmci result re-elaborated\n    \"\"\"\n    vars = self.result['var_names']\n    tmp_dag = DAG(vars, self.min_lag, self.max_lag, self.neglect_only_autodep)\n    tmp_dag.sys_context = self.sys_context\n    N, lags = self.result['graph'][0].shape\n    for s in range(len(self.result['graph'])):\n        for t in range(N):\n            for lag in range(lags):\n                if self.result['graph'][s][t,lag] == '--&gt;':\n                    if vars[t] in self.sys_context.keys() and vars[s] == self.sys_context[vars[t]]:\n                        tmp_dag.g[vars[t]].intervention_node = True\n                        tmp_dag.g[vars[t]].associated_context = vars[s]\n                    tmp_dag.add_source(vars[t], \n                                       vars[s],\n                                       self.result['val_matrix'][s][t,lag],\n                                       self.result['p_matrix'][s][t,lag],\n                                       lag)\n    return tmp_dag\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.__PC_to_DAG","title":"<code>__PC_to_DAG(parents, features)</code>","text":"<p>Re-elaborates the PC result in a dag</p> <p>Parameters:</p> Name Type Description Default <code>parents</code> <code>dict</code> <p>causal structure</p> required <code>features</code> <code>list</code> <p>feature list</p> required <p>Returns:</p> Type Description <code>DAG</code> <p>pc result re-elaborated</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def __PC_to_DAG(self, parents, features):\n\"\"\"\n    Re-elaborates the PC result in a dag\n\n    Args:\n        parents (dict): causal structure\n        features (list): feature list\n\n    Returns:\n        (DAG): pc result re-elaborated\n    \"\"\"\n    tmp_dag = DAG(features, self.min_lag, self.max_lag, self.neglect_only_autodep)\n    tmp_dag.sys_context = self.sys_context\n    for t in parents:\n        for s in parents[t]:\n            if features[t] in self.sys_context.keys() and features[s[0]] == self.sys_context[features[t]]:\n                tmp_dag.g[features[t]].intervention_node = True\n                tmp_dag.g[features[t]].associated_context = features[s[0]]\n            tmp_dag.add_source(features[t], features[s[0]], 1.0, 0.0, s[1])\n    return tmp_dag\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.__init__","title":"<code>__init__(alpha, min_lag, max_lag, neglect_only_autodep, val_condtest, verbosity, sys_context=dict())</code>","text":"<p>PCMCI class constructor</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>significance level</p> required <code>min_lag</code> <code>int</code> <p>minimum time lag</p> required <code>max_lag</code> <code>int</code> <p>maximum time lag</p> required <code>val_condtest</code> <code>CondIndTest</code> <p>validation method</p> required <code>verbosity</code> <code>CPLevel</code> <p>verbosity level</p> required Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def __init__(self, alpha, min_lag, max_lag, neglect_only_autodep, val_condtest: CondIndTest, verbosity: CPLevel, sys_context = dict()):\n\"\"\"\n    PCMCI class constructor\n\n    Args:\n        alpha (float): significance level\n        min_lag (int): minimum time lag\n        max_lag (int): maximum time lag\n        val_condtest (CondIndTest): validation method\n        verbosity (CPLevel): verbosity level\n    \"\"\"\n    self.alpha = alpha\n    self.min_lag = min_lag\n    self.max_lag = max_lag\n    self.neglect_only_autodep = neglect_only_autodep\n    self.result = None\n    self.dependencies = None\n    self.val_method = None\n    self.val_condtest = val_condtest\n    self.verbosity = verbosity.value\n    self.sys_context = sys_context\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.__my_mci","title":"<code>__my_mci(autodep_dag)</code>","text":"<p>Performs MCI test</p> <p>Parameters:</p> Name Type Description Default <code>autodep_dag</code> <code>DAG</code> <p>current DAG to check via MCI</p> required <p>Returns:</p> Type Description <code>dict</code> <p>MCI result</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def __my_mci(self, autodep_dag: DAG):\n\"\"\"\n    Performs MCI test\n\n    Args:\n        autodep_dag (DAG): current DAG to check via MCI\n\n    Returns:\n        (dict): MCI result\n    \"\"\"\n    _int_link_assumptions = self.val_method._set_link_assumptions(autodep_dag.get_link_assumptions(autodep_ok = True), \n                                                                  self.min_lag, self.max_lag)\n\n    # Set the maximum condition dimension for Y and X\n    max_conds_py = self.val_method._set_max_condition_dim(None, self.min_lag, self.max_lag)\n    max_conds_px = self.val_method._set_max_condition_dim(None, self.min_lag, self.max_lag)\n\n    # Get the parents that will be checked\n    _int_parents = self.val_method._get_int_parents(autodep_dag.get_parents())\n\n    # Initialize the return values\n    val_matrix = np.zeros((self.val_method.dataframe.N, self.val_method.dataframe.N, self.max_lag + 1))\n    p_matrix = np.ones((self.val_method.dataframe.N, self.val_method.dataframe.N, self.max_lag + 1))\n    # Initialize the optional return of the confidance matrix\n    conf_matrix = None\n    if self.val_method.cond_ind_test.confidence is not None:\n        conf_matrix = np.zeros((self.val_method.dataframe.N, self.val_method.dataframe.N, self.max_lag + 1, 2))\n\n    # Get the conditions as implied by the input arguments\n    for j, i, tau, Z in self.val_method._iter_indep_conds(_int_parents,\n                                                          _int_link_assumptions,\n                                                          max_conds_py,\n                                                          max_conds_px):\n        # Set X and Y (for clarity of code)\n        X = [(i, tau)]\n        Y = [(j, 0)]\n\n        if ((i, -abs(tau)) in _int_link_assumptions[j] and _int_link_assumptions[j][(i, -abs(tau))] in ['--&gt;', 'o-o']):\n            if autodep_dag.g[autodep_dag.features[j]].is_autodependent:\n                val = autodep_dag.g[autodep_dag.features[j]].sources[(autodep_dag.features[i], abs(tau))][SCORE]\n                pval = autodep_dag.g[autodep_dag.features[j]].sources[(autodep_dag.features[i], abs(tau))][PVAL]\n            else:\n                val = 1. \n                pval = 0.\n        else:\n            val, pval = self.val_method.cond_ind_test.run_test(X, Y, Z=Z, tau_max=self.max_lag)\n        val_matrix[i, j, abs(tau)] = val\n        p_matrix[i, j, abs(tau)] = pval\n        CP.info(\"\\t|val = \" + str(round(val,3)) + \" |pval = \" + str(str(round(pval,3))))\n\n\n        # Get the confidence value, returns None if cond_ind_test.confidence\n        # is False\n        conf = self.val_method.cond_ind_test.get_confidence(X, Y, Z=Z, tau_max=self.max_lag)\n        # Record the value if the conditional independence requires it\n        if self.val_method.cond_ind_test.confidence:\n            conf_matrix[i, j, abs(tau)] = conf\n\n    # Threshold p_matrix to get graph\n    final_graph = p_matrix &lt;= self.alpha\n\n    # Convert to string graph representation\n    graph = self.val_method.convert_to_string_graph(final_graph)\n\n    # Symmetrize p_matrix and val_matrix\n    symmetrized_results = self.val_method.symmetrize_p_and_val_matrix(\n                        p_matrix=p_matrix, \n                        val_matrix=val_matrix, \n                        link_assumptions=_int_link_assumptions,\n                        conf_matrix=conf_matrix)\n\n    if self.verbosity &gt; 0:\n        self.val_method.print_significant_links(graph = graph,\n                                                p_matrix = symmetrized_results['p_matrix'], \n                                                val_matrix = symmetrized_results['val_matrix'],\n                                                conf_matrix = symmetrized_results['conf_matrix'],\n                                                alpha_level = self.alpha)\n\n    # Return the values as a dictionary and store in class\n    results = {\n        'graph': graph,\n        'p_matrix': symmetrized_results['p_matrix'],\n        'val_matrix': symmetrized_results['val_matrix'],\n        'conf_matrix': symmetrized_results['conf_matrix'],\n               }\n    self.results = results\n    return results\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.check_autodependency","title":"<code>check_autodependency(data, dag, link_assumptions)</code>","text":"<p>Run MCI test on observational data using the causal structure computed by the validator </p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>Data obj to analyse</p> required <code>dag</code> <code>DAG</code> <p>causal model</p> required <code>link_assumptions</code> <code>dict</code> <p>prior assumptions on causal model links. Defaults to None.</p> required <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def check_autodependency(self, data: Data, dag:DAG, link_assumptions) -&gt; DAG:\n\"\"\"\n    Run MCI test on observational data using the causal structure computed by the validator \n\n    Args:\n        data (Data): Data obj to analyse\n        dag (DAG): causal model\n        link_assumptions (dict): prior assumptions on causal model links. Defaults to None.\n\n    Returns:\n        (DAG): estimated causal model\n    \"\"\"\n\n    CP.info(\"\\n##\")\n    CP.info(\"## Auto-dependency check on observational data\")\n    CP.info(\"##\")\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    d = vector(data.d)\n    dataframe = pp.DataFrame(data = d, var_names = data.features)\n\n    # init and run pcmci\n    self.val_method = VAL(dataframe = dataframe,\n                          cond_ind_test = self.val_condtest,\n                          verbosity = 0)\n\n    _int_link_assumptions = self.val_method._set_link_assumptions(link_assumptions, self.min_lag, self.max_lag)\n\n    # Set the maximum condition dimension for Y and X\n    max_conds_py = self.val_method._set_max_condition_dim(None, self.min_lag, self.max_lag)\n    max_conds_px = self.val_method._set_max_condition_dim(None, self.min_lag, self.max_lag)\n\n    # Get the parents that will be checked\n    _int_parents = self.val_method._get_int_parents(dag.get_parents())\n\n    # Get the conditions as implied by the input arguments\n    links_tocheck = self.val_method._iter_indep_conds(_int_parents, _int_link_assumptions, max_conds_py, max_conds_px)\n    for j, i, tau, Z in links_tocheck:\n        if data.features[j] not in dag.autodep_nodes or j != i: continue\n        else:\n            # Set X and Y (for clarity of code)\n            X = [(i, tau)]\n            Y = [(j, 0)]\n\n            CP.info(\"\\tlink: (\" + data.features[i] + \" \" + str(tau) + \") -?&gt; (\" + data.features[j] + \"):\")\n            # Run the independence tests and record the results\n            val, pval = self.val_method.cond_ind_test.run_test(X, Y, Z = Z, tau_max = self.max_lag)\n            if pval &gt; self.alpha:\n                dag.del_source(data.features[j], data.features[j], abs(tau))\n                CP.info(\"\\t|val = \" + str(round(val,3)) + \" |pval = \" + str(str(round(pval,3))) + \" -- removed\")\n\n            else:\n                dag.g[data.features[j]].sources[(data.features[i], abs(tau))][SCORE] = val\n                dag.g[data.features[j]].sources[(data.features[i], abs(tau))][PVAL] = pval\n                CP.info(\"\\t|val = \" + str(round(val,3)) + \" |pval = \" + str(str(round(pval,3))) + \" -- ok\")\n\n    return dag\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.run","title":"<code>run(data, link_assumptions=None)</code>","text":"<p>Run causal discovery algorithm</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>Data obj to analyse</p> required <code>link_assumptions</code> <code>dict</code> <p>prior assumptions on causal model links. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def run(self, data: Data, link_assumptions = None):\n\"\"\"\n    Run causal discovery algorithm\n\n    Args:\n        data (Data): Data obj to analyse\n        link_assumptions (dict, optional): prior assumptions on causal model links. Defaults to None.\n\n    Returns:\n        (DAG): estimated causal model\n    \"\"\"\n\n    CP.info('\\n')\n    CP.info(DASH)\n    CP.info(\"Running Causal Discovery Algorithm\")\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    d = vector(data.d)\n    dataframe = pp.DataFrame(data = d, var_names = data.features)\n\n    # init and run pcmci\n    self.val_method = VAL(dataframe = dataframe,\n                          cond_ind_test = self.val_condtest,\n                          verbosity = self.verbosity)\n\n    self.result = self.val_method.run_pcmci(link_assumptions = link_assumptions,\n                                            tau_max = self.max_lag,\n                                            tau_min = self.min_lag,\n                                            alpha_level = self.alpha,\n                                            # pc_alpha = self.alpha\n                                            )\n\n    self.result['var_names'] = data.features\n    self.result['pretty_var_names'] = data.pretty_features\n\n    self.dependencies = self.__PCMCI_to_DAG()\n    return self.dependencies\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.run_mci","title":"<code>run_mci(data, autodep_dag)</code>","text":"<p>Run MCI test on observational data using the causal structure computed by the validator </p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>Data obj to analyse</p> required <code>autodep_dag</code> <code>DAG</code> <p>current DAG to check via MCI</p> required <p>Returns:</p> Type Description <code>DAG</code> <p>estimated causal model</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def run_mci(self, data: Data, autodep_dag:DAG):#, link_assumptions, parents):\n\"\"\"\n    Run MCI test on observational data using the causal structure computed by the validator \n\n    Args:\n        data (Data): Data obj to analyse\n        autodep_dag (DAG): current DAG to check via MCI\n\n    Returns:\n        (DAG): estimated causal model\n    \"\"\"\n\n    CP.info(\"\\n##\")\n    CP.info(\"## MCI test analysis\")\n    CP.info(\"##\")\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    d = vector(data.d)\n    dataframe = pp.DataFrame(data = d, var_names = data.features)\n\n    # init and run pcmci\n    self.val_method = VAL(dataframe = dataframe,\n                          cond_ind_test = self.val_condtest,\n                          verbosity = self.verbosity)\n\n    self.result = self.__my_mci(autodep_dag)\n\n    self.result['var_names'] = data.features\n    self.result['pretty_var_names'] = data.pretty_features\n\n    self.dependencies = self.__PCMCI_to_DAG()\n    return self.dependencies\n</code></pre>"},{"location":"fpcmci/#fpcmci.PCMCI.PCMCI.run_pc","title":"<code>run_pc(data, link_assumptions=None)</code>","text":"<p>Run PC causal discovery algorithm</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Data</code> <p>Data obj to analyse</p> required <code>link_assumptions</code> <code>dict</code> <p>prior assumptions on causal model links. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DAG</code> <p>estimated parents</p> Source code in <code>fpcmci/PCMCI.py</code> <pre><code>def run_pc(self, data: Data, link_assumptions = None):\n\"\"\"\n    Run PC causal discovery algorithm\n\n    Args:\n        data (Data): Data obj to analyse\n        link_assumptions (dict, optional): prior assumptions on causal model links. Defaults to None.\n\n    Returns:\n        (DAG): estimated parents\n    \"\"\"\n\n    CP.info('\\n')\n    CP.info(DASH)\n    CP.info(\"Running Causal Discovery Algorithm\")\n\n    # build tigramite dataset\n    vector = np.vectorize(float)\n    d = vector(data.d)\n    dataframe = pp.DataFrame(data = d, var_names = data.features)\n\n    # init and run pcmci\n    self.val_method = VAL(dataframe = dataframe,\n                          cond_ind_test = self.val_condtest,\n                          verbosity = self.verbosity)\n\n    parents = self.val_method.run_pc_stable(link_assumptions = link_assumptions,\n                                            tau_max = self.max_lag,\n                                            tau_min = self.min_lag,\n                                            # pc_alpha = self.alpha,\n                                            )\n\n    return self.__PC_to_DAG(parents, data.features)\n</code></pre>"},{"location":"preprocessing/","title":"Preprocessing","text":""},{"location":"preprocessing/#fpcmci.preprocessing.data.Data","title":"<code>Data</code>","text":"<p>Data class manages the preprocess of the data before the causal analysis</p> Source code in <code>fpcmci/preprocessing/data.py</code> <pre><code>class Data():\n\"\"\"\n    Data class manages the preprocess of the data before the causal analysis\n    \"\"\"\n    def __init__(self, data, vars = None, fill_nan = True, stand = False, subsampling : SubsamplingMethod = None, show_subsampling = False):\n\"\"\"\n        Data class constructor\n\n        Args:\n            data (str/DataFrame/np.array): it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array\n            vars (list(str), optional): List containing variable names. If unset then, \n                if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N]\n                Defaults to None.\n            fill_nan (bool, optional): Fill NaNs bit. Defaults to True.\n            stand (bool, optional): Standardization bit. Defaults to False.\n            subsampling (SubsamplingMethod, optional): Subsampling method. If None not active. Defaults to None.\n            show_subsampling (bool, optional): If True shows subsampling result. Defaults to False.\n\n        Raises:\n            TypeError: if data is not str - DataFrame - ndarray\n        \"\"\"\n        # Data handling\n        if type(data) == np.ndarray:\n            self.d = pd.DataFrame(data)\n            if vars is None: self.d.columns = list(['X_' + str(f) for f in range(len(self.d.columns))])\n        elif type(data) == pd.DataFrame:\n            self.d = data\n        elif type(data) == str:\n            self.d = pd.read_csv(data)\n        else:\n            raise TypeError(\"data field not in the correct type\\ndata must be one of the following type:\\n- numpy.ndarray\\n- pandas.DataFrame\\n- .csv path\")\n\n\n        # Columns name handling\n        if vars is not None:\n            self.d.columns = list(vars)\n\n\n        self.orig_features = self.features\n        self.orig_pretty_features = self.pretty_features\n        self.orig_N = self.N\n        self.orig_T = len(self.d)\n\n        # Filling NaNs\n        if fill_nan:\n            if self.d.isnull().values.any():\n                self.d.fillna(inplace=True, method=\"ffill\")\n                self.d.fillna(inplace=True, method=\"bfill\")\n\n        # Subsampling data\n        if subsampling is not None:\n            subsampler = Subsampler(self.d, ss_method = subsampling)\n            self.d = pd.DataFrame(subsampler.subsample(), columns = self.features)\n            if show_subsampling: subsampler.plot_subsampled_data()\n\n        # Standardize data\n        if stand:\n            scaler = StandardScaler()\n            scaler = scaler.fit(self.d)\n            self.d = pd.DataFrame(scaler.transform(self.d), columns = self.features)\n\n\n    @property  \n    def features(self):\n\"\"\"\n        Returns list of features\n\n        Returns:\n            list(str): list of feature names\n        \"\"\"\n        return list(self.d.columns)\n\n    @property\n    def pretty_features(self):\n\"\"\"\n        Returns list of features with LATEX symbols\n\n        Returns:\n            list(str): list of feature names\n        \"\"\"\n        return [r'$' + str(v) + '$' for v in self.d.columns]\n\n    @property\n    def N(self):\n\"\"\"\n        Number of features\n\n        Returns:\n            (int): number of features\n        \"\"\"\n        return len(self.d.columns)\n\n    @property\n    def T(self):\n\"\"\"\n        Dataframe length\n\n        Returns:\n            (int): dataframe length\n        \"\"\"\n        return len(self.d)\n\n\n    def shrink(self, selected_features):\n\"\"\"\n        Shrinks dataframe d and dependencies based on the selected features\n\n        Args:\n            selected_features (list(str)): features selected by the selector\n        \"\"\"\n        self.d = self.d[selected_features]\n\n\n    def plot_timeseries(self):\n\"\"\"\n        Plots timeseries data\n        \"\"\"\n        # Create grid\n        gs = gridspec.GridSpec(self.N, 1)\n\n        # Time vector\n        T = list(range(self.T))\n\n        plt.figure()\n        for i in range(0, self.d.shape[1]):\n            ax = plt.subplot(gs[i, 0])\n            plt.plot(T, self.d.values[:, i], color = 'tab:red')\n            plt.ylabel(str(self.pretty_features[i]))\n\n        plt.show()\n</code></pre>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.N","title":"<code>N</code>  <code>property</code>","text":"<p>Number of features</p> <p>Returns:</p> Type Description <code>int</code> <p>number of features</p>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.T","title":"<code>T</code>  <code>property</code>","text":"<p>Dataframe length</p> <p>Returns:</p> Type Description <code>int</code> <p>dataframe length</p>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.features","title":"<code>features</code>  <code>property</code>","text":"<p>Returns list of features</p> <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>list of feature names</p>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.pretty_features","title":"<code>pretty_features</code>  <code>property</code>","text":"<p>Returns list of features with LATEX symbols</p> <p>Returns:</p> Name Type Description <code>list</code> <code>str</code> <p>list of feature names</p>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.__init__","title":"<code>__init__(data, vars=None, fill_nan=True, stand=False, subsampling=None, show_subsampling=False)</code>","text":"<p>Data class constructor</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str / DataFrame / np.array</code> <p>it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array</p> required <code>vars</code> <code>list(str)</code> <p>List containing variable names. If unset then,  if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N] Defaults to None.</p> <code>None</code> <code>fill_nan</code> <code>bool</code> <p>Fill NaNs bit. Defaults to True.</p> <code>True</code> <code>stand</code> <code>bool</code> <p>Standardization bit. Defaults to False.</p> <code>False</code> <code>subsampling</code> <code>SubsamplingMethod</code> <p>Subsampling method. If None not active. Defaults to None.</p> <code>None</code> <code>show_subsampling</code> <code>bool</code> <p>If True shows subsampling result. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>if data is not str - DataFrame - ndarray</p> Source code in <code>fpcmci/preprocessing/data.py</code> <pre><code>def __init__(self, data, vars = None, fill_nan = True, stand = False, subsampling : SubsamplingMethod = None, show_subsampling = False):\n\"\"\"\n    Data class constructor\n\n    Args:\n        data (str/DataFrame/np.array): it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array\n        vars (list(str), optional): List containing variable names. If unset then, \n            if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N]\n            Defaults to None.\n        fill_nan (bool, optional): Fill NaNs bit. Defaults to True.\n        stand (bool, optional): Standardization bit. Defaults to False.\n        subsampling (SubsamplingMethod, optional): Subsampling method. If None not active. Defaults to None.\n        show_subsampling (bool, optional): If True shows subsampling result. Defaults to False.\n\n    Raises:\n        TypeError: if data is not str - DataFrame - ndarray\n    \"\"\"\n    # Data handling\n    if type(data) == np.ndarray:\n        self.d = pd.DataFrame(data)\n        if vars is None: self.d.columns = list(['X_' + str(f) for f in range(len(self.d.columns))])\n    elif type(data) == pd.DataFrame:\n        self.d = data\n    elif type(data) == str:\n        self.d = pd.read_csv(data)\n    else:\n        raise TypeError(\"data field not in the correct type\\ndata must be one of the following type:\\n- numpy.ndarray\\n- pandas.DataFrame\\n- .csv path\")\n\n\n    # Columns name handling\n    if vars is not None:\n        self.d.columns = list(vars)\n\n\n    self.orig_features = self.features\n    self.orig_pretty_features = self.pretty_features\n    self.orig_N = self.N\n    self.orig_T = len(self.d)\n\n    # Filling NaNs\n    if fill_nan:\n        if self.d.isnull().values.any():\n            self.d.fillna(inplace=True, method=\"ffill\")\n            self.d.fillna(inplace=True, method=\"bfill\")\n\n    # Subsampling data\n    if subsampling is not None:\n        subsampler = Subsampler(self.d, ss_method = subsampling)\n        self.d = pd.DataFrame(subsampler.subsample(), columns = self.features)\n        if show_subsampling: subsampler.plot_subsampled_data()\n\n    # Standardize data\n    if stand:\n        scaler = StandardScaler()\n        scaler = scaler.fit(self.d)\n        self.d = pd.DataFrame(scaler.transform(self.d), columns = self.features)\n</code></pre>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.plot_timeseries","title":"<code>plot_timeseries()</code>","text":"<p>Plots timeseries data</p> Source code in <code>fpcmci/preprocessing/data.py</code> <pre><code>def plot_timeseries(self):\n\"\"\"\n    Plots timeseries data\n    \"\"\"\n    # Create grid\n    gs = gridspec.GridSpec(self.N, 1)\n\n    # Time vector\n    T = list(range(self.T))\n\n    plt.figure()\n    for i in range(0, self.d.shape[1]):\n        ax = plt.subplot(gs[i, 0])\n        plt.plot(T, self.d.values[:, i], color = 'tab:red')\n        plt.ylabel(str(self.pretty_features[i]))\n\n    plt.show()\n</code></pre>"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.shrink","title":"<code>shrink(selected_features)</code>","text":"<p>Shrinks dataframe d and dependencies based on the selected features</p> <p>Parameters:</p> Name Type Description Default <code>selected_features</code> <code>list(str</code> <p>features selected by the selector</p> required Source code in <code>fpcmci/preprocessing/data.py</code> <pre><code>def shrink(self, selected_features):\n\"\"\"\n    Shrinks dataframe d and dependencies based on the selected features\n\n    Args:\n        selected_features (list(str)): features selected by the selector\n    \"\"\"\n    self.d = self.d[selected_features]\n</code></pre>"},{"location":"subsampling_method/","title":"Subsampling","text":""},{"location":"subsampling_method/#fpcmci.preprocessing.Subsampler.Subsampler","title":"<code>Subsampler</code>","text":"<p>Subsampler class. </p> It subsamples the data by using a subsampling method chosen among <ul> <li>Static - subsamples data by taking one sample each step-samples</li> <li>WSDynamic - entropy based method with dynamic window size computed by breakpoint analysis</li> <li>WSFFTStatic - entropy based method with fixed window size computed by FFT analysis</li> <li>WSStatic - entropy base method with predefined window size</li> </ul> Source code in <code>fpcmci/preprocessing/Subsampler.py</code> <pre><code>class Subsampler():\n\"\"\"\n    Subsampler class. \n\n    It subsamples the data by using a subsampling method chosen among:\n        - Static - subsamples data by taking one sample each step-samples\n        - WSDynamic - entropy based method with dynamic window size computed by breakpoint analysis\n        - WSFFTStatic - entropy based method with fixed window size computed by FFT analysis\n        - WSStatic - entropy base method with predefined window size\n    \"\"\"\n\n    def __init__(self, \n                 df: pd.DataFrame, \n                 ss_method: SubsamplingMethod):\n\"\"\"\n        Subsampler class constructor\n\n        Args:\n            df (pd.DataFrame): dataframe to subsample\n            ss_method (SubsamplingMethod): subsampling method\n        \"\"\"\n        self.df = df\n        self.ss_method = ss_method\n        self.ss_method.initialise(df)\n\n\n    def subsample(self):\n\"\"\"\n        Runs the subsampling algorithm and returns the subsapled ndarray\n\n        Returns:\n            (ndarray): Subsampled dataframe value\n        \"\"\"\n        self.result = self.ss_method.run()\n        return self.df.values[self.result, :]\n\n\n    def plot_subsampled_data(self, dpi = 100, show = True):\n\"\"\"\n        Plot dataframe sub-sampled data\n\n        Args:\n            dpi (int, optional): image dpi. Defaults to 100.\n            show (bool, optional): if True it shows the figure and block the process. Defaults to True.\n        \"\"\"\n        n_plot = self.df.shape[1]\n\n        # Create grid\n        gs = gridspec.GridSpec(n_plot, 1)\n\n        # Time vector\n        T = list(range(0, self.df.shape[0]))\n\n        pl.figure(dpi = dpi)\n        for i in range(0, n_plot):\n            ax = pl.subplot(gs[i, 0])\n            pl.plot(T, self.df.values[:, i], color = 'tab:red')\n            pl.scatter(np.array(T)[self.result],\n                       self.df.values[self.result, i],\n                       s = 80,\n                       facecolors = 'none',\n                       edgecolors = 'b')\n            pl.gca().set(ylabel = r'$' + str(self.df.columns.values[i]) + '$')\n        if show:\n            pl.show()\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.Subsampler.Subsampler.__init__","title":"<code>__init__(df, ss_method)</code>","text":"<p>Subsampler class constructor</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>dataframe to subsample</p> required <code>ss_method</code> <code>SubsamplingMethod</code> <p>subsampling method</p> required Source code in <code>fpcmci/preprocessing/Subsampler.py</code> <pre><code>def __init__(self, \n             df: pd.DataFrame, \n             ss_method: SubsamplingMethod):\n\"\"\"\n    Subsampler class constructor\n\n    Args:\n        df (pd.DataFrame): dataframe to subsample\n        ss_method (SubsamplingMethod): subsampling method\n    \"\"\"\n    self.df = df\n    self.ss_method = ss_method\n    self.ss_method.initialise(df)\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.Subsampler.Subsampler.plot_subsampled_data","title":"<code>plot_subsampled_data(dpi=100, show=True)</code>","text":"<p>Plot dataframe sub-sampled data</p> <p>Parameters:</p> Name Type Description Default <code>dpi</code> <code>int</code> <p>image dpi. Defaults to 100.</p> <code>100</code> <code>show</code> <code>bool</code> <p>if True it shows the figure and block the process. Defaults to True.</p> <code>True</code> Source code in <code>fpcmci/preprocessing/Subsampler.py</code> <pre><code>def plot_subsampled_data(self, dpi = 100, show = True):\n\"\"\"\n    Plot dataframe sub-sampled data\n\n    Args:\n        dpi (int, optional): image dpi. Defaults to 100.\n        show (bool, optional): if True it shows the figure and block the process. Defaults to True.\n    \"\"\"\n    n_plot = self.df.shape[1]\n\n    # Create grid\n    gs = gridspec.GridSpec(n_plot, 1)\n\n    # Time vector\n    T = list(range(0, self.df.shape[0]))\n\n    pl.figure(dpi = dpi)\n    for i in range(0, n_plot):\n        ax = pl.subplot(gs[i, 0])\n        pl.plot(T, self.df.values[:, i], color = 'tab:red')\n        pl.scatter(np.array(T)[self.result],\n                   self.df.values[self.result, i],\n                   s = 80,\n                   facecolors = 'none',\n                   edgecolors = 'b')\n        pl.gca().set(ylabel = r'$' + str(self.df.columns.values[i]) + '$')\n    if show:\n        pl.show()\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.Subsampler.Subsampler.subsample","title":"<code>subsample()</code>","text":"<p>Runs the subsampling algorithm and returns the subsapled ndarray</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>Subsampled dataframe value</p> Source code in <code>fpcmci/preprocessing/Subsampler.py</code> <pre><code>def subsample(self):\n\"\"\"\n    Runs the subsampling algorithm and returns the subsapled ndarray\n\n    Returns:\n        (ndarray): Subsampled dataframe value\n    \"\"\"\n    self.result = self.ss_method.run()\n    return self.df.values[self.result, :]\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.SubsamplingMethod.SubsamplingMethod","title":"<code>SubsamplingMethod</code>","text":"<p>             Bases: <code>ABC</code></p> <p>SubsamplingMethod abstract class</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/SubsamplingMethod.py</code> <pre><code>class SubsamplingMethod(ABC):\n\"\"\"\n    SubsamplingMethod abstract class\n    \"\"\"\n    def __init__(self, ssmode: SSMode):\n        self.ssmode = ssmode\n        self.df = None\n\n\n    def initialise(self, dataframe: pd.DataFrame):\n\"\"\"\n        Initialise class by setting the dataframe to subsample\n\n        Args:\n            dataframe (pd.DataFrame): _description_\n        \"\"\"\n        self.df = dataframe\n\n\n    @abstractmethod\n    def run(self):\n\"\"\"\n        Run subsampler\n        \"\"\"\n        pass\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.SubsamplingMethod.SubsamplingMethod.initialise","title":"<code>initialise(dataframe)</code>","text":"<p>Initialise class by setting the dataframe to subsample</p> <p>Parameters:</p> Name Type Description Default <code>dataframe</code> <code>pd.DataFrame</code> <p>description</p> required Source code in <code>fpcmci/preprocessing/subsampling_methods/SubsamplingMethod.py</code> <pre><code>def initialise(self, dataframe: pd.DataFrame):\n\"\"\"\n    Initialise class by setting the dataframe to subsample\n\n    Args:\n        dataframe (pd.DataFrame): _description_\n    \"\"\"\n    self.df = dataframe\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.SubsamplingMethod.SubsamplingMethod.run","title":"<code>run()</code>  <code>abstractmethod</code>","text":"<p>Run subsampler</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/SubsamplingMethod.py</code> <pre><code>@abstractmethod\ndef run(self):\n\"\"\"\n    Run subsampler\n    \"\"\"\n    pass\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod","title":"<code>EntropyBasedMethod</code>","text":"<p>             Bases: <code>ABC</code></p> <p>EntropyBasedMethod abstract class</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>class EntropyBasedMethod(ABC):\n\"\"\"\n    EntropyBasedMethod abstract class\n    \"\"\"\n    def __init__(self, threshold):\n        self.windows = list()\n        self.segments = list()\n        self.threshold = threshold\n\n\n    def create_rounded_copy(self):\n\"\"\"\n        Create deepcopy of the dataframe but with rounded values\n\n        Returns:\n            (pd.DataFrame): rounded dataframe\n        \"\"\"\n        de = deepcopy(self.df)\n        de = de.round(1)\n        return de\n\n\n    def __normalization(self):\n\"\"\"\n        Normalize entropy for each moving window\n        \"\"\"\n        max_e = max([mw.entropy for mw in self.windows])\n        for mw in self.windows:\n            mw.entropy = mw.entropy / max_e\n\n\n    def moving_window_analysis(self):\n\"\"\"\n        Compute dataframe entropy on moving windows\n        \"\"\"\n        de = self.create_rounded_copy()\n\n        for ll, rl in self.segments:\n            # Create moving window\n            mw_df = de.values[ll: rl]\n\n            # Build a Moving Window\n            mw = MovingWindow(mw_df)\n\n            # Compute entropy\n            mw.get_entropy()\n\n            # Compute optimal number of samples\n            mw.optimal_sampling(self.threshold)\n\n            # Collect result in a list\n            self.windows.append(mw)\n\n        # Entropy normalization\n        self.__normalization()\n\n\n    # def extract_data(self):\n    #     \"\"\"\n    #     Extract plottable data from moving window analysis\n    #     \"\"\"\n    #     # Entropies and samples numbers list\n    #     self.__entropy_list = [mw.entropy for mw in self.__window_list]\n    #     self.__sample_number_list = [mw.opt_size for mw in self.__window_list]\n    #     self.__original_size = [mw.T for mw in self.__window_list]\n    #     self.num_samples = sum(self.__sample_number_list)\n\n    #     # Make entropy and sample array plottable\n    #     self.__pretty_signals()\n\n\n    # def __pretty_signals(self):\n    #     \"\"\"\n    #     Make entropy list and sample number list plottable\n    #     \"\"\"\n    #     _pretty_entropy = []\n    #     _pretty_sample_number = []\n    #     _pretty_original_size = []\n    #     for i, mw in enumerate(self.__window_list):\n    #         _pretty_entropy += np.repeat(self.__entropy_list[i], mw.T).tolist()\n    #         _pretty_sample_number += np.repeat(self.__sample_number_list[i], mw.T).tolist()\n    #         _pretty_original_size += np.repeat(self.__original_size[i], mw.T).tolist()\n    #     self.__entropy_list = _pretty_entropy\n    #     self.__sample_number_list = _pretty_sample_number\n    #     self.__original_size = _pretty_original_size\n\n    #     _diff = self.df.shape[0] - len(self.__entropy_list)\n    #     if _diff != 0:\n    #         self.__entropy_list = np.append(self.__entropy_list, [self.__entropy_list[-1]] * _diff)\n    #         self.__sample_number_list = np.append(self.__sample_number_list, [self.__sample_number_list[-1]] * _diff)\n\n\n    def extract_indexes(self):\n\"\"\"\n        Extract a list of indexes corresponding to the samples\n        selected by the subsampling procedure\n        \"\"\"\n        _sample_index_list = list()\n        for i, mw in enumerate(self.windows):\n            sum_ws = sum([wind.T for wind in self.windows[:i]])\n            sample_index = [si + sum_ws for si in mw.opt_samples_index]\n            _sample_index_list += sample_index\n        return _sample_index_list\n\n\n    @abstractmethod\n    def dataset_segmentation(self):\n\"\"\"\n        abstract method\n        \"\"\"\n        pass\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.__normalization","title":"<code>__normalization()</code>","text":"<p>Normalize entropy for each moving window</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>def __normalization(self):\n\"\"\"\n    Normalize entropy for each moving window\n    \"\"\"\n    max_e = max([mw.entropy for mw in self.windows])\n    for mw in self.windows:\n        mw.entropy = mw.entropy / max_e\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.create_rounded_copy","title":"<code>create_rounded_copy()</code>","text":"<p>Create deepcopy of the dataframe but with rounded values</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>rounded dataframe</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>def create_rounded_copy(self):\n\"\"\"\n    Create deepcopy of the dataframe but with rounded values\n\n    Returns:\n        (pd.DataFrame): rounded dataframe\n    \"\"\"\n    de = deepcopy(self.df)\n    de = de.round(1)\n    return de\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.dataset_segmentation","title":"<code>dataset_segmentation()</code>  <code>abstractmethod</code>","text":"<p>abstract method</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>@abstractmethod\ndef dataset_segmentation(self):\n\"\"\"\n    abstract method\n    \"\"\"\n    pass\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.extract_indexes","title":"<code>extract_indexes()</code>","text":"<p>Extract a list of indexes corresponding to the samples selected by the subsampling procedure</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>def extract_indexes(self):\n\"\"\"\n    Extract a list of indexes corresponding to the samples\n    selected by the subsampling procedure\n    \"\"\"\n    _sample_index_list = list()\n    for i, mw in enumerate(self.windows):\n        sum_ws = sum([wind.T for wind in self.windows[:i]])\n        sample_index = [si + sum_ws for si in mw.opt_samples_index]\n        _sample_index_list += sample_index\n    return _sample_index_list\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.moving_window_analysis","title":"<code>moving_window_analysis()</code>","text":"<p>Compute dataframe entropy on moving windows</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py</code> <pre><code>def moving_window_analysis(self):\n\"\"\"\n    Compute dataframe entropy on moving windows\n    \"\"\"\n    de = self.create_rounded_copy()\n\n    for ll, rl in self.segments:\n        # Create moving window\n        mw_df = de.values[ll: rl]\n\n        # Build a Moving Window\n        mw = MovingWindow(mw_df)\n\n        # Compute entropy\n        mw.get_entropy()\n\n        # Compute optimal number of samples\n        mw.optimal_sampling(self.threshold)\n\n        # Collect result in a list\n        self.windows.append(mw)\n\n    # Entropy normalization\n    self.__normalization()\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic","title":"<code>WSDynamic</code>","text":"<p>             Bases: <code>SubsamplingMethod</code>, <code>EntropyBasedMethod</code></p> <p>Subsampling method with dynamic window size based on entropy analysis</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>class WSDynamic(SubsamplingMethod, EntropyBasedMethod):\n\"\"\"\n    Subsampling method with dynamic window size based on entropy analysis\n    \"\"\"\n    def __init__(self, window_min_size, entropy_threshold):\n\"\"\"\n        WSDynamic class constructor\n\n        Args:\n            window_min_size (int): minimun window size\n            entropy_threshold (float): entropy threshold\n\n        Raises:\n            ValueError: if window_min_size == None\n        \"\"\"\n        SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n        EntropyBasedMethod.__init__(self, entropy_threshold)\n        if window_min_size is None:\n            raise ValueError(\"window_type = DYNAMIC but window_min_size not specified\")\n        self.wms = window_min_size\n        self.ws = None\n\n    def dataset_segmentation(self):\n\"\"\"\n        Segments dataset based on breakpoint analysis and a min window size\n        \"\"\"\n        de = self.create_rounded_copy()\n        algo = rpt.Pelt(model = \"l2\", min_size = self.wms).fit(de)\n        seg_res = algo.predict(pen = 10)\n        self.segments = [(seg_res[i - 1], seg_res[i]) for i in range(1, len(seg_res))]\n        self.segments.insert(0, (0, seg_res[0]))\n\n\n    def run(self):\n\"\"\"\n        Run subsampler\n\n        Returns:\n            (list[int]): indexes of the remaining samples\n        \"\"\"\n        # build list of segment\n        self.dataset_segmentation()\n\n        # compute entropy moving window\n        self.moving_window_analysis()\n\n        # extracting subsampling procedure results\n        idxs = self.extract_indexes()\n\n        return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.__init__","title":"<code>__init__(window_min_size, entropy_threshold)</code>","text":"<p>WSDynamic class constructor</p> <p>Parameters:</p> Name Type Description Default <code>window_min_size</code> <code>int</code> <p>minimun window size</p> required <code>entropy_threshold</code> <code>float</code> <p>entropy threshold</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if window_min_size == None</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def __init__(self, window_min_size, entropy_threshold):\n\"\"\"\n    WSDynamic class constructor\n\n    Args:\n        window_min_size (int): minimun window size\n        entropy_threshold (float): entropy threshold\n\n    Raises:\n        ValueError: if window_min_size == None\n    \"\"\"\n    SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n    EntropyBasedMethod.__init__(self, entropy_threshold)\n    if window_min_size is None:\n        raise ValueError(\"window_type = DYNAMIC but window_min_size not specified\")\n    self.wms = window_min_size\n    self.ws = None\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.dataset_segmentation","title":"<code>dataset_segmentation()</code>","text":"<p>Segments dataset based on breakpoint analysis and a min window size</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def dataset_segmentation(self):\n\"\"\"\n    Segments dataset based on breakpoint analysis and a min window size\n    \"\"\"\n    de = self.create_rounded_copy()\n    algo = rpt.Pelt(model = \"l2\", min_size = self.wms).fit(de)\n    seg_res = algo.predict(pen = 10)\n    self.segments = [(seg_res[i - 1], seg_res[i]) for i in range(1, len(seg_res))]\n    self.segments.insert(0, (0, seg_res[0]))\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.run","title":"<code>run()</code>","text":"<p>Run subsampler</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>indexes of the remaining samples</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def run(self):\n\"\"\"\n    Run subsampler\n\n    Returns:\n        (list[int]): indexes of the remaining samples\n    \"\"\"\n    # build list of segment\n    self.dataset_segmentation()\n\n    # compute entropy moving window\n    self.moving_window_analysis()\n\n    # extracting subsampling procedure results\n    idxs = self.extract_indexes()\n\n    return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic","title":"<code>WSDynamic</code>","text":"<p>             Bases: <code>SubsamplingMethod</code>, <code>EntropyBasedMethod</code></p> <p>Subsampling method with dynamic window size based on entropy analysis</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>class WSDynamic(SubsamplingMethod, EntropyBasedMethod):\n\"\"\"\n    Subsampling method with dynamic window size based on entropy analysis\n    \"\"\"\n    def __init__(self, window_min_size, entropy_threshold):\n\"\"\"\n        WSDynamic class constructor\n\n        Args:\n            window_min_size (int): minimun window size\n            entropy_threshold (float): entropy threshold\n\n        Raises:\n            ValueError: if window_min_size == None\n        \"\"\"\n        SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n        EntropyBasedMethod.__init__(self, entropy_threshold)\n        if window_min_size is None:\n            raise ValueError(\"window_type = DYNAMIC but window_min_size not specified\")\n        self.wms = window_min_size\n        self.ws = None\n\n    def dataset_segmentation(self):\n\"\"\"\n        Segments dataset based on breakpoint analysis and a min window size\n        \"\"\"\n        de = self.create_rounded_copy()\n        algo = rpt.Pelt(model = \"l2\", min_size = self.wms).fit(de)\n        seg_res = algo.predict(pen = 10)\n        self.segments = [(seg_res[i - 1], seg_res[i]) for i in range(1, len(seg_res))]\n        self.segments.insert(0, (0, seg_res[0]))\n\n\n    def run(self):\n\"\"\"\n        Run subsampler\n\n        Returns:\n            (list[int]): indexes of the remaining samples\n        \"\"\"\n        # build list of segment\n        self.dataset_segmentation()\n\n        # compute entropy moving window\n        self.moving_window_analysis()\n\n        # extracting subsampling procedure results\n        idxs = self.extract_indexes()\n\n        return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.__init__","title":"<code>__init__(window_min_size, entropy_threshold)</code>","text":"<p>WSDynamic class constructor</p> <p>Parameters:</p> Name Type Description Default <code>window_min_size</code> <code>int</code> <p>minimun window size</p> required <code>entropy_threshold</code> <code>float</code> <p>entropy threshold</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if window_min_size == None</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def __init__(self, window_min_size, entropy_threshold):\n\"\"\"\n    WSDynamic class constructor\n\n    Args:\n        window_min_size (int): minimun window size\n        entropy_threshold (float): entropy threshold\n\n    Raises:\n        ValueError: if window_min_size == None\n    \"\"\"\n    SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n    EntropyBasedMethod.__init__(self, entropy_threshold)\n    if window_min_size is None:\n        raise ValueError(\"window_type = DYNAMIC but window_min_size not specified\")\n    self.wms = window_min_size\n    self.ws = None\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.dataset_segmentation","title":"<code>dataset_segmentation()</code>","text":"<p>Segments dataset based on breakpoint analysis and a min window size</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def dataset_segmentation(self):\n\"\"\"\n    Segments dataset based on breakpoint analysis and a min window size\n    \"\"\"\n    de = self.create_rounded_copy()\n    algo = rpt.Pelt(model = \"l2\", min_size = self.wms).fit(de)\n    seg_res = algo.predict(pen = 10)\n    self.segments = [(seg_res[i - 1], seg_res[i]) for i in range(1, len(seg_res))]\n    self.segments.insert(0, (0, seg_res[0]))\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.run","title":"<code>run()</code>","text":"<p>Run subsampler</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>indexes of the remaining samples</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSDynamic.py</code> <pre><code>def run(self):\n\"\"\"\n    Run subsampler\n\n    Returns:\n        (list[int]): indexes of the remaining samples\n    \"\"\"\n    # build list of segment\n    self.dataset_segmentation()\n\n    # compute entropy moving window\n    self.moving_window_analysis()\n\n    # extracting subsampling procedure results\n    idxs = self.extract_indexes()\n\n    return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.Static.Static","title":"<code>Static</code>","text":"<p>             Bases: <code>SubsamplingMethod</code></p> <p>Subsamples data by taking one sample each step-samples</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/Static.py</code> <pre><code>class Static(SubsamplingMethod):\n\"\"\"\n    Subsamples data by taking one sample each step-samples\n    \"\"\"\n    def __init__(self, step):\n\"\"\"\n        Static class constructor\n\n        Args:\n            step (int): integer subsampling step\n\n        Raises:\n            ValueError: if step == None\n        \"\"\"\n        super().__init__(SSMode.Static)\n        if step is None:\n            raise ValueError(\"step not specified\")\n        self.step = step\n\n    def run(self):\n        return range(0, len(self.df.values), self.step)\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.Static.Static.__init__","title":"<code>__init__(step)</code>","text":"<p>Static class constructor</p> <p>Parameters:</p> Name Type Description Default <code>step</code> <code>int</code> <p>integer subsampling step</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if step == None</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/Static.py</code> <pre><code>def __init__(self, step):\n\"\"\"\n    Static class constructor\n\n    Args:\n        step (int): integer subsampling step\n\n    Raises:\n        ValueError: if step == None\n    \"\"\"\n    super().__init__(SSMode.Static)\n    if step is None:\n        raise ValueError(\"step not specified\")\n    self.step = step\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic","title":"<code>WSFFTStatic</code>","text":"<p>             Bases: <code>SubsamplingMethod</code>, <code>EntropyBasedMethod</code></p> <p>Subsampling method with static window size based on Fourier analysis</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>class WSFFTStatic(SubsamplingMethod, EntropyBasedMethod):\n\"\"\"\n    Subsampling method with static window size based on Fourier analysis\n    \"\"\"\n    def __init__(self, sampling_time, entropy_threshold):\n\"\"\"\n        WSFFTStatic class constructor\n\n        Args:\n            sampling_time (float): timeseries sampling time\n            entropy_threshold (float): entropy threshold\n        \"\"\"\n        SubsamplingMethod.__init__(self, SSMode.WSFFTStatic)\n        EntropyBasedMethod.__init__(self, entropy_threshold)\n        self.sampling_time = sampling_time\n\n\n    def __fourier_window(self):\n\"\"\"\n        Compute window size based on Fourier analysis performed on dataframe\n\n        Returns:\n            (int): window size\n        \"\"\"\n        N, dim = self.df.shape\n        xf = rfftfreq(N, self.sampling_time)\n        w_array = list()\n        for i in range(0, dim):\n            yf = np.abs(rfft(self.df.values[:, i]))\n\n            peak_indices, _ = scipy.signal.find_peaks(yf)\n            highest_peak_index = peak_indices[np.argmax(yf[peak_indices])]\n            w_array.append(ceil(1 / (2 * xf[highest_peak_index]) / self.sampling_time))\n            # fig, ax = pl.subplots()\n            # ax.plot(xf, yf)\n            # ax.plot(xf[highest_peak_index], np.abs(yf[highest_peak_index]), \"x\")\n            # pl.show()\n        return min(w_array)\n\n\n    def dataset_segmentation(self):\n\"\"\"\n        Segments dataset with a fixed window size\n        \"\"\"\n        seg_res = [i for i in range(0, len(self.df.values), self.ws)]\n        self.segments = [(i, i + self.ws) for i in range(0, len(self.df.values) - self.ws, self.ws)]\n        if not seg_res.__contains__(len(self.df.values)):\n            self.segments.append((seg_res[-1], len(self.df.values)))\n            seg_res.append(len(self.df.values))\n\n\n    def run(self):\n\"\"\"\n        Run subsampler\n\n        Returns:\n            (list[int]): indexes of the remaining samples\n        \"\"\"\n        # define window size\n        self.ws = self.__fourier_window()\n\n        # build list of segment\n        self.dataset_segmentation()\n\n        # compute entropy moving window\n        self.moving_window_analysis()\n\n        # extracting subsampling procedure results\n        idxs = self.extract_indexes()\n\n        return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.__fourier_window","title":"<code>__fourier_window()</code>","text":"<p>Compute window size based on Fourier analysis performed on dataframe</p> <p>Returns:</p> Type Description <code>int</code> <p>window size</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>def __fourier_window(self):\n\"\"\"\n    Compute window size based on Fourier analysis performed on dataframe\n\n    Returns:\n        (int): window size\n    \"\"\"\n    N, dim = self.df.shape\n    xf = rfftfreq(N, self.sampling_time)\n    w_array = list()\n    for i in range(0, dim):\n        yf = np.abs(rfft(self.df.values[:, i]))\n\n        peak_indices, _ = scipy.signal.find_peaks(yf)\n        highest_peak_index = peak_indices[np.argmax(yf[peak_indices])]\n        w_array.append(ceil(1 / (2 * xf[highest_peak_index]) / self.sampling_time))\n        # fig, ax = pl.subplots()\n        # ax.plot(xf, yf)\n        # ax.plot(xf[highest_peak_index], np.abs(yf[highest_peak_index]), \"x\")\n        # pl.show()\n    return min(w_array)\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.__init__","title":"<code>__init__(sampling_time, entropy_threshold)</code>","text":"<p>WSFFTStatic class constructor</p> <p>Parameters:</p> Name Type Description Default <code>sampling_time</code> <code>float</code> <p>timeseries sampling time</p> required <code>entropy_threshold</code> <code>float</code> <p>entropy threshold</p> required Source code in <code>fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>def __init__(self, sampling_time, entropy_threshold):\n\"\"\"\n    WSFFTStatic class constructor\n\n    Args:\n        sampling_time (float): timeseries sampling time\n        entropy_threshold (float): entropy threshold\n    \"\"\"\n    SubsamplingMethod.__init__(self, SSMode.WSFFTStatic)\n    EntropyBasedMethod.__init__(self, entropy_threshold)\n    self.sampling_time = sampling_time\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.dataset_segmentation","title":"<code>dataset_segmentation()</code>","text":"<p>Segments dataset with a fixed window size</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>def dataset_segmentation(self):\n\"\"\"\n    Segments dataset with a fixed window size\n    \"\"\"\n    seg_res = [i for i in range(0, len(self.df.values), self.ws)]\n    self.segments = [(i, i + self.ws) for i in range(0, len(self.df.values) - self.ws, self.ws)]\n    if not seg_res.__contains__(len(self.df.values)):\n        self.segments.append((seg_res[-1], len(self.df.values)))\n        seg_res.append(len(self.df.values))\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.run","title":"<code>run()</code>","text":"<p>Run subsampler</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>indexes of the remaining samples</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py</code> <pre><code>def run(self):\n\"\"\"\n    Run subsampler\n\n    Returns:\n        (list[int]): indexes of the remaining samples\n    \"\"\"\n    # define window size\n    self.ws = self.__fourier_window()\n\n    # build list of segment\n    self.dataset_segmentation()\n\n    # compute entropy moving window\n    self.moving_window_analysis()\n\n    # extracting subsampling procedure results\n    idxs = self.extract_indexes()\n\n    return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSStatic.WSStatic","title":"<code>WSStatic</code>","text":"<p>             Bases: <code>SubsamplingMethod</code>, <code>EntropyBasedMethod</code></p> <p>Entropy based subsampling method with static window size</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSStatic.py</code> <pre><code>class WSStatic(SubsamplingMethod, EntropyBasedMethod):\n\"\"\"\n    Entropy based subsampling method with static window size\n    \"\"\"\n    def __init__(self, window_size, entropy_threshold):\n\"\"\"\n        WSStatic class constructor\n\n        Args:\n            window_size (int): minimun window size\n            entropy_threshold (float): entropy threshold\n\n        Raises:\n            ValueError: if window_size == None\n        \"\"\"\n\n        SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n        EntropyBasedMethod.__init__(self, entropy_threshold)\n        if window_size is None:\n            raise ValueError(\"window_type = STATIC but window_size not specified\")\n        self.ws = window_size\n\n\n    def dataset_segmentation(self):\n\"\"\"\n        Segments dataset with a fixed window size\n        \"\"\"\n        seg_res = [i for i in range(0, len(self.df.values), self.ws)]\n        self.segments = [(i, i + self.ws) for i in range(0, len(self.df.values) - self.ws, self.ws)]\n        if not seg_res.__contains__(len(self.df.values)):\n            self.segments.append((seg_res[-1], len(self.df.values)))\n            seg_res.append(len(self.df.values))\n\n\n    def run(self):\n\"\"\"\n        Run subsampler\n\n        Returns:\n            (list[int]): indexes of the remaining samples\n        \"\"\"\n        # build list of segment\n        self.dataset_segmentation()\n\n        # compute entropy moving window\n        self.moving_window_analysis()\n\n        # extracting subsampling procedure results\n        idxs = self.extract_indexes()\n\n        return idxs\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSStatic.WSStatic.__init__","title":"<code>__init__(window_size, entropy_threshold)</code>","text":"<p>WSStatic class constructor</p> <p>Parameters:</p> Name Type Description Default <code>window_size</code> <code>int</code> <p>minimun window size</p> required <code>entropy_threshold</code> <code>float</code> <p>entropy threshold</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if window_size == None</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSStatic.py</code> <pre><code>def __init__(self, window_size, entropy_threshold):\n\"\"\"\n    WSStatic class constructor\n\n    Args:\n        window_size (int): minimun window size\n        entropy_threshold (float): entropy threshold\n\n    Raises:\n        ValueError: if window_size == None\n    \"\"\"\n\n    SubsamplingMethod.__init__(self, SSMode.WSDynamic)\n    EntropyBasedMethod.__init__(self, entropy_threshold)\n    if window_size is None:\n        raise ValueError(\"window_type = STATIC but window_size not specified\")\n    self.ws = window_size\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSStatic.WSStatic.dataset_segmentation","title":"<code>dataset_segmentation()</code>","text":"<p>Segments dataset with a fixed window size</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSStatic.py</code> <pre><code>def dataset_segmentation(self):\n\"\"\"\n    Segments dataset with a fixed window size\n    \"\"\"\n    seg_res = [i for i in range(0, len(self.df.values), self.ws)]\n    self.segments = [(i, i + self.ws) for i in range(0, len(self.df.values) - self.ws, self.ws)]\n    if not seg_res.__contains__(len(self.df.values)):\n        self.segments.append((seg_res[-1], len(self.df.values)))\n        seg_res.append(len(self.df.values))\n</code></pre>"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSStatic.WSStatic.run","title":"<code>run()</code>","text":"<p>Run subsampler</p> <p>Returns:</p> Type Description <code>list[int]</code> <p>indexes of the remaining samples</p> Source code in <code>fpcmci/preprocessing/subsampling_methods/WSStatic.py</code> <pre><code>def run(self):\n\"\"\"\n    Run subsampler\n\n    Returns:\n        (list[int]): indexes of the remaining samples\n    \"\"\"\n    # build list of segment\n    self.dataset_segmentation()\n\n    # compute entropy moving window\n    self.moving_window_analysis()\n\n    # extracting subsampling procedure results\n    idxs = self.extract_indexes()\n\n    return idxs\n</code></pre>"}]}