{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"FPCMCI - Filtered PCMCI Extension of the state-of-the-art causal discovery method PCMCI augmented with a feature-selection method based on Transfer Entropy. The algorithm, starting from a prefixed set of variables, identifies the correct subset of features and possible links between them which describe the observed process. Then, from the selected features and links, a causal model is built. Why FPCMCI? Current state-of-the-art causal discovery approaches suffer in terms of speed and accuracy of the causal analysis when the process to be analysed is composed by a large number of features. FPCMCI is able to select the most meaningful features from a set of variables and build a causal model from such selection. To this end, the causal analysis results faster and more accurate . In the following it is presented an example showing a comparison between causal models obtained by PCMCI and FPCMCI causal discovery algorithms on the same data. The latter have been created as follows: min_lag = 1 max_lag = 1 np.random.seed(1) nsample = 1500 nfeature = 6 d = np.random.random(size = (nsample, feature)) for t in range(max_lag, nsample): d[t, 0] += 2 * d[t-1, 1] + 3 * d[t-1, 3] d[t, 2] += 1.1 * d[t-1, 1]**2 d[t, 3] += d[t-1, 3] * d[t-1, 2] d[t, 4] += d[t-1, 4] + d[t-1, 5] * d[t-1, 0] Causal Model by PCMCI Causal Model by FPCMCI Execution time ~ 6min 50sec Execution time ~ 2min 45sec The causal analysis performed by the FPCMCI results not only faster but also more accurate. Indeed, the causal model derived by the FPCMCI agrees with the structure of the system of equations, instead the onoe derived by the PCMCI presents spurious links: * $X_2$ \u2192 $X_4$ * $X_2$ \u2192 $X_5$ Citation If you found this useful for your work, please cite these papers: @article{ghidoni2022human, title={From Human Perception and Action Recognition to Causal Understanding of Human-Robot Interaction in Industrial Environments}, author={Ghidoni, Stefano and Terreran, Matteo and Evangelista, Daniele and Menegatti, Emanuele and Eitzinger, Christian and Villagrossi, Enrico and Pedrocchi, Nicola and Castaman, Nicola and Malecha, Marcin and Mghames, Sariah and others}, year={2022} } @inproceedings{castri2022causal, title={Causal Discovery of Dynamic Models for Predicting Human Spatial Interactions}, author={Castri, Luca and Mghames, Sariah and Hanheide, Marc and Bellotto, Nicola}, booktitle={International Conference on Social Robotics (ICSR)}, year={2022}, } Requirements matplotlib==3.6.1 netgraph==4.10.2 networkx==2.8.6 numpy==1.21.5 pandas==1.5.0 ruptures==1.1.7 scikit_learn==1.1.3 scipy==1.8.0 setuptools==56.0.0 tigramite==5.1.0.3 Installation Before installing the FPCMCI package, you need to install the IDTxl package used for the feature-selection process, following the guide described here . Once complete, you can install the current release of FPCMCI with: pip install fpcmci Useful links Documentation","title":"Overview"},{"location":"#fpcmci-filtered-pcmci","text":"Extension of the state-of-the-art causal discovery method PCMCI augmented with a feature-selection method based on Transfer Entropy. The algorithm, starting from a prefixed set of variables, identifies the correct subset of features and possible links between them which describe the observed process. Then, from the selected features and links, a causal model is built.","title":" FPCMCI - Filtered PCMCI"},{"location":"#why-fpcmci","text":"Current state-of-the-art causal discovery approaches suffer in terms of speed and accuracy of the causal analysis when the process to be analysed is composed by a large number of features. FPCMCI is able to select the most meaningful features from a set of variables and build a causal model from such selection. To this end, the causal analysis results faster and more accurate . In the following it is presented an example showing a comparison between causal models obtained by PCMCI and FPCMCI causal discovery algorithms on the same data. The latter have been created as follows: min_lag = 1 max_lag = 1 np.random.seed(1) nsample = 1500 nfeature = 6 d = np.random.random(size = (nsample, feature)) for t in range(max_lag, nsample): d[t, 0] += 2 * d[t-1, 1] + 3 * d[t-1, 3] d[t, 2] += 1.1 * d[t-1, 1]**2 d[t, 3] += d[t-1, 3] * d[t-1, 2] d[t, 4] += d[t-1, 4] + d[t-1, 5] * d[t-1, 0] Causal Model by PCMCI Causal Model by FPCMCI Execution time ~ 6min 50sec Execution time ~ 2min 45sec The causal analysis performed by the FPCMCI results not only faster but also more accurate. Indeed, the causal model derived by the FPCMCI agrees with the structure of the system of equations, instead the onoe derived by the PCMCI presents spurious links: * $X_2$ \u2192 $X_4$ * $X_2$ \u2192 $X_5$","title":"Why FPCMCI?"},{"location":"#citation","text":"If you found this useful for your work, please cite these papers: @article{ghidoni2022human, title={From Human Perception and Action Recognition to Causal Understanding of Human-Robot Interaction in Industrial Environments}, author={Ghidoni, Stefano and Terreran, Matteo and Evangelista, Daniele and Menegatti, Emanuele and Eitzinger, Christian and Villagrossi, Enrico and Pedrocchi, Nicola and Castaman, Nicola and Malecha, Marcin and Mghames, Sariah and others}, year={2022} } @inproceedings{castri2022causal, title={Causal Discovery of Dynamic Models for Predicting Human Spatial Interactions}, author={Castri, Luca and Mghames, Sariah and Hanheide, Marc and Bellotto, Nicola}, booktitle={International Conference on Social Robotics (ICSR)}, year={2022}, }","title":"Citation"},{"location":"#requirements","text":"matplotlib==3.6.1 netgraph==4.10.2 networkx==2.8.6 numpy==1.21.5 pandas==1.5.0 ruptures==1.1.7 scikit_learn==1.1.3 scipy==1.8.0 setuptools==56.0.0 tigramite==5.1.0.3","title":"Requirements"},{"location":"#installation","text":"Before installing the FPCMCI package, you need to install the IDTxl package used for the feature-selection process, following the guide described here . Once complete, you can install the current release of FPCMCI with: pip install fpcmci","title":"Installation"},{"location":"#useful-links","text":"Documentation","title":"Useful links"},{"location":"causal_graph/","text":"__scale ( score , min_width , max_width , min_score = 0 , max_score = 1 ) Scales the score of the cause-effect relationship strength to a linewitdth Parameters: Name Type Description Default score float score to scale required min_width float minimum linewidth required max_width float maximum linewidth required min_score int minimum score range. Defaults to 0. 0 max_score int maximum score range. Defaults to 1. 1 Returns: Type Description float scaled score Source code in fpcmci/causal_graph.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def __scale ( score , min_width , max_width , min_score = 0 , max_score = 1 ): \"\"\" Scales the score of the cause-effect relationship strength to a linewitdth Args: score (float): score to scale min_width (float): minimum linewidth max_width (float): maximum linewidth min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. Returns: (float): scaled score \"\"\" return (( score - min_score ) / ( max_score - min_score )) * ( max_width - min_width ) + min_width dag ( res , node_layout = 'dot' , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , node_color = 'orange' , edge_color = 'grey' , font_size = 12 , show_edge_labels = True , label_type = LabelType . Lag , save_name = None ) build a dag Parameters: Name Type Description Default res dict dependencies result required node_layout str Node layout. Defaults to 'dot'. 'dot' min_width int minimum linewidth. Defaults to 1. 1 max_width int maximum linewidth. Defaults to 5. 5 min_score int minimum score range. Defaults to 0. 0 max_score int maximum score range. Defaults to 1. 1 node_size int node size. Defaults to 8. 8 node_color str node color. Defaults to 'orange'. 'orange' edge_color str edge color. Defaults to 'grey'. 'grey' font_size int font size. Defaults to 12. 12 show_edge_labels bool bit to show the label of the dependency on the edge/node border. Defaults to True. True label_type Enum LAG/SCORE information of the dependency on the edge/node border. Defaults to LAG. LabelType.Lag save_name str Filename path. If None, plot is shown and not saved. Defaults to None. None Source code in fpcmci/causal_graph.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def dag ( res , node_layout = 'dot' , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , node_color = 'orange' , edge_color = 'grey' , font_size = 12 , show_edge_labels = True , label_type = LabelType . Lag , save_name = None ): \"\"\" build a dag Args: res (dict): dependencies result node_layout (str, optional): Node layout. Defaults to 'dot'. min_width (int, optional): minimum linewidth. Defaults to 1. max_width (int, optional): maximum linewidth. Defaults to 5. min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. node_size (int, optional): node size. Defaults to 8. node_color (str, optional): node color. Defaults to 'orange'. edge_color (str, optional): edge color. Defaults to 'grey'. font_size (int, optional): font size. Defaults to 12. show_edge_labels (bool, optional): bit to show the label of the dependency on the edge/node border. Defaults to True. label_type (Enum, optional): LAG/SCORE information of the dependency on the edge/node border. Defaults to LAG. save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None. \"\"\" G = nx . DiGraph () # add nodes G . add_nodes_from ( res . keys ()) border = dict () for t in res . keys (): border [ t ] = 0 for s in res [ t ]: if t == s [ SOURCE ]: border [ t ] = __scale ( s [ SCORE ], min_width , max_width , min_score , max_score ) if show_edge_labels : if label_type == LabelType . Lag : node_label = { t : s [ LAG ] for t in res . keys () for s in res [ t ] if t == s [ SOURCE ]} elif label_type == LabelType . Score : node_label = { t : round ( s [ SCORE ], 3 ) for t in res . keys () for s in res [ t ] if t == s [ SOURCE ]} else : node_label = None # edges definition edges = [( s [ SOURCE ], t ) for t in res . keys () for s in res [ t ] if t != s [ SOURCE ]] G . add_edges_from ( edges ) edge_width = {( s [ SOURCE ], t ): __scale ( s [ SCORE ], min_width , max_width , min_score , max_score ) for t in res . keys () for s in res [ t ] if t != s [ SOURCE ]} if show_edge_labels : if label_type == LabelType . Lag : edge_label = {( s [ SOURCE ], t ): s [ LAG ] for t in res . keys () for s in res [ t ] if t != s [ SOURCE ]} elif label_type == LabelType . Score : edge_label = {( s [ SOURCE ], t ): round ( s [ SCORE ], 3 ) for t in res . keys () for s in res [ t ] if t != s [ SOURCE ]} else : edge_label = None fig , ax = plt . subplots ( figsize = ( 8 , 6 )) if edges : a = Graph ( G , node_layout = node_layout , node_size = node_size , node_color = node_color , node_labels = node_label , node_edge_width = border , node_label_fontdict = dict ( size = font_size ), node_edge_color = edge_color , node_label_offset = 0.1 , node_alpha = 1 , arrows = True , edge_layout = 'curved' , edge_label = show_edge_labels , edge_labels = edge_label , edge_label_fontdict = dict ( size = font_size ), edge_color = edge_color , edge_width = edge_width , edge_alpha = 1 , edge_zorder = 1 , edge_label_position = 0.35 , edge_layout_kwargs = dict ( bundle_parallel_edges = False , k = 0.05 )) nx . draw_networkx_labels ( G , pos = a . node_positions , labels = { n : n for n in G }, font_size = font_size ) if save_name is not None : plt . savefig ( save_name , dpi = 300 ) else : plt . show () ts_dag ( res , tau , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , node_color = 'orange' , edge_color = 'grey' , font_size = 12 , save_name = None ) build a timeseries dag Parameters: Name Type Description Default res dict dependencies result required tau int max time lag required min_width int minimum linewidth. Defaults to 1. 1 max_width int maximum linewidth. Defaults to 5. 5 min_score int minimum score range. Defaults to 0. 0 max_score int maximum score range. Defaults to 1. 1 node_size int node size. Defaults to 8. 8 node_color str node color. Defaults to 'orange'. 'orange' edge_color str edge color. Defaults to 'grey'. 'grey' font_size int font size. Defaults to 12. 12 save_name str Filename path. If None, plot is shown and not saved. Defaults to None. None Source code in fpcmci/causal_graph.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def ts_dag ( res , tau , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , node_color = 'orange' , edge_color = 'grey' , font_size = 12 , save_name = None ): \"\"\" build a timeseries dag Args: res (dict): dependencies result tau (int): max time lag min_width (int, optional): minimum linewidth. Defaults to 1. max_width (int, optional): maximum linewidth. Defaults to 5. min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. node_size (int, optional): node size. Defaults to 8. node_color (str, optional): node color. Defaults to 'orange'. edge_color (str, optional): edge color. Defaults to 'grey'. font_size (int, optional): font size. Defaults to 12. save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None. \"\"\" # add nodes G = nx . grid_2d_graph ( tau + 1 , len ( res . keys ())) pos = dict () for n in G . nodes (): if n [ 0 ] == 0 : pos [ n ] = ( n [ 0 ], n [ 1 ] / 2 ) else : pos [ n ] = ( n [ 0 ] + .5 , n [ 1 ] / 2 ) scale = max ( pos . values ()) G . remove_edges_from ( G . edges ()) # edges definition edges = list () edge_width = dict () for t in res . keys (): for s in res [ t ]: s_index = len ( res . keys ()) - 1 - list ( res . keys ()) . index ( s [ SOURCE ]) t_index = len ( res . keys ()) - 1 - list ( res . keys ()) . index ( t ) s_node = ( tau - s [ LAG ], s_index ) t_node = ( tau , t_index ) edges . append (( s_node , t_node )) edge_width [( s_node , t_node )] = __scale ( s [ SCORE ], min_width , max_width , min_score , max_score ) G . add_edges_from ( edges ) # label definition labeldict = {} for n in G . nodes (): if n [ 0 ] == 0 : labeldict [ n ] = list ( res . keys ())[ len ( res . keys ()) - 1 - n [ 1 ]] fig , ax = plt . subplots ( figsize = ( 8 , 6 )) # time line text drawing pos_tau = set ([ pos [ p ][ 0 ] for p in pos ]) max_y = max ([ pos [ p ][ 1 ] for p in pos ]) for p in pos_tau : if abs ( int ( p ) - tau ) == 0 : ax . text ( p , max_y + .3 , r \"$t$\" , horizontalalignment = 'center' , fontsize = font_size ) else : ax . text ( p , max_y + .3 , r \"$t-\" + str ( abs ( int ( p ) - tau )) + \"$\" , horizontalalignment = 'center' , fontsize = font_size ) Graph ( G , node_layout = { p : np . array ( pos [ p ]) for p in pos }, node_size = node_size , node_color = node_color , node_labels = labeldict , node_label_offset = 0 , node_edge_width = 0 , node_label_fontdict = dict ( size = font_size ), node_alpha = 1 , arrows = True , edge_layout = 'curved' , edge_label = False , edge_color = edge_color , edge_width = edge_width , edge_alpha = 1 , edge_zorder = 1 , scale = ( scale [ 0 ] + 2 , scale [ 1 ] + 2 )) if save_name is not None : plt . savefig ( save_name , dpi = 300 ) else : plt . show ()","title":"Causal Graph"},{"location":"causal_graph/#fpcmci.causal_graph.__scale","text":"Scales the score of the cause-effect relationship strength to a linewitdth Parameters: Name Type Description Default score float score to scale required min_width float minimum linewidth required max_width float maximum linewidth required min_score int minimum score range. Defaults to 0. 0 max_score int maximum score range. Defaults to 1. 1 Returns: Type Description float scaled score Source code in fpcmci/causal_graph.py 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 def __scale ( score , min_width , max_width , min_score = 0 , max_score = 1 ): \"\"\" Scales the score of the cause-effect relationship strength to a linewitdth Args: score (float): score to scale min_width (float): minimum linewidth max_width (float): maximum linewidth min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. Returns: (float): scaled score \"\"\" return (( score - min_score ) / ( max_score - min_score )) * ( max_width - min_width ) + min_width","title":"__scale()"},{"location":"causal_graph/#fpcmci.causal_graph.dag","text":"build a dag Parameters: Name Type Description Default res dict dependencies result required node_layout str Node layout. Defaults to 'dot'. 'dot' min_width int minimum linewidth. Defaults to 1. 1 max_width int maximum linewidth. Defaults to 5. 5 min_score int minimum score range. Defaults to 0. 0 max_score int maximum score range. Defaults to 1. 1 node_size int node size. Defaults to 8. 8 node_color str node color. Defaults to 'orange'. 'orange' edge_color str edge color. Defaults to 'grey'. 'grey' font_size int font size. Defaults to 12. 12 show_edge_labels bool bit to show the label of the dependency on the edge/node border. Defaults to True. True label_type Enum LAG/SCORE information of the dependency on the edge/node border. Defaults to LAG. LabelType.Lag save_name str Filename path. If None, plot is shown and not saved. Defaults to None. None Source code in fpcmci/causal_graph.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 def dag ( res , node_layout = 'dot' , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , node_color = 'orange' , edge_color = 'grey' , font_size = 12 , show_edge_labels = True , label_type = LabelType . Lag , save_name = None ): \"\"\" build a dag Args: res (dict): dependencies result node_layout (str, optional): Node layout. Defaults to 'dot'. min_width (int, optional): minimum linewidth. Defaults to 1. max_width (int, optional): maximum linewidth. Defaults to 5. min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. node_size (int, optional): node size. Defaults to 8. node_color (str, optional): node color. Defaults to 'orange'. edge_color (str, optional): edge color. Defaults to 'grey'. font_size (int, optional): font size. Defaults to 12. show_edge_labels (bool, optional): bit to show the label of the dependency on the edge/node border. Defaults to True. label_type (Enum, optional): LAG/SCORE information of the dependency on the edge/node border. Defaults to LAG. save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None. \"\"\" G = nx . DiGraph () # add nodes G . add_nodes_from ( res . keys ()) border = dict () for t in res . keys (): border [ t ] = 0 for s in res [ t ]: if t == s [ SOURCE ]: border [ t ] = __scale ( s [ SCORE ], min_width , max_width , min_score , max_score ) if show_edge_labels : if label_type == LabelType . Lag : node_label = { t : s [ LAG ] for t in res . keys () for s in res [ t ] if t == s [ SOURCE ]} elif label_type == LabelType . Score : node_label = { t : round ( s [ SCORE ], 3 ) for t in res . keys () for s in res [ t ] if t == s [ SOURCE ]} else : node_label = None # edges definition edges = [( s [ SOURCE ], t ) for t in res . keys () for s in res [ t ] if t != s [ SOURCE ]] G . add_edges_from ( edges ) edge_width = {( s [ SOURCE ], t ): __scale ( s [ SCORE ], min_width , max_width , min_score , max_score ) for t in res . keys () for s in res [ t ] if t != s [ SOURCE ]} if show_edge_labels : if label_type == LabelType . Lag : edge_label = {( s [ SOURCE ], t ): s [ LAG ] for t in res . keys () for s in res [ t ] if t != s [ SOURCE ]} elif label_type == LabelType . Score : edge_label = {( s [ SOURCE ], t ): round ( s [ SCORE ], 3 ) for t in res . keys () for s in res [ t ] if t != s [ SOURCE ]} else : edge_label = None fig , ax = plt . subplots ( figsize = ( 8 , 6 )) if edges : a = Graph ( G , node_layout = node_layout , node_size = node_size , node_color = node_color , node_labels = node_label , node_edge_width = border , node_label_fontdict = dict ( size = font_size ), node_edge_color = edge_color , node_label_offset = 0.1 , node_alpha = 1 , arrows = True , edge_layout = 'curved' , edge_label = show_edge_labels , edge_labels = edge_label , edge_label_fontdict = dict ( size = font_size ), edge_color = edge_color , edge_width = edge_width , edge_alpha = 1 , edge_zorder = 1 , edge_label_position = 0.35 , edge_layout_kwargs = dict ( bundle_parallel_edges = False , k = 0.05 )) nx . draw_networkx_labels ( G , pos = a . node_positions , labels = { n : n for n in G }, font_size = font_size ) if save_name is not None : plt . savefig ( save_name , dpi = 300 ) else : plt . show ()","title":"dag()"},{"location":"causal_graph/#fpcmci.causal_graph.ts_dag","text":"build a timeseries dag Parameters: Name Type Description Default res dict dependencies result required tau int max time lag required min_width int minimum linewidth. Defaults to 1. 1 max_width int maximum linewidth. Defaults to 5. 5 min_score int minimum score range. Defaults to 0. 0 max_score int maximum score range. Defaults to 1. 1 node_size int node size. Defaults to 8. 8 node_color str node color. Defaults to 'orange'. 'orange' edge_color str edge color. Defaults to 'grey'. 'grey' font_size int font size. Defaults to 12. 12 save_name str Filename path. If None, plot is shown and not saved. Defaults to None. None Source code in fpcmci/causal_graph.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 def ts_dag ( res , tau , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , node_color = 'orange' , edge_color = 'grey' , font_size = 12 , save_name = None ): \"\"\" build a timeseries dag Args: res (dict): dependencies result tau (int): max time lag min_width (int, optional): minimum linewidth. Defaults to 1. max_width (int, optional): maximum linewidth. Defaults to 5. min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. node_size (int, optional): node size. Defaults to 8. node_color (str, optional): node color. Defaults to 'orange'. edge_color (str, optional): edge color. Defaults to 'grey'. font_size (int, optional): font size. Defaults to 12. save_name (str, optional): Filename path. If None, plot is shown and not saved. Defaults to None. \"\"\" # add nodes G = nx . grid_2d_graph ( tau + 1 , len ( res . keys ())) pos = dict () for n in G . nodes (): if n [ 0 ] == 0 : pos [ n ] = ( n [ 0 ], n [ 1 ] / 2 ) else : pos [ n ] = ( n [ 0 ] + .5 , n [ 1 ] / 2 ) scale = max ( pos . values ()) G . remove_edges_from ( G . edges ()) # edges definition edges = list () edge_width = dict () for t in res . keys (): for s in res [ t ]: s_index = len ( res . keys ()) - 1 - list ( res . keys ()) . index ( s [ SOURCE ]) t_index = len ( res . keys ()) - 1 - list ( res . keys ()) . index ( t ) s_node = ( tau - s [ LAG ], s_index ) t_node = ( tau , t_index ) edges . append (( s_node , t_node )) edge_width [( s_node , t_node )] = __scale ( s [ SCORE ], min_width , max_width , min_score , max_score ) G . add_edges_from ( edges ) # label definition labeldict = {} for n in G . nodes (): if n [ 0 ] == 0 : labeldict [ n ] = list ( res . keys ())[ len ( res . keys ()) - 1 - n [ 1 ]] fig , ax = plt . subplots ( figsize = ( 8 , 6 )) # time line text drawing pos_tau = set ([ pos [ p ][ 0 ] for p in pos ]) max_y = max ([ pos [ p ][ 1 ] for p in pos ]) for p in pos_tau : if abs ( int ( p ) - tau ) == 0 : ax . text ( p , max_y + .3 , r \"$t$\" , horizontalalignment = 'center' , fontsize = font_size ) else : ax . text ( p , max_y + .3 , r \"$t-\" + str ( abs ( int ( p ) - tau )) + \"$\" , horizontalalignment = 'center' , fontsize = font_size ) Graph ( G , node_layout = { p : np . array ( pos [ p ]) for p in pos }, node_size = node_size , node_color = node_color , node_labels = labeldict , node_label_offset = 0 , node_edge_width = 0 , node_label_fontdict = dict ( size = font_size ), node_alpha = 1 , arrows = True , edge_layout = 'curved' , edge_label = False , edge_color = edge_color , edge_width = edge_width , edge_alpha = 1 , edge_zorder = 1 , scale = ( scale [ 0 ] + 2 , scale [ 1 ] + 2 )) if save_name is not None : plt . savefig ( save_name , dpi = 300 ) else : plt . show ()","title":"ts_dag()"},{"location":"feature_selection_method/","text":"SelectionMethod Bases: ABC SelectionMethod abstract class Source code in fpcmci/selection_methods/SelectionMethod.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class SelectionMethod ( ABC ): \"\"\" SelectionMethod abstract class \"\"\" def __init__ ( self , ctest ): self . ctest = ctest self . data = None self . alpha = None self . min_lag = None self . max_lag = None self . result = dict () @property def name ( self ): \"\"\" Returns Selection Method name Returns: (str): Selection Method name \"\"\" return self . ctest . value def initialise ( self , data : Data , alpha , min_lag , max_lag ): \"\"\" Initialises the selection method Args: data (Data): Data alpha (float): significance threshold min_lag (int): min lag time max_lag (int): max lag time \"\"\" self . data = data self . alpha = alpha self . min_lag = min_lag self . max_lag = max_lag self . result = { f : list () for f in self . data . features } @abstractmethod def compute_dependencies ( self ) -> dict : \"\"\" abstract method \"\"\" pass def _prepare_ts ( self , target , lag , apply_lag = True , consider_autodep = True ): \"\"\" prepare the dataframe to the analysis Args: target (str): name target var lag (int): lag time to apply apply_lag (bool, optional): True if you want to apply the lag, False otherwise. Defaults to True. Returns: tuple(DataFrame, DataFrame): source and target dataframe \"\"\" if not consider_autodep : if apply_lag : Y = self . data . d [ target ][ lag :] X = self . data . d . loc [:, self . data . d . columns != target ][: - lag ] else : Y = self . data . d [ target ] X = self . data . d . loc [:, self . data . d . columns != target ] else : if apply_lag : Y = self . data . d [ target ][ lag :] X = self . data . d [: - lag ] else : Y = self . data . d [ target ] X = self . data . d return X , Y def _get_sources ( self , t ): \"\"\" Return target sources Args: t (str): target variable name Returns: list(str): list of target sources \"\"\" return [ s [ SOURCE ] for s in self . result [ t ]] def _add_dependecies ( self , t , s , score , pval , lag ): \"\"\" Adds found dependency from source (s) to target (t) specifying the score, pval and the lag Args: t (str): target feature name s (str): source feature name score (float): selection method score pval (float): pval associated to the dependency lag (int): lag time of the dependency \"\"\" self . result [ t ] . append ({ SOURCE : s , SCORE : score , PVAL : pval , LAG : lag }) str_s = \"(\" + s + \" -\" + str ( lag ) + \")\" str_arrow = \" --> \" str_t = \"(\" + t + \")\" str_score = \"|score: \" + \" {:.3f} \" . format ( score ) str_pval = \"|pval: \" + \" {:.3f} \" . format ( pval ) print ( ' {:<20s}{:<10s}{:<10s}{:<20s}{:<20s} ' . format ( str_s , str_arrow , str_t , str_score , str_pval )) compute_dependencies () abstractmethod abstract method Source code in fpcmci/selection_methods/SelectionMethod.py 67 68 69 70 71 72 @abstractmethod def compute_dependencies ( self ) -> dict : \"\"\" abstract method \"\"\" pass initialise ( data , alpha , min_lag , max_lag ) Initialises the selection method Parameters: Name Type Description Default data Data Data required alpha float significance threshold required min_lag int min lag time required max_lag int max lag time required Source code in fpcmci/selection_methods/SelectionMethod.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def initialise ( self , data : Data , alpha , min_lag , max_lag ): \"\"\" Initialises the selection method Args: data (Data): Data alpha (float): significance threshold min_lag (int): min lag time max_lag (int): max lag time \"\"\" self . data = data self . alpha = alpha self . min_lag = min_lag self . max_lag = max_lag self . result = { f : list () for f in self . data . features } name () property Returns Selection Method name Returns: Type Description str Selection Method name Source code in fpcmci/selection_methods/SelectionMethod.py 39 40 41 42 43 44 45 46 47 @property def name ( self ): \"\"\" Returns Selection Method name Returns: (str): Selection Method name \"\"\" return self . ctest . value Corr Bases: SelectionMethod Feature selection method based on Correlation analysis Source code in fpcmci/selection_methods/Corr.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Corr ( SelectionMethod ): \"\"\" Feature selection method based on Correlation analysis \"\"\" def __init__ ( self ): \"\"\" Corr contructor class \"\"\" super () . __init__ ( CTest . Corr ) def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by correlation analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" CP . info ( \" \\n ##\" ) CP . info ( \"## \" + self . name + \" analysis\" ) CP . info ( \"##\" ) for lag in range ( self . min_lag , self . max_lag + 1 ): for target in self . data . features : CP . info ( \" \\n ## Target variable: \" + target ) X , Y = self . _prepare_ts ( target , lag ) scores , pval = f_regression ( X , Y ) # Filter on pvalue f = pval < self . alpha # Result of the selection sel_sources , sel_sources_score , sel_sources_pval = X . columns [ f ] . tolist (), scores [ f ] . tolist (), pval [ f ] . tolist () for s , score , pval in zip ( sel_sources , sel_sources_score , sel_sources_pval ): self . _add_dependecies ( target , s , score , pval , lag ) return self . result __init__ () Corr contructor class Source code in fpcmci/selection_methods/Corr.py 9 10 11 12 13 def __init__ ( self ): \"\"\" Corr contructor class \"\"\" super () . __init__ ( CTest . Corr ) compute_dependencies () compute list of dependencies for each target by correlation analysis Returns: Type Description dict dictonary(TARGET: list SOURCES) Source code in fpcmci/selection_methods/Corr.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by correlation analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" CP . info ( \" \\n ##\" ) CP . info ( \"## \" + self . name + \" analysis\" ) CP . info ( \"##\" ) for lag in range ( self . min_lag , self . max_lag + 1 ): for target in self . data . features : CP . info ( \" \\n ## Target variable: \" + target ) X , Y = self . _prepare_ts ( target , lag ) scores , pval = f_regression ( X , Y ) # Filter on pvalue f = pval < self . alpha # Result of the selection sel_sources , sel_sources_score , sel_sources_pval = X . columns [ f ] . tolist (), scores [ f ] . tolist (), pval [ f ] . tolist () for s , score , pval in zip ( sel_sources , sel_sources_score , sel_sources_pval ): self . _add_dependecies ( target , s , score , pval , lag ) return self . result ParCorr Bases: SelectionMethod Feature selection method based on Partial Correlation analysis Source code in fpcmci/selection_methods/ParCorr.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class ParCorr ( SelectionMethod ): \"\"\" Feature selection method based on Partial Correlation analysis \"\"\" def __init__ ( self ): \"\"\" ParCorr class contructor \"\"\" super () . __init__ ( CTest . Corr ) def get_residual ( self , covar , target ): \"\"\" Calculate residual of the target variable obtaining conditioning on the covar variables Args: covar (np.array): conditioning variables target (np.array): target variable Returns: (np.array): residual \"\"\" beta = np . linalg . lstsq ( covar , target , rcond = None )[ 0 ] return target - np . dot ( covar , beta ) def partial_corr ( self , X , Y , Z ): \"\"\" Calculate Partial correlation between X and Y conditioning on Z Args: X (np.array): source candidate variable Y (np.array): target variable Z (np.array): conditioning variable Returns: (float, float): partial correlation, p-value \"\"\" pcorr , pval = stats . pearsonr ( self . get_residual ( Z , X ), self . get_residual ( Z , Y )) return pcorr , pval def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by partial correlation analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" CP . info ( \" \\n ##\" ) CP . info ( \"## \" + self . name + \" analysis\" ) CP . info ( \"##\" ) for lag in range ( self . min_lag , self . max_lag + 1 ): for target in self . data . features : CP . info ( \" \\n ## Target variable: \" + target ) candidates = self . data . features Y = np . array ( self . data . d [ target ][ lag :]) while candidates : tmp_res = None covars = self . _get_sources ( target ) Z = np . array ( self . data . d [ covars ][: - lag ]) for candidate in candidates : X = np . array ( self . data . d [ candidate ][: - lag ]) score , pval = self . partial_corr ( X , Y , Z ) if pval < self . alpha and ( tmp_res is None or abs ( tmp_res [ 1 ]) < abs ( score )): tmp_res = ( candidate , score , pval ) if tmp_res is not None : self . _add_dependecies ( target , tmp_res [ 0 ], tmp_res [ 1 ], tmp_res [ 2 ], lag ) candidates . remove ( tmp_res [ 0 ]) else : break return self . result __init__ () ParCorr class contructor Source code in fpcmci/selection_methods/ParCorr.py 10 11 12 13 14 def __init__ ( self ): \"\"\" ParCorr class contructor \"\"\" super () . __init__ ( CTest . Corr ) compute_dependencies () compute list of dependencies for each target by partial correlation analysis Returns: Type Description dict dictonary(TARGET: list SOURCES) Source code in fpcmci/selection_methods/ParCorr.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by partial correlation analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" CP . info ( \" \\n ##\" ) CP . info ( \"## \" + self . name + \" analysis\" ) CP . info ( \"##\" ) for lag in range ( self . min_lag , self . max_lag + 1 ): for target in self . data . features : CP . info ( \" \\n ## Target variable: \" + target ) candidates = self . data . features Y = np . array ( self . data . d [ target ][ lag :]) while candidates : tmp_res = None covars = self . _get_sources ( target ) Z = np . array ( self . data . d [ covars ][: - lag ]) for candidate in candidates : X = np . array ( self . data . d [ candidate ][: - lag ]) score , pval = self . partial_corr ( X , Y , Z ) if pval < self . alpha and ( tmp_res is None or abs ( tmp_res [ 1 ]) < abs ( score )): tmp_res = ( candidate , score , pval ) if tmp_res is not None : self . _add_dependecies ( target , tmp_res [ 0 ], tmp_res [ 1 ], tmp_res [ 2 ], lag ) candidates . remove ( tmp_res [ 0 ]) else : break return self . result get_residual ( covar , target ) Calculate residual of the target variable obtaining conditioning on the covar variables Parameters: Name Type Description Default covar np . array conditioning variables required target np . array target variable required Returns: Type Description np . array residual Source code in fpcmci/selection_methods/ParCorr.py 17 18 19 20 21 22 23 24 25 26 27 28 29 def get_residual ( self , covar , target ): \"\"\" Calculate residual of the target variable obtaining conditioning on the covar variables Args: covar (np.array): conditioning variables target (np.array): target variable Returns: (np.array): residual \"\"\" beta = np . linalg . lstsq ( covar , target , rcond = None )[ 0 ] return target - np . dot ( covar , beta ) partial_corr ( X , Y , Z ) Calculate Partial correlation between X and Y conditioning on Z Parameters: Name Type Description Default X np . array source candidate variable required Y np . array target variable required Z np . array conditioning variable required Returns: Type Description float , float partial correlation, p-value Source code in fpcmci/selection_methods/ParCorr.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def partial_corr ( self , X , Y , Z ): \"\"\" Calculate Partial correlation between X and Y conditioning on Z Args: X (np.array): source candidate variable Y (np.array): target variable Z (np.array): conditioning variable Returns: (float, float): partial correlation, p-value \"\"\" pcorr , pval = stats . pearsonr ( self . get_residual ( Z , X ), self . get_residual ( Z , Y )) return pcorr , pval MI Bases: SelectionMethod Feature selection method based on Mutual Information analysis Source code in fpcmci/selection_methods/MI.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class MI ( SelectionMethod ): \"\"\" Feature selection method based on Mutual Information analysis \"\"\" def __init__ ( self , estimator : MIestimator ): \"\"\" MI class contructor Args: estimator (MIestimator): Gaussian/Kraskov \"\"\" super () . __init__ ( CTest . MI ) self . estimator = estimator def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by mutual information analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" with _suppress_stdout (): data = Data ( self . d . values , dim_order = 'sp' ) # sp = samples(row) x processes(col) network_analysis = MultivariateMI () settings = { 'cmi_estimator' : self . estimator . value , 'max_lag_sources' : self . max_lag , 'min_lag_sources' : self . min_lag , 'alpha_max_stats' : self . alpha , 'alpha_min_stats' : self . alpha , 'alpha_omnibus' : self . alpha , 'alpha_max_seq' : self . alpha , 'verbose' : False } results = network_analysis . analyse_network ( settings = settings , data = data ) for t in results . _single_target . keys (): sel_sources = [ s [ 0 ] for s in results . _single_target [ t ][ 'selected_vars_sources' ]] if sel_sources : sel_sources_lag = [ s [ 1 ] for s in results . _single_target [ t ][ 'selected_vars_sources' ]] sel_sources_score = results . _single_target [ t ][ 'selected_sources_mi' ] sel_sources_pval = results . _single_target [ t ][ 'selected_sources_pval' ] for s , score , pval , lag in zip ( sel_sources , sel_sources_score , sel_sources_pval , sel_sources_lag ): self . _add_dependecies ( self . features [ t ], self . features [ s ], score , pval , lag ) return self . result __init__ ( estimator ) MI class contructor Parameters: Name Type Description Default estimator MIestimator Gaussian/Kraskov required Source code in fpcmci/selection_methods/MI.py 15 16 17 18 19 20 21 22 23 def __init__ ( self , estimator : MIestimator ): \"\"\" MI class contructor Args: estimator (MIestimator): Gaussian/Kraskov \"\"\" super () . __init__ ( CTest . MI ) self . estimator = estimator compute_dependencies () compute list of dependencies for each target by mutual information analysis Returns: Type Description dict dictonary(TARGET: list SOURCES) Source code in fpcmci/selection_methods/MI.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by mutual information analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" with _suppress_stdout (): data = Data ( self . d . values , dim_order = 'sp' ) # sp = samples(row) x processes(col) network_analysis = MultivariateMI () settings = { 'cmi_estimator' : self . estimator . value , 'max_lag_sources' : self . max_lag , 'min_lag_sources' : self . min_lag , 'alpha_max_stats' : self . alpha , 'alpha_min_stats' : self . alpha , 'alpha_omnibus' : self . alpha , 'alpha_max_seq' : self . alpha , 'verbose' : False } results = network_analysis . analyse_network ( settings = settings , data = data ) for t in results . _single_target . keys (): sel_sources = [ s [ 0 ] for s in results . _single_target [ t ][ 'selected_vars_sources' ]] if sel_sources : sel_sources_lag = [ s [ 1 ] for s in results . _single_target [ t ][ 'selected_vars_sources' ]] sel_sources_score = results . _single_target [ t ][ 'selected_sources_mi' ] sel_sources_pval = results . _single_target [ t ][ 'selected_sources_pval' ] for s , score , pval , lag in zip ( sel_sources , sel_sources_score , sel_sources_pval , sel_sources_lag ): self . _add_dependecies ( self . features [ t ], self . features [ s ], score , pval , lag ) return self . result TE Bases: SelectionMethod Feature selection method based on Trasfer Entropy analysis Source code in fpcmci/selection_methods/TE.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class TE ( SelectionMethod ): \"\"\" Feature selection method based on Trasfer Entropy analysis \"\"\" def __init__ ( self , estimator : TEestimator ): \"\"\" TE class contructor Args: estimator (TEestimator): Gaussian/Kraskov \"\"\" super () . __init__ ( CTest . TE ) self . estimator = estimator def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by transfer entropy analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" multi_network_analysis = MultivariateTE () bi_network_analysis = BivariateMI () settings = { 'cmi_estimator' : self . estimator . value , 'max_lag_sources' : self . max_lag , 'min_lag_sources' : self . min_lag , 'max_lag_target' : self . max_lag , 'min_lag_target' : self . min_lag , 'alpha_max_stats' : self . alpha , 'alpha_min_stats' : self . alpha , 'alpha_omnibus' : self . alpha , 'alpha_max_seq' : self . alpha , 'verbose' : False } CP . info ( \" \\n ##\" ) CP . info ( \"## \" + self . name + \" analysis\" ) CP . info ( \"##\" ) for target in self . data . features : CP . info ( \" \\n ## Target variable: \" + target ) with _suppress_stdout (): t = self . data . features . index ( target ) # Check auto-dependency tmp_d = np . c_ [ self . data . d . values [:, t ], self . data . d . values [:, t ]] data = Data ( tmp_d , dim_order = 'sp' ) # sp = samples(row) x processes(col) res_auto = bi_network_analysis . analyse_single_target ( settings = settings , data = data , target = 0 , sources = 1 ) # Check cross-dependencies data = Data ( self . data . d . values , dim_order = 'sp' ) # sp = samples(row) x processes(col) res_cross = multi_network_analysis . analyse_single_target ( settings = settings , data = data , target = t ) # Auto-dependency handling auto_lag = [ s [ 1 ] for s in res_auto . _single_target [ 0 ][ 'selected_vars_sources' ]] auto_score = res_auto . _single_target [ 0 ][ 'selected_sources_mi' ] auto_pval = res_auto . _single_target [ 0 ][ 'selected_sources_pval' ] if auto_score is not None : for score , pval , lag in zip ( auto_score , auto_pval , auto_lag ): self . _add_dependecies ( self . data . features [ t ], self . data . features [ t ], score , pval , lag ) # Cross-dependencies handling sel_sources = [ s [ 0 ] for s in res_cross . _single_target [ t ][ 'selected_vars_sources' ]] if sel_sources : sel_sources_lag = [ s [ 1 ] for s in res_cross . _single_target [ t ][ 'selected_vars_sources' ]] sel_sources_score = res_cross . _single_target [ t ][ 'selected_sources_te' ] sel_sources_pval = res_cross . _single_target [ t ][ 'selected_sources_pval' ] for s , score , pval , lag in zip ( sel_sources , sel_sources_score , sel_sources_pval , sel_sources_lag ): self . _add_dependecies ( self . data . features [ t ], self . data . features [ s ], score , pval , lag ) if auto_score is None and not sel_sources : CP . info ( \"no sources selected\" ) return self . result __init__ ( estimator ) TE class contructor Parameters: Name Type Description Default estimator TEestimator Gaussian/Kraskov required Source code in fpcmci/selection_methods/TE.py 19 20 21 22 23 24 25 26 27 def __init__ ( self , estimator : TEestimator ): \"\"\" TE class contructor Args: estimator (TEestimator): Gaussian/Kraskov \"\"\" super () . __init__ ( CTest . TE ) self . estimator = estimator compute_dependencies () compute list of dependencies for each target by transfer entropy analysis Returns: Type Description dict dictonary(TARGET: list SOURCES) Source code in fpcmci/selection_methods/TE.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by transfer entropy analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" multi_network_analysis = MultivariateTE () bi_network_analysis = BivariateMI () settings = { 'cmi_estimator' : self . estimator . value , 'max_lag_sources' : self . max_lag , 'min_lag_sources' : self . min_lag , 'max_lag_target' : self . max_lag , 'min_lag_target' : self . min_lag , 'alpha_max_stats' : self . alpha , 'alpha_min_stats' : self . alpha , 'alpha_omnibus' : self . alpha , 'alpha_max_seq' : self . alpha , 'verbose' : False } CP . info ( \" \\n ##\" ) CP . info ( \"## \" + self . name + \" analysis\" ) CP . info ( \"##\" ) for target in self . data . features : CP . info ( \" \\n ## Target variable: \" + target ) with _suppress_stdout (): t = self . data . features . index ( target ) # Check auto-dependency tmp_d = np . c_ [ self . data . d . values [:, t ], self . data . d . values [:, t ]] data = Data ( tmp_d , dim_order = 'sp' ) # sp = samples(row) x processes(col) res_auto = bi_network_analysis . analyse_single_target ( settings = settings , data = data , target = 0 , sources = 1 ) # Check cross-dependencies data = Data ( self . data . d . values , dim_order = 'sp' ) # sp = samples(row) x processes(col) res_cross = multi_network_analysis . analyse_single_target ( settings = settings , data = data , target = t ) # Auto-dependency handling auto_lag = [ s [ 1 ] for s in res_auto . _single_target [ 0 ][ 'selected_vars_sources' ]] auto_score = res_auto . _single_target [ 0 ][ 'selected_sources_mi' ] auto_pval = res_auto . _single_target [ 0 ][ 'selected_sources_pval' ] if auto_score is not None : for score , pval , lag in zip ( auto_score , auto_pval , auto_lag ): self . _add_dependecies ( self . data . features [ t ], self . data . features [ t ], score , pval , lag ) # Cross-dependencies handling sel_sources = [ s [ 0 ] for s in res_cross . _single_target [ t ][ 'selected_vars_sources' ]] if sel_sources : sel_sources_lag = [ s [ 1 ] for s in res_cross . _single_target [ t ][ 'selected_vars_sources' ]] sel_sources_score = res_cross . _single_target [ t ][ 'selected_sources_te' ] sel_sources_pval = res_cross . _single_target [ t ][ 'selected_sources_pval' ] for s , score , pval , lag in zip ( sel_sources , sel_sources_score , sel_sources_pval , sel_sources_lag ): self . _add_dependecies ( self . data . features [ t ], self . data . features [ s ], score , pval , lag ) if auto_score is None and not sel_sources : CP . info ( \"no sources selected\" ) return self . result","title":"Feature Selection Methods"},{"location":"feature_selection_method/#fpcmci.selection_methods.SelectionMethod.SelectionMethod","text":"Bases: ABC SelectionMethod abstract class Source code in fpcmci/selection_methods/SelectionMethod.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 class SelectionMethod ( ABC ): \"\"\" SelectionMethod abstract class \"\"\" def __init__ ( self , ctest ): self . ctest = ctest self . data = None self . alpha = None self . min_lag = None self . max_lag = None self . result = dict () @property def name ( self ): \"\"\" Returns Selection Method name Returns: (str): Selection Method name \"\"\" return self . ctest . value def initialise ( self , data : Data , alpha , min_lag , max_lag ): \"\"\" Initialises the selection method Args: data (Data): Data alpha (float): significance threshold min_lag (int): min lag time max_lag (int): max lag time \"\"\" self . data = data self . alpha = alpha self . min_lag = min_lag self . max_lag = max_lag self . result = { f : list () for f in self . data . features } @abstractmethod def compute_dependencies ( self ) -> dict : \"\"\" abstract method \"\"\" pass def _prepare_ts ( self , target , lag , apply_lag = True , consider_autodep = True ): \"\"\" prepare the dataframe to the analysis Args: target (str): name target var lag (int): lag time to apply apply_lag (bool, optional): True if you want to apply the lag, False otherwise. Defaults to True. Returns: tuple(DataFrame, DataFrame): source and target dataframe \"\"\" if not consider_autodep : if apply_lag : Y = self . data . d [ target ][ lag :] X = self . data . d . loc [:, self . data . d . columns != target ][: - lag ] else : Y = self . data . d [ target ] X = self . data . d . loc [:, self . data . d . columns != target ] else : if apply_lag : Y = self . data . d [ target ][ lag :] X = self . data . d [: - lag ] else : Y = self . data . d [ target ] X = self . data . d return X , Y def _get_sources ( self , t ): \"\"\" Return target sources Args: t (str): target variable name Returns: list(str): list of target sources \"\"\" return [ s [ SOURCE ] for s in self . result [ t ]] def _add_dependecies ( self , t , s , score , pval , lag ): \"\"\" Adds found dependency from source (s) to target (t) specifying the score, pval and the lag Args: t (str): target feature name s (str): source feature name score (float): selection method score pval (float): pval associated to the dependency lag (int): lag time of the dependency \"\"\" self . result [ t ] . append ({ SOURCE : s , SCORE : score , PVAL : pval , LAG : lag }) str_s = \"(\" + s + \" -\" + str ( lag ) + \")\" str_arrow = \" --> \" str_t = \"(\" + t + \")\" str_score = \"|score: \" + \" {:.3f} \" . format ( score ) str_pval = \"|pval: \" + \" {:.3f} \" . format ( pval ) print ( ' {:<20s}{:<10s}{:<10s}{:<20s}{:<20s} ' . format ( str_s , str_arrow , str_t , str_score , str_pval ))","title":"SelectionMethod"},{"location":"feature_selection_method/#fpcmci.selection_methods.SelectionMethod.SelectionMethod.compute_dependencies","text":"abstract method Source code in fpcmci/selection_methods/SelectionMethod.py 67 68 69 70 71 72 @abstractmethod def compute_dependencies ( self ) -> dict : \"\"\" abstract method \"\"\" pass","title":"compute_dependencies()"},{"location":"feature_selection_method/#fpcmci.selection_methods.SelectionMethod.SelectionMethod.initialise","text":"Initialises the selection method Parameters: Name Type Description Default data Data Data required alpha float significance threshold required min_lag int min lag time required max_lag int max lag time required Source code in fpcmci/selection_methods/SelectionMethod.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def initialise ( self , data : Data , alpha , min_lag , max_lag ): \"\"\" Initialises the selection method Args: data (Data): Data alpha (float): significance threshold min_lag (int): min lag time max_lag (int): max lag time \"\"\" self . data = data self . alpha = alpha self . min_lag = min_lag self . max_lag = max_lag self . result = { f : list () for f in self . data . features }","title":"initialise()"},{"location":"feature_selection_method/#fpcmci.selection_methods.SelectionMethod.SelectionMethod.name","text":"Returns Selection Method name Returns: Type Description str Selection Method name Source code in fpcmci/selection_methods/SelectionMethod.py 39 40 41 42 43 44 45 46 47 @property def name ( self ): \"\"\" Returns Selection Method name Returns: (str): Selection Method name \"\"\" return self . ctest . value","title":"name()"},{"location":"feature_selection_method/#fpcmci.selection_methods.Corr.Corr","text":"Bases: SelectionMethod Feature selection method based on Correlation analysis Source code in fpcmci/selection_methods/Corr.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 class Corr ( SelectionMethod ): \"\"\" Feature selection method based on Correlation analysis \"\"\" def __init__ ( self ): \"\"\" Corr contructor class \"\"\" super () . __init__ ( CTest . Corr ) def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by correlation analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" CP . info ( \" \\n ##\" ) CP . info ( \"## \" + self . name + \" analysis\" ) CP . info ( \"##\" ) for lag in range ( self . min_lag , self . max_lag + 1 ): for target in self . data . features : CP . info ( \" \\n ## Target variable: \" + target ) X , Y = self . _prepare_ts ( target , lag ) scores , pval = f_regression ( X , Y ) # Filter on pvalue f = pval < self . alpha # Result of the selection sel_sources , sel_sources_score , sel_sources_pval = X . columns [ f ] . tolist (), scores [ f ] . tolist (), pval [ f ] . tolist () for s , score , pval in zip ( sel_sources , sel_sources_score , sel_sources_pval ): self . _add_dependecies ( target , s , score , pval , lag ) return self . result","title":"Corr"},{"location":"feature_selection_method/#fpcmci.selection_methods.Corr.Corr.__init__","text":"Corr contructor class Source code in fpcmci/selection_methods/Corr.py 9 10 11 12 13 def __init__ ( self ): \"\"\" Corr contructor class \"\"\" super () . __init__ ( CTest . Corr )","title":"__init__()"},{"location":"feature_selection_method/#fpcmci.selection_methods.Corr.Corr.compute_dependencies","text":"compute list of dependencies for each target by correlation analysis Returns: Type Description dict dictonary(TARGET: list SOURCES) Source code in fpcmci/selection_methods/Corr.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by correlation analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" CP . info ( \" \\n ##\" ) CP . info ( \"## \" + self . name + \" analysis\" ) CP . info ( \"##\" ) for lag in range ( self . min_lag , self . max_lag + 1 ): for target in self . data . features : CP . info ( \" \\n ## Target variable: \" + target ) X , Y = self . _prepare_ts ( target , lag ) scores , pval = f_regression ( X , Y ) # Filter on pvalue f = pval < self . alpha # Result of the selection sel_sources , sel_sources_score , sel_sources_pval = X . columns [ f ] . tolist (), scores [ f ] . tolist (), pval [ f ] . tolist () for s , score , pval in zip ( sel_sources , sel_sources_score , sel_sources_pval ): self . _add_dependecies ( target , s , score , pval , lag ) return self . result","title":"compute_dependencies()"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr","text":"Bases: SelectionMethod Feature selection method based on Partial Correlation analysis Source code in fpcmci/selection_methods/ParCorr.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class ParCorr ( SelectionMethod ): \"\"\" Feature selection method based on Partial Correlation analysis \"\"\" def __init__ ( self ): \"\"\" ParCorr class contructor \"\"\" super () . __init__ ( CTest . Corr ) def get_residual ( self , covar , target ): \"\"\" Calculate residual of the target variable obtaining conditioning on the covar variables Args: covar (np.array): conditioning variables target (np.array): target variable Returns: (np.array): residual \"\"\" beta = np . linalg . lstsq ( covar , target , rcond = None )[ 0 ] return target - np . dot ( covar , beta ) def partial_corr ( self , X , Y , Z ): \"\"\" Calculate Partial correlation between X and Y conditioning on Z Args: X (np.array): source candidate variable Y (np.array): target variable Z (np.array): conditioning variable Returns: (float, float): partial correlation, p-value \"\"\" pcorr , pval = stats . pearsonr ( self . get_residual ( Z , X ), self . get_residual ( Z , Y )) return pcorr , pval def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by partial correlation analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" CP . info ( \" \\n ##\" ) CP . info ( \"## \" + self . name + \" analysis\" ) CP . info ( \"##\" ) for lag in range ( self . min_lag , self . max_lag + 1 ): for target in self . data . features : CP . info ( \" \\n ## Target variable: \" + target ) candidates = self . data . features Y = np . array ( self . data . d [ target ][ lag :]) while candidates : tmp_res = None covars = self . _get_sources ( target ) Z = np . array ( self . data . d [ covars ][: - lag ]) for candidate in candidates : X = np . array ( self . data . d [ candidate ][: - lag ]) score , pval = self . partial_corr ( X , Y , Z ) if pval < self . alpha and ( tmp_res is None or abs ( tmp_res [ 1 ]) < abs ( score )): tmp_res = ( candidate , score , pval ) if tmp_res is not None : self . _add_dependecies ( target , tmp_res [ 0 ], tmp_res [ 1 ], tmp_res [ 2 ], lag ) candidates . remove ( tmp_res [ 0 ]) else : break return self . result","title":"ParCorr"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr.__init__","text":"ParCorr class contructor Source code in fpcmci/selection_methods/ParCorr.py 10 11 12 13 14 def __init__ ( self ): \"\"\" ParCorr class contructor \"\"\" super () . __init__ ( CTest . Corr )","title":"__init__()"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr.compute_dependencies","text":"compute list of dependencies for each target by partial correlation analysis Returns: Type Description dict dictonary(TARGET: list SOURCES) Source code in fpcmci/selection_methods/ParCorr.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by partial correlation analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" CP . info ( \" \\n ##\" ) CP . info ( \"## \" + self . name + \" analysis\" ) CP . info ( \"##\" ) for lag in range ( self . min_lag , self . max_lag + 1 ): for target in self . data . features : CP . info ( \" \\n ## Target variable: \" + target ) candidates = self . data . features Y = np . array ( self . data . d [ target ][ lag :]) while candidates : tmp_res = None covars = self . _get_sources ( target ) Z = np . array ( self . data . d [ covars ][: - lag ]) for candidate in candidates : X = np . array ( self . data . d [ candidate ][: - lag ]) score , pval = self . partial_corr ( X , Y , Z ) if pval < self . alpha and ( tmp_res is None or abs ( tmp_res [ 1 ]) < abs ( score )): tmp_res = ( candidate , score , pval ) if tmp_res is not None : self . _add_dependecies ( target , tmp_res [ 0 ], tmp_res [ 1 ], tmp_res [ 2 ], lag ) candidates . remove ( tmp_res [ 0 ]) else : break return self . result","title":"compute_dependencies()"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr.get_residual","text":"Calculate residual of the target variable obtaining conditioning on the covar variables Parameters: Name Type Description Default covar np . array conditioning variables required target np . array target variable required Returns: Type Description np . array residual Source code in fpcmci/selection_methods/ParCorr.py 17 18 19 20 21 22 23 24 25 26 27 28 29 def get_residual ( self , covar , target ): \"\"\" Calculate residual of the target variable obtaining conditioning on the covar variables Args: covar (np.array): conditioning variables target (np.array): target variable Returns: (np.array): residual \"\"\" beta = np . linalg . lstsq ( covar , target , rcond = None )[ 0 ] return target - np . dot ( covar , beta )","title":"get_residual()"},{"location":"feature_selection_method/#fpcmci.selection_methods.ParCorr.ParCorr.partial_corr","text":"Calculate Partial correlation between X and Y conditioning on Z Parameters: Name Type Description Default X np . array source candidate variable required Y np . array target variable required Z np . array conditioning variable required Returns: Type Description float , float partial correlation, p-value Source code in fpcmci/selection_methods/ParCorr.py 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def partial_corr ( self , X , Y , Z ): \"\"\" Calculate Partial correlation between X and Y conditioning on Z Args: X (np.array): source candidate variable Y (np.array): target variable Z (np.array): conditioning variable Returns: (float, float): partial correlation, p-value \"\"\" pcorr , pval = stats . pearsonr ( self . get_residual ( Z , X ), self . get_residual ( Z , Y )) return pcorr , pval","title":"partial_corr()"},{"location":"feature_selection_method/#fpcmci.selection_methods.MI.MI","text":"Bases: SelectionMethod Feature selection method based on Mutual Information analysis Source code in fpcmci/selection_methods/MI.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class MI ( SelectionMethod ): \"\"\" Feature selection method based on Mutual Information analysis \"\"\" def __init__ ( self , estimator : MIestimator ): \"\"\" MI class contructor Args: estimator (MIestimator): Gaussian/Kraskov \"\"\" super () . __init__ ( CTest . MI ) self . estimator = estimator def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by mutual information analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" with _suppress_stdout (): data = Data ( self . d . values , dim_order = 'sp' ) # sp = samples(row) x processes(col) network_analysis = MultivariateMI () settings = { 'cmi_estimator' : self . estimator . value , 'max_lag_sources' : self . max_lag , 'min_lag_sources' : self . min_lag , 'alpha_max_stats' : self . alpha , 'alpha_min_stats' : self . alpha , 'alpha_omnibus' : self . alpha , 'alpha_max_seq' : self . alpha , 'verbose' : False } results = network_analysis . analyse_network ( settings = settings , data = data ) for t in results . _single_target . keys (): sel_sources = [ s [ 0 ] for s in results . _single_target [ t ][ 'selected_vars_sources' ]] if sel_sources : sel_sources_lag = [ s [ 1 ] for s in results . _single_target [ t ][ 'selected_vars_sources' ]] sel_sources_score = results . _single_target [ t ][ 'selected_sources_mi' ] sel_sources_pval = results . _single_target [ t ][ 'selected_sources_pval' ] for s , score , pval , lag in zip ( sel_sources , sel_sources_score , sel_sources_pval , sel_sources_lag ): self . _add_dependecies ( self . features [ t ], self . features [ s ], score , pval , lag ) return self . result","title":"MI"},{"location":"feature_selection_method/#fpcmci.selection_methods.MI.MI.__init__","text":"MI class contructor Parameters: Name Type Description Default estimator MIestimator Gaussian/Kraskov required Source code in fpcmci/selection_methods/MI.py 15 16 17 18 19 20 21 22 23 def __init__ ( self , estimator : MIestimator ): \"\"\" MI class contructor Args: estimator (MIestimator): Gaussian/Kraskov \"\"\" super () . __init__ ( CTest . MI ) self . estimator = estimator","title":"__init__()"},{"location":"feature_selection_method/#fpcmci.selection_methods.MI.MI.compute_dependencies","text":"compute list of dependencies for each target by mutual information analysis Returns: Type Description dict dictonary(TARGET: list SOURCES) Source code in fpcmci/selection_methods/MI.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by mutual information analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" with _suppress_stdout (): data = Data ( self . d . values , dim_order = 'sp' ) # sp = samples(row) x processes(col) network_analysis = MultivariateMI () settings = { 'cmi_estimator' : self . estimator . value , 'max_lag_sources' : self . max_lag , 'min_lag_sources' : self . min_lag , 'alpha_max_stats' : self . alpha , 'alpha_min_stats' : self . alpha , 'alpha_omnibus' : self . alpha , 'alpha_max_seq' : self . alpha , 'verbose' : False } results = network_analysis . analyse_network ( settings = settings , data = data ) for t in results . _single_target . keys (): sel_sources = [ s [ 0 ] for s in results . _single_target [ t ][ 'selected_vars_sources' ]] if sel_sources : sel_sources_lag = [ s [ 1 ] for s in results . _single_target [ t ][ 'selected_vars_sources' ]] sel_sources_score = results . _single_target [ t ][ 'selected_sources_mi' ] sel_sources_pval = results . _single_target [ t ][ 'selected_sources_pval' ] for s , score , pval , lag in zip ( sel_sources , sel_sources_score , sel_sources_pval , sel_sources_lag ): self . _add_dependecies ( self . features [ t ], self . features [ s ], score , pval , lag ) return self . result","title":"compute_dependencies()"},{"location":"feature_selection_method/#fpcmci.selection_methods.TE.TE","text":"Bases: SelectionMethod Feature selection method based on Trasfer Entropy analysis Source code in fpcmci/selection_methods/TE.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class TE ( SelectionMethod ): \"\"\" Feature selection method based on Trasfer Entropy analysis \"\"\" def __init__ ( self , estimator : TEestimator ): \"\"\" TE class contructor Args: estimator (TEestimator): Gaussian/Kraskov \"\"\" super () . __init__ ( CTest . TE ) self . estimator = estimator def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by transfer entropy analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" multi_network_analysis = MultivariateTE () bi_network_analysis = BivariateMI () settings = { 'cmi_estimator' : self . estimator . value , 'max_lag_sources' : self . max_lag , 'min_lag_sources' : self . min_lag , 'max_lag_target' : self . max_lag , 'min_lag_target' : self . min_lag , 'alpha_max_stats' : self . alpha , 'alpha_min_stats' : self . alpha , 'alpha_omnibus' : self . alpha , 'alpha_max_seq' : self . alpha , 'verbose' : False } CP . info ( \" \\n ##\" ) CP . info ( \"## \" + self . name + \" analysis\" ) CP . info ( \"##\" ) for target in self . data . features : CP . info ( \" \\n ## Target variable: \" + target ) with _suppress_stdout (): t = self . data . features . index ( target ) # Check auto-dependency tmp_d = np . c_ [ self . data . d . values [:, t ], self . data . d . values [:, t ]] data = Data ( tmp_d , dim_order = 'sp' ) # sp = samples(row) x processes(col) res_auto = bi_network_analysis . analyse_single_target ( settings = settings , data = data , target = 0 , sources = 1 ) # Check cross-dependencies data = Data ( self . data . d . values , dim_order = 'sp' ) # sp = samples(row) x processes(col) res_cross = multi_network_analysis . analyse_single_target ( settings = settings , data = data , target = t ) # Auto-dependency handling auto_lag = [ s [ 1 ] for s in res_auto . _single_target [ 0 ][ 'selected_vars_sources' ]] auto_score = res_auto . _single_target [ 0 ][ 'selected_sources_mi' ] auto_pval = res_auto . _single_target [ 0 ][ 'selected_sources_pval' ] if auto_score is not None : for score , pval , lag in zip ( auto_score , auto_pval , auto_lag ): self . _add_dependecies ( self . data . features [ t ], self . data . features [ t ], score , pval , lag ) # Cross-dependencies handling sel_sources = [ s [ 0 ] for s in res_cross . _single_target [ t ][ 'selected_vars_sources' ]] if sel_sources : sel_sources_lag = [ s [ 1 ] for s in res_cross . _single_target [ t ][ 'selected_vars_sources' ]] sel_sources_score = res_cross . _single_target [ t ][ 'selected_sources_te' ] sel_sources_pval = res_cross . _single_target [ t ][ 'selected_sources_pval' ] for s , score , pval , lag in zip ( sel_sources , sel_sources_score , sel_sources_pval , sel_sources_lag ): self . _add_dependecies ( self . data . features [ t ], self . data . features [ s ], score , pval , lag ) if auto_score is None and not sel_sources : CP . info ( \"no sources selected\" ) return self . result","title":"TE"},{"location":"feature_selection_method/#fpcmci.selection_methods.TE.TE.__init__","text":"TE class contructor Parameters: Name Type Description Default estimator TEestimator Gaussian/Kraskov required Source code in fpcmci/selection_methods/TE.py 19 20 21 22 23 24 25 26 27 def __init__ ( self , estimator : TEestimator ): \"\"\" TE class contructor Args: estimator (TEestimator): Gaussian/Kraskov \"\"\" super () . __init__ ( CTest . TE ) self . estimator = estimator","title":"__init__()"},{"location":"feature_selection_method/#fpcmci.selection_methods.TE.TE.compute_dependencies","text":"compute list of dependencies for each target by transfer entropy analysis Returns: Type Description dict dictonary(TARGET: list SOURCES) Source code in fpcmci/selection_methods/TE.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def compute_dependencies ( self ): \"\"\" compute list of dependencies for each target by transfer entropy analysis Returns: (dict): dictonary(TARGET: list SOURCES) \"\"\" multi_network_analysis = MultivariateTE () bi_network_analysis = BivariateMI () settings = { 'cmi_estimator' : self . estimator . value , 'max_lag_sources' : self . max_lag , 'min_lag_sources' : self . min_lag , 'max_lag_target' : self . max_lag , 'min_lag_target' : self . min_lag , 'alpha_max_stats' : self . alpha , 'alpha_min_stats' : self . alpha , 'alpha_omnibus' : self . alpha , 'alpha_max_seq' : self . alpha , 'verbose' : False } CP . info ( \" \\n ##\" ) CP . info ( \"## \" + self . name + \" analysis\" ) CP . info ( \"##\" ) for target in self . data . features : CP . info ( \" \\n ## Target variable: \" + target ) with _suppress_stdout (): t = self . data . features . index ( target ) # Check auto-dependency tmp_d = np . c_ [ self . data . d . values [:, t ], self . data . d . values [:, t ]] data = Data ( tmp_d , dim_order = 'sp' ) # sp = samples(row) x processes(col) res_auto = bi_network_analysis . analyse_single_target ( settings = settings , data = data , target = 0 , sources = 1 ) # Check cross-dependencies data = Data ( self . data . d . values , dim_order = 'sp' ) # sp = samples(row) x processes(col) res_cross = multi_network_analysis . analyse_single_target ( settings = settings , data = data , target = t ) # Auto-dependency handling auto_lag = [ s [ 1 ] for s in res_auto . _single_target [ 0 ][ 'selected_vars_sources' ]] auto_score = res_auto . _single_target [ 0 ][ 'selected_sources_mi' ] auto_pval = res_auto . _single_target [ 0 ][ 'selected_sources_pval' ] if auto_score is not None : for score , pval , lag in zip ( auto_score , auto_pval , auto_lag ): self . _add_dependecies ( self . data . features [ t ], self . data . features [ t ], score , pval , lag ) # Cross-dependencies handling sel_sources = [ s [ 0 ] for s in res_cross . _single_target [ t ][ 'selected_vars_sources' ]] if sel_sources : sel_sources_lag = [ s [ 1 ] for s in res_cross . _single_target [ t ][ 'selected_vars_sources' ]] sel_sources_score = res_cross . _single_target [ t ][ 'selected_sources_te' ] sel_sources_pval = res_cross . _single_target [ t ][ 'selected_sources_pval' ] for s , score , pval , lag in zip ( sel_sources , sel_sources_score , sel_sources_pval , sel_sources_lag ): self . _add_dependecies ( self . data . features [ t ], self . data . features [ s ], score , pval , lag ) if auto_score is None and not sel_sources : CP . info ( \"no sources selected\" ) return self . result","title":"compute_dependencies()"},{"location":"fpcmci/","text":"FSelector FSelector class. FSelector is a causal feature selector framework for large-scale time series datasets. Sarting from a Data object and it selects the main features responsible for the evolution of the analysed system. Based on the selected features, the framework outputs a causal model. Source code in fpcmci/FSelector.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 class FSelector (): \"\"\" FSelector class. FSelector is a causal feature selector framework for large-scale time series datasets. Sarting from a Data object and it selects the main features responsible for the evolution of the analysed system. Based on the selected features, the framework outputs a causal model. \"\"\" def __init__ ( self , data : Data , min_lag , max_lag , sel_method : SelectionMethod , val_condtest : CondIndTest , verbosity : CPLevel , alpha = 0.05 , resfolder = None , neglect_only_autodep = False ): \"\"\" FSelector class contructor Args: data (Data): data to analyse min_lag (int): minimum time lag max_lag (int): maximum time lag sel_method (SelectionMethod): selection method val_condtest (CondIndTest): validation method verbosity (CPLevel): verbosity level alpha (float, optional): significance level. Defaults to 0.05. resfolder (string, optional): result folder to create. Defaults to None. neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False. \"\"\" self . data = data self . alpha = alpha self . min_lag = min_lag self . max_lag = max_lag self . sel_method = sel_method self . dependencies = None self . result = None self . neglect_only_autodep = neglect_only_autodep self . dependency_path = None if resfolder is not None : utils . create_results_folder () logpath , self . dependency_path = utils . get_selectorpath ( resfolder ) sys . stdout = Logger ( logpath ) self . validator = FValidator ( data , alpha , min_lag , max_lag , val_condtest , resfolder , verbosity ) CP . set_verbosity ( verbosity ) def run_selector ( self ): \"\"\" Run selection method \"\"\" CP . info ( \" \\n \" ) CP . info ( DASH ) CP . info ( \"Selecting relevant features among: \" + str ( self . data . features )) CP . info ( \"Selection method: \" + self . sel_method . name ) CP . info ( \"Significance level: \" + str ( self . alpha )) CP . info ( \"Max lag time: \" + str ( self . max_lag )) CP . info ( \"Min lag time: \" + str ( self . min_lag )) CP . info ( \"Data length: \" + str ( self . data . T )) self . sel_method . initialise ( self . data , self . alpha , self . min_lag , self . max_lag ) self . dependencies = self . sel_method . compute_dependencies () self . o_dependecies = copy . deepcopy ( self . dependencies ) def run_validator ( self ): \"\"\" Run Validator Returns: list(str): list of selected variable names \"\"\" CP . info ( \"Significance level: \" + str ( self . alpha )) CP . info ( \"Max lag time: \" + str ( self . max_lag )) CP . info ( \"Min lag time: \" + str ( self . min_lag )) CP . info ( \"Data length: \" + str ( self . data . T )) # causal model self . validator . data = self . data pcmci_result = self . validator . run () self . result = self . data . features self . save_validator_res () return self . result def run ( self ): \"\"\" Run Selector and Validator without feedback Returns: list(str): list of selected variable names \"\"\" self . run_selector () # list of selected features based on dependencies tmp_sel_features = self . get_selected_features () if not tmp_sel_features : return self . result # shrink dataframe d and dependencies by the selector result self . shrink ( tmp_sel_features ) # selected links to check by the validator selected_links = self . __get_selected_links () # causal model on selected links self . validator . data = self . data pcmci_result = self . validator . run ( selected_links ) self . __apply_validator_result ( pcmci_result ) self . result = self . get_selected_features () # shrink dataframe d and dependencies by the validator result self . shrink ( self . result ) self . save_validator_res () CP . info ( \" \\n Feature selected: \" + str ( self . result )) return self . result def shrink ( self , sel_features ): \"\"\" Wrapper in order to shrink data.d and dependencies Args: sel_features (list(str)): list of selected features \"\"\" self . data . shrink ( sel_features ) self . __shrink_dependencies () def save_validator_res ( self ): \"\"\" Saves dag plot if resfolder has been set otherwise it shows the figure \"\"\" if self . result : self . validator . save_result () else : CP . warning ( \"Result impossible to save: no feature selected\" ) def dag ( self , node_layout = 'dot' , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , node_color = 'orange' , edge_color = 'grey' , font_size = 12 , show_edge_labels = True , label_type = LabelType . Lag ): \"\"\" Saves dag plot if resfolder has been set otherwise it shows the figure Args: node_layout (str, optional): Node layout. Defaults to 'dot. min_width (int, optional): minimum linewidth. Defaults to 1. max_width (int, optional): maximum linewidth. Defaults to 5. min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. node_size (int, optional): node size. Defaults to 8. node_color (str, optional): node color. Defaults to 'orange'. edge_color (str, optional): edge color. Defaults to 'grey'. font_size (int, optional): font size. Defaults to 12. show_edge_labels (bool, optional): bit to show the time-lag label of the dependency on the edge. Defaults to True. \"\"\" if self . result : self . validator . build_dag ( node_layout , min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size , show_edge_labels , label_type ) else : CP . warning ( \"Dag impossible to create: no feature selected\" ) def timeseries_dag ( self , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , font_size = 12 , node_color = 'orange' , edge_color = 'grey' ): \"\"\" Saves timeseries dag plot if resfolder has been set otherwise it shows the figure Args: min_width (int, optional): minimum linewidth. Defaults to 1. max_width (int, optional): maximum linewidth. Defaults to 5. min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. node_size (int, optional): node size. Defaults to 8. node_color (str, optional): node color. Defaults to 'orange'. edge_color (str, optional): edge color. Defaults to 'grey'. font_size (int, optional): font size. Defaults to 12. \"\"\" if self . result : self . validator . build_ts_dag ( min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size ) else : CP . warning ( \"Timeseries dag impossible to create: no feature selected\" ) def get_selected_features ( self ): \"\"\" Defines the list of selected variables for d Returns: list(str): list of selected variable names \"\"\" f_list = list () for t in self . dependencies : sources_t = self . __get_dependencies_for_target ( t ) if self . neglect_only_autodep and self . __is_only_autodep ( sources_t , t ): sources_t . remove ( t ) if sources_t : sources_t . append ( t ) f_list = list ( set ( f_list + sources_t )) res = [ f for f in self . data . features if f in f_list ] return res def show_dependencies ( self ): \"\"\" Saves dependencies graph if resfolder is set otherwise it shows the figure \"\"\" # FIXME: LAG not considered dependencies_matrix = self . __get_dependencies_matrix () fig , ax = plt . subplots () im = ax . imshow ( dependencies_matrix , cmap = plt . cm . Greens , interpolation = 'nearest' , vmin = 0 , vmax = 1 , origin = 'lower' ) fig . colorbar ( im , orientation = 'vertical' , label = \"score\" ) plt . xlabel ( \"Sources\" ) plt . ylabel ( \"Targets\" ) plt . xticks ( ticks = range ( 0 , self . data . orig_N ), labels = self . data . orig_pretty_features , fontsize = 8 ) plt . yticks ( ticks = range ( 0 , self . data . orig_N ), labels = self . data . orig_pretty_features , fontsize = 8 ) plt . title ( \"Dependencies\" ) if self . dependency_path is not None : plt . savefig ( self . dependency_path , dpi = 300 ) else : plt . show () def print_dependencies ( self ): \"\"\" Print dependencies found by the selector \"\"\" for t in self . o_dependecies : print () print () print ( DASH ) print ( \"Target\" , t ) print ( DASH ) print ( ' {:<10s}{:>15s}{:>15s}{:>15s} ' . format ( 'SOURCE' , 'SCORE' , 'PVAL' , 'LAG' )) print ( DASH ) for s in self . o_dependecies [ t ]: print ( ' {:<10s}{:>15.3f}{:>15.3f}{:>15d} ' . format ( s [ SOURCE ], s [ SCORE ], s [ PVAL ], s [ LAG ])) def load_result ( self , res_path ): with open ( res_path , 'rb' ) as f : self . validator . result = pickle . load ( f ) def __shrink_dependencies ( self ): \"\"\" Shrinks dependencies based on the selected features \"\"\" difference_set = self . dependencies . keys () - self . data . features for d in difference_set : del self . dependencies [ d ] def __get_dependencies_for_target ( self , t ): \"\"\" Returns list of sources for a specified target Args: t (str): target variable name Returns: list(str): list of sources for target t \"\"\" return [ s [ SOURCE ] for s in self . dependencies [ t ]] def __is_only_autodep ( self , sources , t ): \"\"\" Returns list of sources for a specified target Args: sources (list(str)): list of sources for the selected target t (str): target variable name Returns: (bool): True if sources list contains only the target. False otherwise \"\"\" if len ( sources ) == 1 and sources [ 0 ] == t : return True return False def __get_dependencies_matrix ( self ): \"\"\" Returns a matrix composed by scores for each target Returns: (np.array): score matrix \"\"\" dep_mat = list () for t in self . o_dependecies : dep_vet = [ 0 ] * self . data . orig_N for s in self . o_dependecies [ t ]: dep_vet [ self . data . orig_features . index ( s [ SOURCE ])] = s [ SCORE ] dep_mat . append ( dep_vet ) dep_mat = np . array ( dep_mat ) inf_mask = np . isinf ( dep_mat ) neginf_mask = np . isneginf ( dep_mat ) max_dep_mat = np . max ( dep_mat [( dep_mat != - np . inf ) & ( dep_mat != np . inf )]) min_dep_mat = np . min ( dep_mat [( dep_mat != - np . inf ) & ( dep_mat != np . inf )]) dep_mat [ inf_mask ] = max_dep_mat dep_mat [ neginf_mask ] = min_dep_mat dep_mat = ( dep_mat - min_dep_mat ) / ( max_dep_mat - min_dep_mat ) return dep_mat def __get_selected_links ( self ): \"\"\" Return selected links found by the selector in this form: {0: [(0,-1), (2,-1)]} Returns: (dict): selected links \"\"\" sel_links = { self . data . features . index ( f ): list () for f in self . data . features } for t in self . dependencies : # add links for s in self . dependencies [ t ]: sel_links [ self . data . features . index ( t )] . append (( self . data . features . index ( s [ SOURCE ]), - s [ LAG ])) return sel_links def __apply_validator_result ( self , causal_model ): \"\"\" Exclude dependencies based on validator result \"\"\" list_diffs = list () tmp_dependencies = copy . deepcopy ( self . dependencies ) for t in tmp_dependencies : for s in tmp_dependencies [ t ]: if ( self . data . features . index ( s [ SOURCE ]), - s [ LAG ]) not in causal_model [ self . data . features . index ( t )]: list_diffs . append (( s [ SOURCE ], str ( s [ LAG ]), t )) self . dependencies [ t ] . remove ( s ) if list_diffs : CP . debug ( DASH ) CP . debug ( \"Difference(s)\" ) CP . debug ( DASH ) for diff in list_diffs : CP . debug ( \"Removing (\" + diff [ 0 ] + \" -\" + diff [ 1 ] + \") --> (\" + diff [ 2 ] + \")\" ) __apply_validator_result ( causal_model ) Exclude dependencies based on validator result Source code in fpcmci/FSelector.py 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def __apply_validator_result ( self , causal_model ): \"\"\" Exclude dependencies based on validator result \"\"\" list_diffs = list () tmp_dependencies = copy . deepcopy ( self . dependencies ) for t in tmp_dependencies : for s in tmp_dependencies [ t ]: if ( self . data . features . index ( s [ SOURCE ]), - s [ LAG ]) not in causal_model [ self . data . features . index ( t )]: list_diffs . append (( s [ SOURCE ], str ( s [ LAG ]), t )) self . dependencies [ t ] . remove ( s ) if list_diffs : CP . debug ( DASH ) CP . debug ( \"Difference(s)\" ) CP . debug ( DASH ) for diff in list_diffs : CP . debug ( \"Removing (\" + diff [ 0 ] + \" -\" + diff [ 1 ] + \") --> (\" + diff [ 2 ] + \")\" ) __get_dependencies_for_target ( t ) Returns list of sources for a specified target Parameters: Name Type Description Default t str target variable name required Returns: Name Type Description list str list of sources for target t Source code in fpcmci/FSelector.py 319 320 321 322 323 324 325 326 327 328 329 def __get_dependencies_for_target ( self , t ): \"\"\" Returns list of sources for a specified target Args: t (str): target variable name Returns: list(str): list of sources for target t \"\"\" return [ s [ SOURCE ] for s in self . dependencies [ t ]] __get_dependencies_matrix () Returns a matrix composed by scores for each target Returns: Type Description np . array score matrix Source code in fpcmci/FSelector.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 def __get_dependencies_matrix ( self ): \"\"\" Returns a matrix composed by scores for each target Returns: (np.array): score matrix \"\"\" dep_mat = list () for t in self . o_dependecies : dep_vet = [ 0 ] * self . data . orig_N for s in self . o_dependecies [ t ]: dep_vet [ self . data . orig_features . index ( s [ SOURCE ])] = s [ SCORE ] dep_mat . append ( dep_vet ) dep_mat = np . array ( dep_mat ) inf_mask = np . isinf ( dep_mat ) neginf_mask = np . isneginf ( dep_mat ) max_dep_mat = np . max ( dep_mat [( dep_mat != - np . inf ) & ( dep_mat != np . inf )]) min_dep_mat = np . min ( dep_mat [( dep_mat != - np . inf ) & ( dep_mat != np . inf )]) dep_mat [ inf_mask ] = max_dep_mat dep_mat [ neginf_mask ] = min_dep_mat dep_mat = ( dep_mat - min_dep_mat ) / ( max_dep_mat - min_dep_mat ) return dep_mat __get_selected_links () Return selected links found by the selector in this form: {0: [(0,-1), (2,-1)]} Returns: Type Description dict selected links Source code in fpcmci/FSelector.py 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 def __get_selected_links ( self ): \"\"\" Return selected links found by the selector in this form: {0: [(0,-1), (2,-1)]} Returns: (dict): selected links \"\"\" sel_links = { self . data . features . index ( f ): list () for f in self . data . features } for t in self . dependencies : # add links for s in self . dependencies [ t ]: sel_links [ self . data . features . index ( t )] . append (( self . data . features . index ( s [ SOURCE ]), - s [ LAG ])) return sel_links __init__ ( data , min_lag , max_lag , sel_method , val_condtest , verbosity , alpha = 0.05 , resfolder = None , neglect_only_autodep = False ) FSelector class contructor Parameters: Name Type Description Default data Data data to analyse required min_lag int minimum time lag required max_lag int maximum time lag required sel_method SelectionMethod selection method required val_condtest CondIndTest validation method required verbosity CPLevel verbosity level required alpha float significance level. Defaults to 0.05. 0.05 resfolder string result folder to create. Defaults to None. None neglect_only_autodep bool Bit for neglecting variables with only autodependency. Defaults to False. False Source code in fpcmci/FSelector.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , data : Data , min_lag , max_lag , sel_method : SelectionMethod , val_condtest : CondIndTest , verbosity : CPLevel , alpha = 0.05 , resfolder = None , neglect_only_autodep = False ): \"\"\" FSelector class contructor Args: data (Data): data to analyse min_lag (int): minimum time lag max_lag (int): maximum time lag sel_method (SelectionMethod): selection method val_condtest (CondIndTest): validation method verbosity (CPLevel): verbosity level alpha (float, optional): significance level. Defaults to 0.05. resfolder (string, optional): result folder to create. Defaults to None. neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False. \"\"\" self . data = data self . alpha = alpha self . min_lag = min_lag self . max_lag = max_lag self . sel_method = sel_method self . dependencies = None self . result = None self . neglect_only_autodep = neglect_only_autodep self . dependency_path = None if resfolder is not None : utils . create_results_folder () logpath , self . dependency_path = utils . get_selectorpath ( resfolder ) sys . stdout = Logger ( logpath ) self . validator = FValidator ( data , alpha , min_lag , max_lag , val_condtest , resfolder , verbosity ) CP . set_verbosity ( verbosity ) __is_only_autodep ( sources , t ) Returns list of sources for a specified target Parameters: Name Type Description Default sources list(str list of sources for the selected target required t str target variable name required Returns: Type Description bool True if sources list contains only the target. False otherwise Source code in fpcmci/FSelector.py 332 333 334 335 336 337 338 339 340 341 342 343 344 def __is_only_autodep ( self , sources , t ): \"\"\" Returns list of sources for a specified target Args: sources (list(str)): list of sources for the selected target t (str): target variable name Returns: (bool): True if sources list contains only the target. False otherwise \"\"\" if len ( sources ) == 1 and sources [ 0 ] == t : return True return False __shrink_dependencies () Shrinks dependencies based on the selected features Source code in fpcmci/FSelector.py 311 312 313 314 315 316 def __shrink_dependencies ( self ): \"\"\" Shrinks dependencies based on the selected features \"\"\" difference_set = self . dependencies . keys () - self . data . features for d in difference_set : del self . dependencies [ d ] dag ( node_layout = 'dot' , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , node_color = 'orange' , edge_color = 'grey' , font_size = 12 , show_edge_labels = True , label_type = LabelType . Lag ) Saves dag plot if resfolder has been set otherwise it shows the figure Parameters: Name Type Description Default node_layout str Node layout. Defaults to 'dot. 'dot' min_width int minimum linewidth. Defaults to 1. 1 max_width int maximum linewidth. Defaults to 5. 5 min_score int minimum score range. Defaults to 0. 0 max_score int maximum score range. Defaults to 1. 1 node_size int node size. Defaults to 8. 8 node_color str node color. Defaults to 'orange'. 'orange' edge_color str edge color. Defaults to 'grey'. 'grey' font_size int font size. Defaults to 12. 12 show_edge_labels bool bit to show the time-lag label of the dependency on the edge. Defaults to True. True Source code in fpcmci/FSelector.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def dag ( self , node_layout = 'dot' , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , node_color = 'orange' , edge_color = 'grey' , font_size = 12 , show_edge_labels = True , label_type = LabelType . Lag ): \"\"\" Saves dag plot if resfolder has been set otherwise it shows the figure Args: node_layout (str, optional): Node layout. Defaults to 'dot. min_width (int, optional): minimum linewidth. Defaults to 1. max_width (int, optional): maximum linewidth. Defaults to 5. min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. node_size (int, optional): node size. Defaults to 8. node_color (str, optional): node color. Defaults to 'orange'. edge_color (str, optional): edge color. Defaults to 'grey'. font_size (int, optional): font size. Defaults to 12. show_edge_labels (bool, optional): bit to show the time-lag label of the dependency on the edge. Defaults to True. \"\"\" if self . result : self . validator . build_dag ( node_layout , min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size , show_edge_labels , label_type ) else : CP . warning ( \"Dag impossible to create: no feature selected\" ) get_selected_features () Defines the list of selected variables for d Returns: Name Type Description list str list of selected variable names Source code in fpcmci/FSelector.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def get_selected_features ( self ): \"\"\" Defines the list of selected variables for d Returns: list(str): list of selected variable names \"\"\" f_list = list () for t in self . dependencies : sources_t = self . __get_dependencies_for_target ( t ) if self . neglect_only_autodep and self . __is_only_autodep ( sources_t , t ): sources_t . remove ( t ) if sources_t : sources_t . append ( t ) f_list = list ( set ( f_list + sources_t )) res = [ f for f in self . data . features if f in f_list ] return res print_dependencies () Print dependencies found by the selector Source code in fpcmci/FSelector.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def print_dependencies ( self ): \"\"\" Print dependencies found by the selector \"\"\" for t in self . o_dependecies : print () print () print ( DASH ) print ( \"Target\" , t ) print ( DASH ) print ( ' {:<10s}{:>15s}{:>15s}{:>15s} ' . format ( 'SOURCE' , 'SCORE' , 'PVAL' , 'LAG' )) print ( DASH ) for s in self . o_dependecies [ t ]: print ( ' {:<10s}{:>15.3f}{:>15.3f}{:>15d} ' . format ( s [ SOURCE ], s [ SCORE ], s [ PVAL ], s [ LAG ])) run () Run Selector and Validator without feedback Returns: Name Type Description list str list of selected variable names Source code in fpcmci/FSelector.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def run ( self ): \"\"\" Run Selector and Validator without feedback Returns: list(str): list of selected variable names \"\"\" self . run_selector () # list of selected features based on dependencies tmp_sel_features = self . get_selected_features () if not tmp_sel_features : return self . result # shrink dataframe d and dependencies by the selector result self . shrink ( tmp_sel_features ) # selected links to check by the validator selected_links = self . __get_selected_links () # causal model on selected links self . validator . data = self . data pcmci_result = self . validator . run ( selected_links ) self . __apply_validator_result ( pcmci_result ) self . result = self . get_selected_features () # shrink dataframe d and dependencies by the validator result self . shrink ( self . result ) self . save_validator_res () CP . info ( \" \\n Feature selected: \" + str ( self . result )) return self . result run_selector () Run selection method Source code in fpcmci/FSelector.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def run_selector ( self ): \"\"\" Run selection method \"\"\" CP . info ( \" \\n \" ) CP . info ( DASH ) CP . info ( \"Selecting relevant features among: \" + str ( self . data . features )) CP . info ( \"Selection method: \" + self . sel_method . name ) CP . info ( \"Significance level: \" + str ( self . alpha )) CP . info ( \"Max lag time: \" + str ( self . max_lag )) CP . info ( \"Min lag time: \" + str ( self . min_lag )) CP . info ( \"Data length: \" + str ( self . data . T )) self . sel_method . initialise ( self . data , self . alpha , self . min_lag , self . max_lag ) self . dependencies = self . sel_method . compute_dependencies () self . o_dependecies = copy . deepcopy ( self . dependencies ) run_validator () Run Validator Returns: Name Type Description list str list of selected variable names Source code in fpcmci/FSelector.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def run_validator ( self ): \"\"\" Run Validator Returns: list(str): list of selected variable names \"\"\" CP . info ( \"Significance level: \" + str ( self . alpha )) CP . info ( \"Max lag time: \" + str ( self . max_lag )) CP . info ( \"Min lag time: \" + str ( self . min_lag )) CP . info ( \"Data length: \" + str ( self . data . T )) # causal model self . validator . data = self . data pcmci_result = self . validator . run () self . result = self . data . features self . save_validator_res () return self . result save_validator_res () Saves dag plot if resfolder has been set otherwise it shows the figure Source code in fpcmci/FSelector.py 158 159 160 161 162 163 164 165 def save_validator_res ( self ): \"\"\" Saves dag plot if resfolder has been set otherwise it shows the figure \"\"\" if self . result : self . validator . save_result () else : CP . warning ( \"Result impossible to save: no feature selected\" ) show_dependencies () Saves dependencies graph if resfolder is set otherwise it shows the figure Source code in fpcmci/FSelector.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def show_dependencies ( self ): \"\"\" Saves dependencies graph if resfolder is set otherwise it shows the figure \"\"\" # FIXME: LAG not considered dependencies_matrix = self . __get_dependencies_matrix () fig , ax = plt . subplots () im = ax . imshow ( dependencies_matrix , cmap = plt . cm . Greens , interpolation = 'nearest' , vmin = 0 , vmax = 1 , origin = 'lower' ) fig . colorbar ( im , orientation = 'vertical' , label = \"score\" ) plt . xlabel ( \"Sources\" ) plt . ylabel ( \"Targets\" ) plt . xticks ( ticks = range ( 0 , self . data . orig_N ), labels = self . data . orig_pretty_features , fontsize = 8 ) plt . yticks ( ticks = range ( 0 , self . data . orig_N ), labels = self . data . orig_pretty_features , fontsize = 8 ) plt . title ( \"Dependencies\" ) if self . dependency_path is not None : plt . savefig ( self . dependency_path , dpi = 300 ) else : plt . show () shrink ( sel_features ) Wrapper in order to shrink data.d and dependencies Parameters: Name Type Description Default sel_features list(str list of selected features required Source code in fpcmci/FSelector.py 147 148 149 150 151 152 153 154 155 def shrink ( self , sel_features ): \"\"\" Wrapper in order to shrink data.d and dependencies Args: sel_features (list(str)): list of selected features \"\"\" self . data . shrink ( sel_features ) self . __shrink_dependencies () timeseries_dag ( min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , font_size = 12 , node_color = 'orange' , edge_color = 'grey' ) Saves timeseries dag plot if resfolder has been set otherwise it shows the figure Parameters: Name Type Description Default min_width int minimum linewidth. Defaults to 1. 1 max_width int maximum linewidth. Defaults to 5. 5 min_score int minimum score range. Defaults to 0. 0 max_score int maximum score range. Defaults to 1. 1 node_size int node size. Defaults to 8. 8 node_color str node color. Defaults to 'orange'. 'orange' edge_color str edge color. Defaults to 'grey'. 'grey' font_size int font size. Defaults to 12. 12 Source code in fpcmci/FSelector.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def timeseries_dag ( self , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , font_size = 12 , node_color = 'orange' , edge_color = 'grey' ): \"\"\" Saves timeseries dag plot if resfolder has been set otherwise it shows the figure Args: min_width (int, optional): minimum linewidth. Defaults to 1. max_width (int, optional): maximum linewidth. Defaults to 5. min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. node_size (int, optional): node size. Defaults to 8. node_color (str, optional): node color. Defaults to 'orange'. edge_color (str, optional): edge color. Defaults to 'grey'. font_size (int, optional): font size. Defaults to 12. \"\"\" if self . result : self . validator . build_ts_dag ( min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size ) else : CP . warning ( \"Timeseries dag impossible to create: no feature selected\" ) FValidator FValidator class. FValidator works with FSelector in order to find the causal model starting from a prefixed set of variables and links. Source code in fpcmci/FValidator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 class FValidator (): \"\"\" FValidator class. FValidator works with FSelector in order to find the causal model starting from a prefixed set of variables and links. \"\"\" def __init__ ( self , data : Data , alpha , min_lag , max_lag , val_condtest : CondIndTest , resfolder , verbosity : CPLevel ): \"\"\" Validator class constructor Args: data (Data): data to analyse alpha (float): significance level min_lag (int): minimum time lag max_lag (int): maximum time lag val_condtest (CondIndTest): validation method resfolder (str): result folder. If None then the results are not saved. verbosity (CPLevel): verbosity level \"\"\" self . data = data self . alpha = alpha self . min_lag = min_lag self . max_lag = max_lag self . result = None self . val_method = None self . val_condtest = val_condtest self . verbosity = verbosity . value self . respath = None self . dag_path = None self . ts_dag_path = None if resfolder is not None : self . respath , self . dag_path , self . ts_dag_path = utils . get_validatorpaths ( resfolder ) def run ( self , selected_links = None ): \"\"\" Run causal discovery algorithm Returns: (dict): estimated causal model \"\"\" CP . info ( ' \\n ' ) CP . info ( DASH ) CP . info ( \"Running Causal Discovery Algorithm\" ) # build tigramite dataset vector = np . vectorize ( float ) data = vector ( self . data . d ) dataframe = pp . DataFrame ( data = data , var_names = self . data . pretty_features ) # init and run pcmci self . val_method = PCMCI ( dataframe = dataframe , cond_ind_test = self . val_condtest , verbosity = self . verbosity ) self . result = self . val_method . run_pcmci ( selected_links = selected_links , tau_max = self . max_lag , tau_min = self . min_lag , pc_alpha = 0.05 ) self . result [ 'var_names' ] = self . data . pretty_features # apply significance level self . result [ 'graph' ] = self . __apply_alpha () return self . __return_parents_dict () def build_ts_dag ( self , min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size ): \"\"\" Saves timeseries dag plot if resfolder is set otherwise it shows the figure Args: min_width (int): minimum linewidt max_width (int): maximum linewidth min_score (int): minimum score range max_score (int): maximum score range node_size (int): node size node_color (str): node color edge_color (str): edge color font_size (int): font size \"\"\" # convert to dictionary res = self . __PCMCIres_converter () # filter only dependencies tmp_res = { k : res [ k ] for k in self . data . pretty_features } res = tmp_res ts_dag ( res , tau = self . max_lag , min_width = min_width , max_width = max_width , min_score = min_score , max_score = max_score , node_size = node_size , node_color = node_color , edge_color = edge_color , font_size = font_size , save_name = self . ts_dag_path ) def build_dag ( self , node_layout , min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size , show_edge_labels , label_type ): \"\"\" Saves dag plot if resfolder is set otherwise it shows the figure Args: node_layout (str): node_layout min_width (int): minimum linewidth max_width (int): maximum linewidth min_score (int): minimum score range max_score (int): maximum score range node_size (int): node size node_color (str): node color edge_color (str): edge color font_size (int): font size show_edge_labels (bool): bit to show the time-lag label of the dependency on the edge \"\"\" # convert to dictionary res = self . __PCMCIres_converter () # filter only dependencies tmp_res = { k : res [ k ] for k in self . data . pretty_features } res = tmp_res dag ( res , node_layout = node_layout , min_width = min_width , max_width = max_width , min_score = min_score , max_score = max_score , node_size = node_size , font_size = font_size , node_color = node_color , edge_color = edge_color , show_edge_labels = show_edge_labels , label_type = label_type , save_name = self . dag_path ) def save_result ( self ): \"\"\" Save causal discovery results as pickle file if resfolder is set \"\"\" if self . respath is not None : res = copy . deepcopy ( self . result ) res [ 'alpha' ] = self . alpha res [ 'var_names' ] = self . data . pretty_features res [ 'dag_path' ] = self . dag_path res [ 'ts_dag_path' ] = self . ts_dag_path with open ( self . respath , 'wb' ) as resfile : pickle . dump ( res , resfile ) def __return_parents_dict ( self ): \"\"\" Returns dictionary of parents sorted by val_matrix filtered by alpha Returns: (dict): Dictionary of form {0:[(0, -1), (3, -2), ...], 1:[], ...} containing estimated parents. \"\"\" graph = self . result [ 'graph' ] val_matrix = self . result [ 'val_matrix' ] p_matrix = self . result [ 'p_matrix' ] # Initialize the return value parents_dict = dict () for j in range ( self . data . N ): # Get the good links good_links = np . argwhere ( graph [:, j , 1 :] == \"-->\" ) # Build a dictionary from these links to their values links = {( i , - tau - 1 ): np . abs ( val_matrix [ i , j , abs ( tau ) + 1 ]) for i , tau in good_links if p_matrix [ i , j , abs ( tau ) + 1 ] <= self . alpha } # Sort by value parents_dict [ j ] = sorted ( links , key = links . get , reverse = True ) return parents_dict def __PCMCIres_converter ( self ): \"\"\" Re-elaborates the PCMCI result in a new dictionary Returns: (dict): pcmci result re-elaborated \"\"\" res_dict = { f : list () for f in self . result [ 'var_names' ]} N , lags = self . result [ 'graph' ][ 0 ] . shape for s in range ( len ( self . result [ 'graph' ])): for t in range ( N ): for lag in range ( lags ): if self . result [ 'graph' ][ s ][ t , lag ] == '-->' : res_dict [ self . result [ 'var_names' ][ t ]] . append ({ SOURCE : self . result [ 'var_names' ][ s ], SCORE : self . result [ 'val_matrix' ][ s ][ t , lag ], PVAL : self . result [ 'p_matrix' ][ s ][ t , lag ], LAG : lag }) return res_dict def __apply_alpha ( self ): \"\"\" Applies alpha threshold to the pcmci result Returns: (ndarray): graph filtered by alpha \"\"\" mask = np . ones ( self . result [ 'p_matrix' ] . shape , dtype = 'bool' ) # Set all p-values of absent links to 1. self . result [ 'p_matrix' ][ mask == False ] == 1. # Threshold p_matrix to get graph graph_bool = self . result [ 'p_matrix' ] <= self . alpha # Convert to string graph representation graph = self . __convert_to_string_graph ( graph_bool ) return graph def __convert_to_string_graph ( self , graph_bool ): \"\"\" Converts the 0,1-based graph returned by PCMCI to a string array with links '-->' Args: graph_bool (array): 0,1-based graph array output by PCMCI Returns: (array): graph as string array with links '-->'. \"\"\" graph = np . zeros ( graph_bool . shape , dtype = '<U3' ) graph [:] = \"\" # Lagged links graph [:,:, 1 :][ graph_bool [:,:, 1 :] == 1 ] = \"-->\" # Unoriented contemporaneous links graph [:,:, 0 ][ np . logical_and ( graph_bool [:,:, 0 ] == 1 , graph_bool [:,:, 0 ] . T == 1 )] = \"o-o\" # Conflicting contemporaneous links graph [:,:, 0 ][ np . logical_and ( graph_bool [:,:, 0 ] == 2 , graph_bool [:,:, 0 ] . T == 2 )] = \"x-x\" # Directed contemporaneous links for ( i , j ) in zip ( * np . where ( np . logical_and ( graph_bool [:,:, 0 ] == 1 , graph_bool [:,:, 0 ] . T == 0 ))): graph [ i , j , 0 ] = \"-->\" graph [ j , i , 0 ] = \"<--\" return graph __PCMCIres_converter () Re-elaborates the PCMCI result in a new dictionary Returns: Type Description dict pcmci result re-elaborated Source code in fpcmci/FValidator.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def __PCMCIres_converter ( self ): \"\"\" Re-elaborates the PCMCI result in a new dictionary Returns: (dict): pcmci result re-elaborated \"\"\" res_dict = { f : list () for f in self . result [ 'var_names' ]} N , lags = self . result [ 'graph' ][ 0 ] . shape for s in range ( len ( self . result [ 'graph' ])): for t in range ( N ): for lag in range ( lags ): if self . result [ 'graph' ][ s ][ t , lag ] == '-->' : res_dict [ self . result [ 'var_names' ][ t ]] . append ({ SOURCE : self . result [ 'var_names' ][ s ], SCORE : self . result [ 'val_matrix' ][ s ][ t , lag ], PVAL : self . result [ 'p_matrix' ][ s ][ t , lag ], LAG : lag }) return res_dict __apply_alpha () Applies alpha threshold to the pcmci result Returns: Type Description ndarray graph filtered by alpha Source code in fpcmci/FValidator.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def __apply_alpha ( self ): \"\"\" Applies alpha threshold to the pcmci result Returns: (ndarray): graph filtered by alpha \"\"\" mask = np . ones ( self . result [ 'p_matrix' ] . shape , dtype = 'bool' ) # Set all p-values of absent links to 1. self . result [ 'p_matrix' ][ mask == False ] == 1. # Threshold p_matrix to get graph graph_bool = self . result [ 'p_matrix' ] <= self . alpha # Convert to string graph representation graph = self . __convert_to_string_graph ( graph_bool ) return graph __convert_to_string_graph ( graph_bool ) Converts the 0,1-based graph returned by PCMCI to a string array with links '-->' Parameters: Name Type Description Default graph_bool array 0,1-based graph array output by PCMCI required Returns: Type Description array graph as string array with links '-->'. Source code in fpcmci/FValidator.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def __convert_to_string_graph ( self , graph_bool ): \"\"\" Converts the 0,1-based graph returned by PCMCI to a string array with links '-->' Args: graph_bool (array): 0,1-based graph array output by PCMCI Returns: (array): graph as string array with links '-->'. \"\"\" graph = np . zeros ( graph_bool . shape , dtype = '<U3' ) graph [:] = \"\" # Lagged links graph [:,:, 1 :][ graph_bool [:,:, 1 :] == 1 ] = \"-->\" # Unoriented contemporaneous links graph [:,:, 0 ][ np . logical_and ( graph_bool [:,:, 0 ] == 1 , graph_bool [:,:, 0 ] . T == 1 )] = \"o-o\" # Conflicting contemporaneous links graph [:,:, 0 ][ np . logical_and ( graph_bool [:,:, 0 ] == 2 , graph_bool [:,:, 0 ] . T == 2 )] = \"x-x\" # Directed contemporaneous links for ( i , j ) in zip ( * np . where ( np . logical_and ( graph_bool [:,:, 0 ] == 1 , graph_bool [:,:, 0 ] . T == 0 ))): graph [ i , j , 0 ] = \"-->\" graph [ j , i , 0 ] = \"<--\" return graph __init__ ( data , alpha , min_lag , max_lag , val_condtest , resfolder , verbosity ) Validator class constructor Parameters: Name Type Description Default data Data data to analyse required alpha float significance level required min_lag int minimum time lag required max_lag int maximum time lag required val_condtest CondIndTest validation method required resfolder str result folder. If None then the results are not saved. required verbosity CPLevel verbosity level required Source code in fpcmci/FValidator.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , data : Data , alpha , min_lag , max_lag , val_condtest : CondIndTest , resfolder , verbosity : CPLevel ): \"\"\" Validator class constructor Args: data (Data): data to analyse alpha (float): significance level min_lag (int): minimum time lag max_lag (int): maximum time lag val_condtest (CondIndTest): validation method resfolder (str): result folder. If None then the results are not saved. verbosity (CPLevel): verbosity level \"\"\" self . data = data self . alpha = alpha self . min_lag = min_lag self . max_lag = max_lag self . result = None self . val_method = None self . val_condtest = val_condtest self . verbosity = verbosity . value self . respath = None self . dag_path = None self . ts_dag_path = None if resfolder is not None : self . respath , self . dag_path , self . ts_dag_path = utils . get_validatorpaths ( resfolder ) __return_parents_dict () Returns dictionary of parents sorted by val_matrix filtered by alpha Returns: Type Description dict Dictionary of form {0:[(0, -1), (3, -2), ...], 1:[], ...} containing estimated parents. Source code in fpcmci/FValidator.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def __return_parents_dict ( self ): \"\"\" Returns dictionary of parents sorted by val_matrix filtered by alpha Returns: (dict): Dictionary of form {0:[(0, -1), (3, -2), ...], 1:[], ...} containing estimated parents. \"\"\" graph = self . result [ 'graph' ] val_matrix = self . result [ 'val_matrix' ] p_matrix = self . result [ 'p_matrix' ] # Initialize the return value parents_dict = dict () for j in range ( self . data . N ): # Get the good links good_links = np . argwhere ( graph [:, j , 1 :] == \"-->\" ) # Build a dictionary from these links to their values links = {( i , - tau - 1 ): np . abs ( val_matrix [ i , j , abs ( tau ) + 1 ]) for i , tau in good_links if p_matrix [ i , j , abs ( tau ) + 1 ] <= self . alpha } # Sort by value parents_dict [ j ] = sorted ( links , key = links . get , reverse = True ) return parents_dict build_dag ( node_layout , min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size , show_edge_labels , label_type ) Saves dag plot if resfolder is set otherwise it shows the figure Parameters: Name Type Description Default node_layout str node_layout required min_width int minimum linewidth required max_width int maximum linewidth required min_score int minimum score range required max_score int maximum score range required node_size int node size required node_color str node color required edge_color str edge color required font_size int font size required show_edge_labels bool bit to show the time-lag label of the dependency on the edge required Source code in fpcmci/FValidator.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def build_dag ( self , node_layout , min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size , show_edge_labels , label_type ): \"\"\" Saves dag plot if resfolder is set otherwise it shows the figure Args: node_layout (str): node_layout min_width (int): minimum linewidth max_width (int): maximum linewidth min_score (int): minimum score range max_score (int): maximum score range node_size (int): node size node_color (str): node color edge_color (str): edge color font_size (int): font size show_edge_labels (bool): bit to show the time-lag label of the dependency on the edge \"\"\" # convert to dictionary res = self . __PCMCIres_converter () # filter only dependencies tmp_res = { k : res [ k ] for k in self . data . pretty_features } res = tmp_res dag ( res , node_layout = node_layout , min_width = min_width , max_width = max_width , min_score = min_score , max_score = max_score , node_size = node_size , font_size = font_size , node_color = node_color , edge_color = edge_color , show_edge_labels = show_edge_labels , label_type = label_type , save_name = self . dag_path ) build_ts_dag ( min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size ) Saves timeseries dag plot if resfolder is set otherwise it shows the figure Parameters: Name Type Description Default min_width int minimum linewidt required max_width int maximum linewidth required min_score int minimum score range required max_score int maximum score range required node_size int node size required node_color str node color required edge_color str edge color required font_size int font size required Source code in fpcmci/FValidator.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def build_ts_dag ( self , min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size ): \"\"\" Saves timeseries dag plot if resfolder is set otherwise it shows the figure Args: min_width (int): minimum linewidt max_width (int): maximum linewidth min_score (int): minimum score range max_score (int): maximum score range node_size (int): node size node_color (str): node color edge_color (str): edge color font_size (int): font size \"\"\" # convert to dictionary res = self . __PCMCIres_converter () # filter only dependencies tmp_res = { k : res [ k ] for k in self . data . pretty_features } res = tmp_res ts_dag ( res , tau = self . max_lag , min_width = min_width , max_width = max_width , min_score = min_score , max_score = max_score , node_size = node_size , node_color = node_color , edge_color = edge_color , font_size = font_size , save_name = self . ts_dag_path ) run ( selected_links = None ) Run causal discovery algorithm Returns: Type Description dict estimated causal model Source code in fpcmci/FValidator.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def run ( self , selected_links = None ): \"\"\" Run causal discovery algorithm Returns: (dict): estimated causal model \"\"\" CP . info ( ' \\n ' ) CP . info ( DASH ) CP . info ( \"Running Causal Discovery Algorithm\" ) # build tigramite dataset vector = np . vectorize ( float ) data = vector ( self . data . d ) dataframe = pp . DataFrame ( data = data , var_names = self . data . pretty_features ) # init and run pcmci self . val_method = PCMCI ( dataframe = dataframe , cond_ind_test = self . val_condtest , verbosity = self . verbosity ) self . result = self . val_method . run_pcmci ( selected_links = selected_links , tau_max = self . max_lag , tau_min = self . min_lag , pc_alpha = 0.05 ) self . result [ 'var_names' ] = self . data . pretty_features # apply significance level self . result [ 'graph' ] = self . __apply_alpha () return self . __return_parents_dict () save_result () Save causal discovery results as pickle file if resfolder is set Source code in fpcmci/FValidator.py 177 178 179 180 181 182 183 184 185 186 187 188 def save_result ( self ): \"\"\" Save causal discovery results as pickle file if resfolder is set \"\"\" if self . respath is not None : res = copy . deepcopy ( self . result ) res [ 'alpha' ] = self . alpha res [ 'var_names' ] = self . data . pretty_features res [ 'dag_path' ] = self . dag_path res [ 'ts_dag_path' ] = self . ts_dag_path with open ( self . respath , 'wb' ) as resfile : pickle . dump ( res , resfile )","title":"FPCMCI"},{"location":"fpcmci/#fpcmci.FSelector.FSelector","text":"FSelector class. FSelector is a causal feature selector framework for large-scale time series datasets. Sarting from a Data object and it selects the main features responsible for the evolution of the analysed system. Based on the selected features, the framework outputs a causal model. Source code in fpcmci/FSelector.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 class FSelector (): \"\"\" FSelector class. FSelector is a causal feature selector framework for large-scale time series datasets. Sarting from a Data object and it selects the main features responsible for the evolution of the analysed system. Based on the selected features, the framework outputs a causal model. \"\"\" def __init__ ( self , data : Data , min_lag , max_lag , sel_method : SelectionMethod , val_condtest : CondIndTest , verbosity : CPLevel , alpha = 0.05 , resfolder = None , neglect_only_autodep = False ): \"\"\" FSelector class contructor Args: data (Data): data to analyse min_lag (int): minimum time lag max_lag (int): maximum time lag sel_method (SelectionMethod): selection method val_condtest (CondIndTest): validation method verbosity (CPLevel): verbosity level alpha (float, optional): significance level. Defaults to 0.05. resfolder (string, optional): result folder to create. Defaults to None. neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False. \"\"\" self . data = data self . alpha = alpha self . min_lag = min_lag self . max_lag = max_lag self . sel_method = sel_method self . dependencies = None self . result = None self . neglect_only_autodep = neglect_only_autodep self . dependency_path = None if resfolder is not None : utils . create_results_folder () logpath , self . dependency_path = utils . get_selectorpath ( resfolder ) sys . stdout = Logger ( logpath ) self . validator = FValidator ( data , alpha , min_lag , max_lag , val_condtest , resfolder , verbosity ) CP . set_verbosity ( verbosity ) def run_selector ( self ): \"\"\" Run selection method \"\"\" CP . info ( \" \\n \" ) CP . info ( DASH ) CP . info ( \"Selecting relevant features among: \" + str ( self . data . features )) CP . info ( \"Selection method: \" + self . sel_method . name ) CP . info ( \"Significance level: \" + str ( self . alpha )) CP . info ( \"Max lag time: \" + str ( self . max_lag )) CP . info ( \"Min lag time: \" + str ( self . min_lag )) CP . info ( \"Data length: \" + str ( self . data . T )) self . sel_method . initialise ( self . data , self . alpha , self . min_lag , self . max_lag ) self . dependencies = self . sel_method . compute_dependencies () self . o_dependecies = copy . deepcopy ( self . dependencies ) def run_validator ( self ): \"\"\" Run Validator Returns: list(str): list of selected variable names \"\"\" CP . info ( \"Significance level: \" + str ( self . alpha )) CP . info ( \"Max lag time: \" + str ( self . max_lag )) CP . info ( \"Min lag time: \" + str ( self . min_lag )) CP . info ( \"Data length: \" + str ( self . data . T )) # causal model self . validator . data = self . data pcmci_result = self . validator . run () self . result = self . data . features self . save_validator_res () return self . result def run ( self ): \"\"\" Run Selector and Validator without feedback Returns: list(str): list of selected variable names \"\"\" self . run_selector () # list of selected features based on dependencies tmp_sel_features = self . get_selected_features () if not tmp_sel_features : return self . result # shrink dataframe d and dependencies by the selector result self . shrink ( tmp_sel_features ) # selected links to check by the validator selected_links = self . __get_selected_links () # causal model on selected links self . validator . data = self . data pcmci_result = self . validator . run ( selected_links ) self . __apply_validator_result ( pcmci_result ) self . result = self . get_selected_features () # shrink dataframe d and dependencies by the validator result self . shrink ( self . result ) self . save_validator_res () CP . info ( \" \\n Feature selected: \" + str ( self . result )) return self . result def shrink ( self , sel_features ): \"\"\" Wrapper in order to shrink data.d and dependencies Args: sel_features (list(str)): list of selected features \"\"\" self . data . shrink ( sel_features ) self . __shrink_dependencies () def save_validator_res ( self ): \"\"\" Saves dag plot if resfolder has been set otherwise it shows the figure \"\"\" if self . result : self . validator . save_result () else : CP . warning ( \"Result impossible to save: no feature selected\" ) def dag ( self , node_layout = 'dot' , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , node_color = 'orange' , edge_color = 'grey' , font_size = 12 , show_edge_labels = True , label_type = LabelType . Lag ): \"\"\" Saves dag plot if resfolder has been set otherwise it shows the figure Args: node_layout (str, optional): Node layout. Defaults to 'dot. min_width (int, optional): minimum linewidth. Defaults to 1. max_width (int, optional): maximum linewidth. Defaults to 5. min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. node_size (int, optional): node size. Defaults to 8. node_color (str, optional): node color. Defaults to 'orange'. edge_color (str, optional): edge color. Defaults to 'grey'. font_size (int, optional): font size. Defaults to 12. show_edge_labels (bool, optional): bit to show the time-lag label of the dependency on the edge. Defaults to True. \"\"\" if self . result : self . validator . build_dag ( node_layout , min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size , show_edge_labels , label_type ) else : CP . warning ( \"Dag impossible to create: no feature selected\" ) def timeseries_dag ( self , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , font_size = 12 , node_color = 'orange' , edge_color = 'grey' ): \"\"\" Saves timeseries dag plot if resfolder has been set otherwise it shows the figure Args: min_width (int, optional): minimum linewidth. Defaults to 1. max_width (int, optional): maximum linewidth. Defaults to 5. min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. node_size (int, optional): node size. Defaults to 8. node_color (str, optional): node color. Defaults to 'orange'. edge_color (str, optional): edge color. Defaults to 'grey'. font_size (int, optional): font size. Defaults to 12. \"\"\" if self . result : self . validator . build_ts_dag ( min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size ) else : CP . warning ( \"Timeseries dag impossible to create: no feature selected\" ) def get_selected_features ( self ): \"\"\" Defines the list of selected variables for d Returns: list(str): list of selected variable names \"\"\" f_list = list () for t in self . dependencies : sources_t = self . __get_dependencies_for_target ( t ) if self . neglect_only_autodep and self . __is_only_autodep ( sources_t , t ): sources_t . remove ( t ) if sources_t : sources_t . append ( t ) f_list = list ( set ( f_list + sources_t )) res = [ f for f in self . data . features if f in f_list ] return res def show_dependencies ( self ): \"\"\" Saves dependencies graph if resfolder is set otherwise it shows the figure \"\"\" # FIXME: LAG not considered dependencies_matrix = self . __get_dependencies_matrix () fig , ax = plt . subplots () im = ax . imshow ( dependencies_matrix , cmap = plt . cm . Greens , interpolation = 'nearest' , vmin = 0 , vmax = 1 , origin = 'lower' ) fig . colorbar ( im , orientation = 'vertical' , label = \"score\" ) plt . xlabel ( \"Sources\" ) plt . ylabel ( \"Targets\" ) plt . xticks ( ticks = range ( 0 , self . data . orig_N ), labels = self . data . orig_pretty_features , fontsize = 8 ) plt . yticks ( ticks = range ( 0 , self . data . orig_N ), labels = self . data . orig_pretty_features , fontsize = 8 ) plt . title ( \"Dependencies\" ) if self . dependency_path is not None : plt . savefig ( self . dependency_path , dpi = 300 ) else : plt . show () def print_dependencies ( self ): \"\"\" Print dependencies found by the selector \"\"\" for t in self . o_dependecies : print () print () print ( DASH ) print ( \"Target\" , t ) print ( DASH ) print ( ' {:<10s}{:>15s}{:>15s}{:>15s} ' . format ( 'SOURCE' , 'SCORE' , 'PVAL' , 'LAG' )) print ( DASH ) for s in self . o_dependecies [ t ]: print ( ' {:<10s}{:>15.3f}{:>15.3f}{:>15d} ' . format ( s [ SOURCE ], s [ SCORE ], s [ PVAL ], s [ LAG ])) def load_result ( self , res_path ): with open ( res_path , 'rb' ) as f : self . validator . result = pickle . load ( f ) def __shrink_dependencies ( self ): \"\"\" Shrinks dependencies based on the selected features \"\"\" difference_set = self . dependencies . keys () - self . data . features for d in difference_set : del self . dependencies [ d ] def __get_dependencies_for_target ( self , t ): \"\"\" Returns list of sources for a specified target Args: t (str): target variable name Returns: list(str): list of sources for target t \"\"\" return [ s [ SOURCE ] for s in self . dependencies [ t ]] def __is_only_autodep ( self , sources , t ): \"\"\" Returns list of sources for a specified target Args: sources (list(str)): list of sources for the selected target t (str): target variable name Returns: (bool): True if sources list contains only the target. False otherwise \"\"\" if len ( sources ) == 1 and sources [ 0 ] == t : return True return False def __get_dependencies_matrix ( self ): \"\"\" Returns a matrix composed by scores for each target Returns: (np.array): score matrix \"\"\" dep_mat = list () for t in self . o_dependecies : dep_vet = [ 0 ] * self . data . orig_N for s in self . o_dependecies [ t ]: dep_vet [ self . data . orig_features . index ( s [ SOURCE ])] = s [ SCORE ] dep_mat . append ( dep_vet ) dep_mat = np . array ( dep_mat ) inf_mask = np . isinf ( dep_mat ) neginf_mask = np . isneginf ( dep_mat ) max_dep_mat = np . max ( dep_mat [( dep_mat != - np . inf ) & ( dep_mat != np . inf )]) min_dep_mat = np . min ( dep_mat [( dep_mat != - np . inf ) & ( dep_mat != np . inf )]) dep_mat [ inf_mask ] = max_dep_mat dep_mat [ neginf_mask ] = min_dep_mat dep_mat = ( dep_mat - min_dep_mat ) / ( max_dep_mat - min_dep_mat ) return dep_mat def __get_selected_links ( self ): \"\"\" Return selected links found by the selector in this form: {0: [(0,-1), (2,-1)]} Returns: (dict): selected links \"\"\" sel_links = { self . data . features . index ( f ): list () for f in self . data . features } for t in self . dependencies : # add links for s in self . dependencies [ t ]: sel_links [ self . data . features . index ( t )] . append (( self . data . features . index ( s [ SOURCE ]), - s [ LAG ])) return sel_links def __apply_validator_result ( self , causal_model ): \"\"\" Exclude dependencies based on validator result \"\"\" list_diffs = list () tmp_dependencies = copy . deepcopy ( self . dependencies ) for t in tmp_dependencies : for s in tmp_dependencies [ t ]: if ( self . data . features . index ( s [ SOURCE ]), - s [ LAG ]) not in causal_model [ self . data . features . index ( t )]: list_diffs . append (( s [ SOURCE ], str ( s [ LAG ]), t )) self . dependencies [ t ] . remove ( s ) if list_diffs : CP . debug ( DASH ) CP . debug ( \"Difference(s)\" ) CP . debug ( DASH ) for diff in list_diffs : CP . debug ( \"Removing (\" + diff [ 0 ] + \" -\" + diff [ 1 ] + \") --> (\" + diff [ 2 ] + \")\" )","title":"FSelector"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.__apply_validator_result","text":"Exclude dependencies based on validator result Source code in fpcmci/FSelector.py 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 def __apply_validator_result ( self , causal_model ): \"\"\" Exclude dependencies based on validator result \"\"\" list_diffs = list () tmp_dependencies = copy . deepcopy ( self . dependencies ) for t in tmp_dependencies : for s in tmp_dependencies [ t ]: if ( self . data . features . index ( s [ SOURCE ]), - s [ LAG ]) not in causal_model [ self . data . features . index ( t )]: list_diffs . append (( s [ SOURCE ], str ( s [ LAG ]), t )) self . dependencies [ t ] . remove ( s ) if list_diffs : CP . debug ( DASH ) CP . debug ( \"Difference(s)\" ) CP . debug ( DASH ) for diff in list_diffs : CP . debug ( \"Removing (\" + diff [ 0 ] + \" -\" + diff [ 1 ] + \") --> (\" + diff [ 2 ] + \")\" )","title":"__apply_validator_result()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.__get_dependencies_for_target","text":"Returns list of sources for a specified target Parameters: Name Type Description Default t str target variable name required Returns: Name Type Description list str list of sources for target t Source code in fpcmci/FSelector.py 319 320 321 322 323 324 325 326 327 328 329 def __get_dependencies_for_target ( self , t ): \"\"\" Returns list of sources for a specified target Args: t (str): target variable name Returns: list(str): list of sources for target t \"\"\" return [ s [ SOURCE ] for s in self . dependencies [ t ]]","title":"__get_dependencies_for_target()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.__get_dependencies_matrix","text":"Returns a matrix composed by scores for each target Returns: Type Description np . array score matrix Source code in fpcmci/FSelector.py 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 def __get_dependencies_matrix ( self ): \"\"\" Returns a matrix composed by scores for each target Returns: (np.array): score matrix \"\"\" dep_mat = list () for t in self . o_dependecies : dep_vet = [ 0 ] * self . data . orig_N for s in self . o_dependecies [ t ]: dep_vet [ self . data . orig_features . index ( s [ SOURCE ])] = s [ SCORE ] dep_mat . append ( dep_vet ) dep_mat = np . array ( dep_mat ) inf_mask = np . isinf ( dep_mat ) neginf_mask = np . isneginf ( dep_mat ) max_dep_mat = np . max ( dep_mat [( dep_mat != - np . inf ) & ( dep_mat != np . inf )]) min_dep_mat = np . min ( dep_mat [( dep_mat != - np . inf ) & ( dep_mat != np . inf )]) dep_mat [ inf_mask ] = max_dep_mat dep_mat [ neginf_mask ] = min_dep_mat dep_mat = ( dep_mat - min_dep_mat ) / ( max_dep_mat - min_dep_mat ) return dep_mat","title":"__get_dependencies_matrix()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.__get_selected_links","text":"Return selected links found by the selector in this form: {0: [(0,-1), (2,-1)]} Returns: Type Description dict selected links Source code in fpcmci/FSelector.py 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 def __get_selected_links ( self ): \"\"\" Return selected links found by the selector in this form: {0: [(0,-1), (2,-1)]} Returns: (dict): selected links \"\"\" sel_links = { self . data . features . index ( f ): list () for f in self . data . features } for t in self . dependencies : # add links for s in self . dependencies [ t ]: sel_links [ self . data . features . index ( t )] . append (( self . data . features . index ( s [ SOURCE ]), - s [ LAG ])) return sel_links","title":"__get_selected_links()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.__init__","text":"FSelector class contructor Parameters: Name Type Description Default data Data data to analyse required min_lag int minimum time lag required max_lag int maximum time lag required sel_method SelectionMethod selection method required val_condtest CondIndTest validation method required verbosity CPLevel verbosity level required alpha float significance level. Defaults to 0.05. 0.05 resfolder string result folder to create. Defaults to None. None neglect_only_autodep bool Bit for neglecting variables with only autodependency. Defaults to False. False Source code in fpcmci/FSelector.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , data : Data , min_lag , max_lag , sel_method : SelectionMethod , val_condtest : CondIndTest , verbosity : CPLevel , alpha = 0.05 , resfolder = None , neglect_only_autodep = False ): \"\"\" FSelector class contructor Args: data (Data): data to analyse min_lag (int): minimum time lag max_lag (int): maximum time lag sel_method (SelectionMethod): selection method val_condtest (CondIndTest): validation method verbosity (CPLevel): verbosity level alpha (float, optional): significance level. Defaults to 0.05. resfolder (string, optional): result folder to create. Defaults to None. neglect_only_autodep (bool, optional): Bit for neglecting variables with only autodependency. Defaults to False. \"\"\" self . data = data self . alpha = alpha self . min_lag = min_lag self . max_lag = max_lag self . sel_method = sel_method self . dependencies = None self . result = None self . neglect_only_autodep = neglect_only_autodep self . dependency_path = None if resfolder is not None : utils . create_results_folder () logpath , self . dependency_path = utils . get_selectorpath ( resfolder ) sys . stdout = Logger ( logpath ) self . validator = FValidator ( data , alpha , min_lag , max_lag , val_condtest , resfolder , verbosity ) CP . set_verbosity ( verbosity )","title":"__init__()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.__is_only_autodep","text":"Returns list of sources for a specified target Parameters: Name Type Description Default sources list(str list of sources for the selected target required t str target variable name required Returns: Type Description bool True if sources list contains only the target. False otherwise Source code in fpcmci/FSelector.py 332 333 334 335 336 337 338 339 340 341 342 343 344 def __is_only_autodep ( self , sources , t ): \"\"\" Returns list of sources for a specified target Args: sources (list(str)): list of sources for the selected target t (str): target variable name Returns: (bool): True if sources list contains only the target. False otherwise \"\"\" if len ( sources ) == 1 and sources [ 0 ] == t : return True return False","title":"__is_only_autodep()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.__shrink_dependencies","text":"Shrinks dependencies based on the selected features Source code in fpcmci/FSelector.py 311 312 313 314 315 316 def __shrink_dependencies ( self ): \"\"\" Shrinks dependencies based on the selected features \"\"\" difference_set = self . dependencies . keys () - self . data . features for d in difference_set : del self . dependencies [ d ]","title":"__shrink_dependencies()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.dag","text":"Saves dag plot if resfolder has been set otherwise it shows the figure Parameters: Name Type Description Default node_layout str Node layout. Defaults to 'dot. 'dot' min_width int minimum linewidth. Defaults to 1. 1 max_width int maximum linewidth. Defaults to 5. 5 min_score int minimum score range. Defaults to 0. 0 max_score int maximum score range. Defaults to 1. 1 node_size int node size. Defaults to 8. 8 node_color str node color. Defaults to 'orange'. 'orange' edge_color str edge color. Defaults to 'grey'. 'grey' font_size int font size. Defaults to 12. 12 show_edge_labels bool bit to show the time-lag label of the dependency on the edge. Defaults to True. True Source code in fpcmci/FSelector.py 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def dag ( self , node_layout = 'dot' , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , node_color = 'orange' , edge_color = 'grey' , font_size = 12 , show_edge_labels = True , label_type = LabelType . Lag ): \"\"\" Saves dag plot if resfolder has been set otherwise it shows the figure Args: node_layout (str, optional): Node layout. Defaults to 'dot. min_width (int, optional): minimum linewidth. Defaults to 1. max_width (int, optional): maximum linewidth. Defaults to 5. min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. node_size (int, optional): node size. Defaults to 8. node_color (str, optional): node color. Defaults to 'orange'. edge_color (str, optional): edge color. Defaults to 'grey'. font_size (int, optional): font size. Defaults to 12. show_edge_labels (bool, optional): bit to show the time-lag label of the dependency on the edge. Defaults to True. \"\"\" if self . result : self . validator . build_dag ( node_layout , min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size , show_edge_labels , label_type ) else : CP . warning ( \"Dag impossible to create: no feature selected\" )","title":"dag()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.get_selected_features","text":"Defines the list of selected variables for d Returns: Name Type Description list str list of selected variable names Source code in fpcmci/FSelector.py 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 def get_selected_features ( self ): \"\"\" Defines the list of selected variables for d Returns: list(str): list of selected variable names \"\"\" f_list = list () for t in self . dependencies : sources_t = self . __get_dependencies_for_target ( t ) if self . neglect_only_autodep and self . __is_only_autodep ( sources_t , t ): sources_t . remove ( t ) if sources_t : sources_t . append ( t ) f_list = list ( set ( f_list + sources_t )) res = [ f for f in self . data . features if f in f_list ] return res","title":"get_selected_features()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.print_dependencies","text":"Print dependencies found by the selector Source code in fpcmci/FSelector.py 290 291 292 293 294 295 296 297 298 299 300 301 302 303 def print_dependencies ( self ): \"\"\" Print dependencies found by the selector \"\"\" for t in self . o_dependecies : print () print () print ( DASH ) print ( \"Target\" , t ) print ( DASH ) print ( ' {:<10s}{:>15s}{:>15s}{:>15s} ' . format ( 'SOURCE' , 'SCORE' , 'PVAL' , 'LAG' )) print ( DASH ) for s in self . o_dependecies [ t ]: print ( ' {:<10s}{:>15.3f}{:>15.3f}{:>15d} ' . format ( s [ SOURCE ], s [ SCORE ], s [ PVAL ], s [ LAG ]))","title":"print_dependencies()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.run","text":"Run Selector and Validator without feedback Returns: Name Type Description list str list of selected variable names Source code in fpcmci/FSelector.py 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def run ( self ): \"\"\" Run Selector and Validator without feedback Returns: list(str): list of selected variable names \"\"\" self . run_selector () # list of selected features based on dependencies tmp_sel_features = self . get_selected_features () if not tmp_sel_features : return self . result # shrink dataframe d and dependencies by the selector result self . shrink ( tmp_sel_features ) # selected links to check by the validator selected_links = self . __get_selected_links () # causal model on selected links self . validator . data = self . data pcmci_result = self . validator . run ( selected_links ) self . __apply_validator_result ( pcmci_result ) self . result = self . get_selected_features () # shrink dataframe d and dependencies by the validator result self . shrink ( self . result ) self . save_validator_res () CP . info ( \" \\n Feature selected: \" + str ( self . result )) return self . result","title":"run()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.run_selector","text":"Run selection method Source code in fpcmci/FSelector.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def run_selector ( self ): \"\"\" Run selection method \"\"\" CP . info ( \" \\n \" ) CP . info ( DASH ) CP . info ( \"Selecting relevant features among: \" + str ( self . data . features )) CP . info ( \"Selection method: \" + self . sel_method . name ) CP . info ( \"Significance level: \" + str ( self . alpha )) CP . info ( \"Max lag time: \" + str ( self . max_lag )) CP . info ( \"Min lag time: \" + str ( self . min_lag )) CP . info ( \"Data length: \" + str ( self . data . T )) self . sel_method . initialise ( self . data , self . alpha , self . min_lag , self . max_lag ) self . dependencies = self . sel_method . compute_dependencies () self . o_dependecies = copy . deepcopy ( self . dependencies )","title":"run_selector()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.run_validator","text":"Run Validator Returns: Name Type Description list str list of selected variable names Source code in fpcmci/FSelector.py 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def run_validator ( self ): \"\"\" Run Validator Returns: list(str): list of selected variable names \"\"\" CP . info ( \"Significance level: \" + str ( self . alpha )) CP . info ( \"Max lag time: \" + str ( self . max_lag )) CP . info ( \"Min lag time: \" + str ( self . min_lag )) CP . info ( \"Data length: \" + str ( self . data . T )) # causal model self . validator . data = self . data pcmci_result = self . validator . run () self . result = self . data . features self . save_validator_res () return self . result","title":"run_validator()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.save_validator_res","text":"Saves dag plot if resfolder has been set otherwise it shows the figure Source code in fpcmci/FSelector.py 158 159 160 161 162 163 164 165 def save_validator_res ( self ): \"\"\" Saves dag plot if resfolder has been set otherwise it shows the figure \"\"\" if self . result : self . validator . save_result () else : CP . warning ( \"Result impossible to save: no feature selected\" )","title":"save_validator_res()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.show_dependencies","text":"Saves dependencies graph if resfolder is set otherwise it shows the figure Source code in fpcmci/FSelector.py 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def show_dependencies ( self ): \"\"\" Saves dependencies graph if resfolder is set otherwise it shows the figure \"\"\" # FIXME: LAG not considered dependencies_matrix = self . __get_dependencies_matrix () fig , ax = plt . subplots () im = ax . imshow ( dependencies_matrix , cmap = plt . cm . Greens , interpolation = 'nearest' , vmin = 0 , vmax = 1 , origin = 'lower' ) fig . colorbar ( im , orientation = 'vertical' , label = \"score\" ) plt . xlabel ( \"Sources\" ) plt . ylabel ( \"Targets\" ) plt . xticks ( ticks = range ( 0 , self . data . orig_N ), labels = self . data . orig_pretty_features , fontsize = 8 ) plt . yticks ( ticks = range ( 0 , self . data . orig_N ), labels = self . data . orig_pretty_features , fontsize = 8 ) plt . title ( \"Dependencies\" ) if self . dependency_path is not None : plt . savefig ( self . dependency_path , dpi = 300 ) else : plt . show ()","title":"show_dependencies()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.shrink","text":"Wrapper in order to shrink data.d and dependencies Parameters: Name Type Description Default sel_features list(str list of selected features required Source code in fpcmci/FSelector.py 147 148 149 150 151 152 153 154 155 def shrink ( self , sel_features ): \"\"\" Wrapper in order to shrink data.d and dependencies Args: sel_features (list(str)): list of selected features \"\"\" self . data . shrink ( sel_features ) self . __shrink_dependencies ()","title":"shrink()"},{"location":"fpcmci/#fpcmci.FSelector.FSelector.timeseries_dag","text":"Saves timeseries dag plot if resfolder has been set otherwise it shows the figure Parameters: Name Type Description Default min_width int minimum linewidth. Defaults to 1. 1 max_width int maximum linewidth. Defaults to 5. 5 min_score int minimum score range. Defaults to 0. 0 max_score int maximum score range. Defaults to 1. 1 node_size int node size. Defaults to 8. 8 node_color str node color. Defaults to 'orange'. 'orange' edge_color str edge color. Defaults to 'grey'. 'grey' font_size int font size. Defaults to 12. 12 Source code in fpcmci/FSelector.py 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def timeseries_dag ( self , min_width = 1 , max_width = 5 , min_score = 0 , max_score = 1 , node_size = 8 , font_size = 12 , node_color = 'orange' , edge_color = 'grey' ): \"\"\" Saves timeseries dag plot if resfolder has been set otherwise it shows the figure Args: min_width (int, optional): minimum linewidth. Defaults to 1. max_width (int, optional): maximum linewidth. Defaults to 5. min_score (int, optional): minimum score range. Defaults to 0. max_score (int, optional): maximum score range. Defaults to 1. node_size (int, optional): node size. Defaults to 8. node_color (str, optional): node color. Defaults to 'orange'. edge_color (str, optional): edge color. Defaults to 'grey'. font_size (int, optional): font size. Defaults to 12. \"\"\" if self . result : self . validator . build_ts_dag ( min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size ) else : CP . warning ( \"Timeseries dag impossible to create: no feature selected\" )","title":"timeseries_dag()"},{"location":"fpcmci/#fpcmci.FValidator.FValidator","text":"FValidator class. FValidator works with FSelector in order to find the causal model starting from a prefixed set of variables and links. Source code in fpcmci/FValidator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 class FValidator (): \"\"\" FValidator class. FValidator works with FSelector in order to find the causal model starting from a prefixed set of variables and links. \"\"\" def __init__ ( self , data : Data , alpha , min_lag , max_lag , val_condtest : CondIndTest , resfolder , verbosity : CPLevel ): \"\"\" Validator class constructor Args: data (Data): data to analyse alpha (float): significance level min_lag (int): minimum time lag max_lag (int): maximum time lag val_condtest (CondIndTest): validation method resfolder (str): result folder. If None then the results are not saved. verbosity (CPLevel): verbosity level \"\"\" self . data = data self . alpha = alpha self . min_lag = min_lag self . max_lag = max_lag self . result = None self . val_method = None self . val_condtest = val_condtest self . verbosity = verbosity . value self . respath = None self . dag_path = None self . ts_dag_path = None if resfolder is not None : self . respath , self . dag_path , self . ts_dag_path = utils . get_validatorpaths ( resfolder ) def run ( self , selected_links = None ): \"\"\" Run causal discovery algorithm Returns: (dict): estimated causal model \"\"\" CP . info ( ' \\n ' ) CP . info ( DASH ) CP . info ( \"Running Causal Discovery Algorithm\" ) # build tigramite dataset vector = np . vectorize ( float ) data = vector ( self . data . d ) dataframe = pp . DataFrame ( data = data , var_names = self . data . pretty_features ) # init and run pcmci self . val_method = PCMCI ( dataframe = dataframe , cond_ind_test = self . val_condtest , verbosity = self . verbosity ) self . result = self . val_method . run_pcmci ( selected_links = selected_links , tau_max = self . max_lag , tau_min = self . min_lag , pc_alpha = 0.05 ) self . result [ 'var_names' ] = self . data . pretty_features # apply significance level self . result [ 'graph' ] = self . __apply_alpha () return self . __return_parents_dict () def build_ts_dag ( self , min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size ): \"\"\" Saves timeseries dag plot if resfolder is set otherwise it shows the figure Args: min_width (int): minimum linewidt max_width (int): maximum linewidth min_score (int): minimum score range max_score (int): maximum score range node_size (int): node size node_color (str): node color edge_color (str): edge color font_size (int): font size \"\"\" # convert to dictionary res = self . __PCMCIres_converter () # filter only dependencies tmp_res = { k : res [ k ] for k in self . data . pretty_features } res = tmp_res ts_dag ( res , tau = self . max_lag , min_width = min_width , max_width = max_width , min_score = min_score , max_score = max_score , node_size = node_size , node_color = node_color , edge_color = edge_color , font_size = font_size , save_name = self . ts_dag_path ) def build_dag ( self , node_layout , min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size , show_edge_labels , label_type ): \"\"\" Saves dag plot if resfolder is set otherwise it shows the figure Args: node_layout (str): node_layout min_width (int): minimum linewidth max_width (int): maximum linewidth min_score (int): minimum score range max_score (int): maximum score range node_size (int): node size node_color (str): node color edge_color (str): edge color font_size (int): font size show_edge_labels (bool): bit to show the time-lag label of the dependency on the edge \"\"\" # convert to dictionary res = self . __PCMCIres_converter () # filter only dependencies tmp_res = { k : res [ k ] for k in self . data . pretty_features } res = tmp_res dag ( res , node_layout = node_layout , min_width = min_width , max_width = max_width , min_score = min_score , max_score = max_score , node_size = node_size , font_size = font_size , node_color = node_color , edge_color = edge_color , show_edge_labels = show_edge_labels , label_type = label_type , save_name = self . dag_path ) def save_result ( self ): \"\"\" Save causal discovery results as pickle file if resfolder is set \"\"\" if self . respath is not None : res = copy . deepcopy ( self . result ) res [ 'alpha' ] = self . alpha res [ 'var_names' ] = self . data . pretty_features res [ 'dag_path' ] = self . dag_path res [ 'ts_dag_path' ] = self . ts_dag_path with open ( self . respath , 'wb' ) as resfile : pickle . dump ( res , resfile ) def __return_parents_dict ( self ): \"\"\" Returns dictionary of parents sorted by val_matrix filtered by alpha Returns: (dict): Dictionary of form {0:[(0, -1), (3, -2), ...], 1:[], ...} containing estimated parents. \"\"\" graph = self . result [ 'graph' ] val_matrix = self . result [ 'val_matrix' ] p_matrix = self . result [ 'p_matrix' ] # Initialize the return value parents_dict = dict () for j in range ( self . data . N ): # Get the good links good_links = np . argwhere ( graph [:, j , 1 :] == \"-->\" ) # Build a dictionary from these links to their values links = {( i , - tau - 1 ): np . abs ( val_matrix [ i , j , abs ( tau ) + 1 ]) for i , tau in good_links if p_matrix [ i , j , abs ( tau ) + 1 ] <= self . alpha } # Sort by value parents_dict [ j ] = sorted ( links , key = links . get , reverse = True ) return parents_dict def __PCMCIres_converter ( self ): \"\"\" Re-elaborates the PCMCI result in a new dictionary Returns: (dict): pcmci result re-elaborated \"\"\" res_dict = { f : list () for f in self . result [ 'var_names' ]} N , lags = self . result [ 'graph' ][ 0 ] . shape for s in range ( len ( self . result [ 'graph' ])): for t in range ( N ): for lag in range ( lags ): if self . result [ 'graph' ][ s ][ t , lag ] == '-->' : res_dict [ self . result [ 'var_names' ][ t ]] . append ({ SOURCE : self . result [ 'var_names' ][ s ], SCORE : self . result [ 'val_matrix' ][ s ][ t , lag ], PVAL : self . result [ 'p_matrix' ][ s ][ t , lag ], LAG : lag }) return res_dict def __apply_alpha ( self ): \"\"\" Applies alpha threshold to the pcmci result Returns: (ndarray): graph filtered by alpha \"\"\" mask = np . ones ( self . result [ 'p_matrix' ] . shape , dtype = 'bool' ) # Set all p-values of absent links to 1. self . result [ 'p_matrix' ][ mask == False ] == 1. # Threshold p_matrix to get graph graph_bool = self . result [ 'p_matrix' ] <= self . alpha # Convert to string graph representation graph = self . __convert_to_string_graph ( graph_bool ) return graph def __convert_to_string_graph ( self , graph_bool ): \"\"\" Converts the 0,1-based graph returned by PCMCI to a string array with links '-->' Args: graph_bool (array): 0,1-based graph array output by PCMCI Returns: (array): graph as string array with links '-->'. \"\"\" graph = np . zeros ( graph_bool . shape , dtype = '<U3' ) graph [:] = \"\" # Lagged links graph [:,:, 1 :][ graph_bool [:,:, 1 :] == 1 ] = \"-->\" # Unoriented contemporaneous links graph [:,:, 0 ][ np . logical_and ( graph_bool [:,:, 0 ] == 1 , graph_bool [:,:, 0 ] . T == 1 )] = \"o-o\" # Conflicting contemporaneous links graph [:,:, 0 ][ np . logical_and ( graph_bool [:,:, 0 ] == 2 , graph_bool [:,:, 0 ] . T == 2 )] = \"x-x\" # Directed contemporaneous links for ( i , j ) in zip ( * np . where ( np . logical_and ( graph_bool [:,:, 0 ] == 1 , graph_bool [:,:, 0 ] . T == 0 ))): graph [ i , j , 0 ] = \"-->\" graph [ j , i , 0 ] = \"<--\" return graph","title":"FValidator"},{"location":"fpcmci/#fpcmci.FValidator.FValidator.__PCMCIres_converter","text":"Re-elaborates the PCMCI result in a new dictionary Returns: Type Description dict pcmci result re-elaborated Source code in fpcmci/FValidator.py 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def __PCMCIres_converter ( self ): \"\"\" Re-elaborates the PCMCI result in a new dictionary Returns: (dict): pcmci result re-elaborated \"\"\" res_dict = { f : list () for f in self . result [ 'var_names' ]} N , lags = self . result [ 'graph' ][ 0 ] . shape for s in range ( len ( self . result [ 'graph' ])): for t in range ( N ): for lag in range ( lags ): if self . result [ 'graph' ][ s ][ t , lag ] == '-->' : res_dict [ self . result [ 'var_names' ][ t ]] . append ({ SOURCE : self . result [ 'var_names' ][ s ], SCORE : self . result [ 'val_matrix' ][ s ][ t , lag ], PVAL : self . result [ 'p_matrix' ][ s ][ t , lag ], LAG : lag }) return res_dict","title":"__PCMCIres_converter()"},{"location":"fpcmci/#fpcmci.FValidator.FValidator.__apply_alpha","text":"Applies alpha threshold to the pcmci result Returns: Type Description ndarray graph filtered by alpha Source code in fpcmci/FValidator.py 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 def __apply_alpha ( self ): \"\"\" Applies alpha threshold to the pcmci result Returns: (ndarray): graph filtered by alpha \"\"\" mask = np . ones ( self . result [ 'p_matrix' ] . shape , dtype = 'bool' ) # Set all p-values of absent links to 1. self . result [ 'p_matrix' ][ mask == False ] == 1. # Threshold p_matrix to get graph graph_bool = self . result [ 'p_matrix' ] <= self . alpha # Convert to string graph representation graph = self . __convert_to_string_graph ( graph_bool ) return graph","title":"__apply_alpha()"},{"location":"fpcmci/#fpcmci.FValidator.FValidator.__convert_to_string_graph","text":"Converts the 0,1-based graph returned by PCMCI to a string array with links '-->' Parameters: Name Type Description Default graph_bool array 0,1-based graph array output by PCMCI required Returns: Type Description array graph as string array with links '-->'. Source code in fpcmci/FValidator.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 def __convert_to_string_graph ( self , graph_bool ): \"\"\" Converts the 0,1-based graph returned by PCMCI to a string array with links '-->' Args: graph_bool (array): 0,1-based graph array output by PCMCI Returns: (array): graph as string array with links '-->'. \"\"\" graph = np . zeros ( graph_bool . shape , dtype = '<U3' ) graph [:] = \"\" # Lagged links graph [:,:, 1 :][ graph_bool [:,:, 1 :] == 1 ] = \"-->\" # Unoriented contemporaneous links graph [:,:, 0 ][ np . logical_and ( graph_bool [:,:, 0 ] == 1 , graph_bool [:,:, 0 ] . T == 1 )] = \"o-o\" # Conflicting contemporaneous links graph [:,:, 0 ][ np . logical_and ( graph_bool [:,:, 0 ] == 2 , graph_bool [:,:, 0 ] . T == 2 )] = \"x-x\" # Directed contemporaneous links for ( i , j ) in zip ( * np . where ( np . logical_and ( graph_bool [:,:, 0 ] == 1 , graph_bool [:,:, 0 ] . T == 0 ))): graph [ i , j , 0 ] = \"-->\" graph [ j , i , 0 ] = \"<--\" return graph","title":"__convert_to_string_graph()"},{"location":"fpcmci/#fpcmci.FValidator.FValidator.__init__","text":"Validator class constructor Parameters: Name Type Description Default data Data data to analyse required alpha float significance level required min_lag int minimum time lag required max_lag int maximum time lag required val_condtest CondIndTest validation method required resfolder str result folder. If None then the results are not saved. required verbosity CPLevel verbosity level required Source code in fpcmci/FValidator.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __init__ ( self , data : Data , alpha , min_lag , max_lag , val_condtest : CondIndTest , resfolder , verbosity : CPLevel ): \"\"\" Validator class constructor Args: data (Data): data to analyse alpha (float): significance level min_lag (int): minimum time lag max_lag (int): maximum time lag val_condtest (CondIndTest): validation method resfolder (str): result folder. If None then the results are not saved. verbosity (CPLevel): verbosity level \"\"\" self . data = data self . alpha = alpha self . min_lag = min_lag self . max_lag = max_lag self . result = None self . val_method = None self . val_condtest = val_condtest self . verbosity = verbosity . value self . respath = None self . dag_path = None self . ts_dag_path = None if resfolder is not None : self . respath , self . dag_path , self . ts_dag_path = utils . get_validatorpaths ( resfolder )","title":"__init__()"},{"location":"fpcmci/#fpcmci.FValidator.FValidator.__return_parents_dict","text":"Returns dictionary of parents sorted by val_matrix filtered by alpha Returns: Type Description dict Dictionary of form {0:[(0, -1), (3, -2), ...], 1:[], ...} containing estimated parents. Source code in fpcmci/FValidator.py 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 def __return_parents_dict ( self ): \"\"\" Returns dictionary of parents sorted by val_matrix filtered by alpha Returns: (dict): Dictionary of form {0:[(0, -1), (3, -2), ...], 1:[], ...} containing estimated parents. \"\"\" graph = self . result [ 'graph' ] val_matrix = self . result [ 'val_matrix' ] p_matrix = self . result [ 'p_matrix' ] # Initialize the return value parents_dict = dict () for j in range ( self . data . N ): # Get the good links good_links = np . argwhere ( graph [:, j , 1 :] == \"-->\" ) # Build a dictionary from these links to their values links = {( i , - tau - 1 ): np . abs ( val_matrix [ i , j , abs ( tau ) + 1 ]) for i , tau in good_links if p_matrix [ i , j , abs ( tau ) + 1 ] <= self . alpha } # Sort by value parents_dict [ j ] = sorted ( links , key = links . get , reverse = True ) return parents_dict","title":"__return_parents_dict()"},{"location":"fpcmci/#fpcmci.FValidator.FValidator.build_dag","text":"Saves dag plot if resfolder is set otherwise it shows the figure Parameters: Name Type Description Default node_layout str node_layout required min_width int minimum linewidth required max_width int maximum linewidth required min_score int minimum score range required max_score int maximum score range required node_size int node size required node_color str node color required edge_color str edge color required font_size int font size required show_edge_labels bool bit to show the time-lag label of the dependency on the edge required Source code in fpcmci/FValidator.py 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 def build_dag ( self , node_layout , min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size , show_edge_labels , label_type ): \"\"\" Saves dag plot if resfolder is set otherwise it shows the figure Args: node_layout (str): node_layout min_width (int): minimum linewidth max_width (int): maximum linewidth min_score (int): minimum score range max_score (int): maximum score range node_size (int): node size node_color (str): node color edge_color (str): edge color font_size (int): font size show_edge_labels (bool): bit to show the time-lag label of the dependency on the edge \"\"\" # convert to dictionary res = self . __PCMCIres_converter () # filter only dependencies tmp_res = { k : res [ k ] for k in self . data . pretty_features } res = tmp_res dag ( res , node_layout = node_layout , min_width = min_width , max_width = max_width , min_score = min_score , max_score = max_score , node_size = node_size , font_size = font_size , node_color = node_color , edge_color = edge_color , show_edge_labels = show_edge_labels , label_type = label_type , save_name = self . dag_path )","title":"build_dag()"},{"location":"fpcmci/#fpcmci.FValidator.FValidator.build_ts_dag","text":"Saves timeseries dag plot if resfolder is set otherwise it shows the figure Parameters: Name Type Description Default min_width int minimum linewidt required max_width int maximum linewidth required min_score int minimum score range required max_score int maximum score range required node_size int node size required node_color str node color required edge_color str edge color required font_size int font size required Source code in fpcmci/FValidator.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def build_ts_dag ( self , min_width , max_width , min_score , max_score , node_size , node_color , edge_color , font_size ): \"\"\" Saves timeseries dag plot if resfolder is set otherwise it shows the figure Args: min_width (int): minimum linewidt max_width (int): maximum linewidth min_score (int): minimum score range max_score (int): maximum score range node_size (int): node size node_color (str): node color edge_color (str): edge color font_size (int): font size \"\"\" # convert to dictionary res = self . __PCMCIres_converter () # filter only dependencies tmp_res = { k : res [ k ] for k in self . data . pretty_features } res = tmp_res ts_dag ( res , tau = self . max_lag , min_width = min_width , max_width = max_width , min_score = min_score , max_score = max_score , node_size = node_size , node_color = node_color , edge_color = edge_color , font_size = font_size , save_name = self . ts_dag_path )","title":"build_ts_dag()"},{"location":"fpcmci/#fpcmci.FValidator.FValidator.run","text":"Run causal discovery algorithm Returns: Type Description dict estimated causal model Source code in fpcmci/FValidator.py 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def run ( self , selected_links = None ): \"\"\" Run causal discovery algorithm Returns: (dict): estimated causal model \"\"\" CP . info ( ' \\n ' ) CP . info ( DASH ) CP . info ( \"Running Causal Discovery Algorithm\" ) # build tigramite dataset vector = np . vectorize ( float ) data = vector ( self . data . d ) dataframe = pp . DataFrame ( data = data , var_names = self . data . pretty_features ) # init and run pcmci self . val_method = PCMCI ( dataframe = dataframe , cond_ind_test = self . val_condtest , verbosity = self . verbosity ) self . result = self . val_method . run_pcmci ( selected_links = selected_links , tau_max = self . max_lag , tau_min = self . min_lag , pc_alpha = 0.05 ) self . result [ 'var_names' ] = self . data . pretty_features # apply significance level self . result [ 'graph' ] = self . __apply_alpha () return self . __return_parents_dict ()","title":"run()"},{"location":"fpcmci/#fpcmci.FValidator.FValidator.save_result","text":"Save causal discovery results as pickle file if resfolder is set Source code in fpcmci/FValidator.py 177 178 179 180 181 182 183 184 185 186 187 188 def save_result ( self ): \"\"\" Save causal discovery results as pickle file if resfolder is set \"\"\" if self . respath is not None : res = copy . deepcopy ( self . result ) res [ 'alpha' ] = self . alpha res [ 'var_names' ] = self . data . pretty_features res [ 'dag_path' ] = self . dag_path res [ 'ts_dag_path' ] = self . ts_dag_path with open ( self . respath , 'wb' ) as resfile : pickle . dump ( res , resfile )","title":"save_result()"},{"location":"preprocessing/","text":"Data Data class manages the preprocess of the data before the causal analysis Source code in fpcmci/preprocessing/data.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 class Data (): \"\"\" Data class manages the preprocess of the data before the causal analysis \"\"\" def __init__ ( self , data , vars = None , fill_nan = True , stand = False , subsampling : SubsamplingMethod = None , show_subsampling = False ): \"\"\" Data class constructor Args: data (str/DataFrame/np.array): it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array vars (list(str), optional): List containing variable names. If unset then, if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N] Defaults to None. fill_nan (bool, optional): Fill NaNs bit. Defaults to True. stand (bool, optional): Standardization bit. Defaults to False. subsampling (SubsamplingMethod, optional): Subsampling method. If None not active. Defaults to None. show_subsampling (bool, optional): If True shows subsampling result. Defaults to False. Raises: TypeError: if data is not str - DataFrame - ndarray \"\"\" # Data handling if type ( data ) == np . ndarray : self . d = pd . DataFrame ( data ) if vars is None : self . d . columns = list ([ 'X_' + str ( f ) for f in range ( len ( self . d . columns ))]) elif type ( data ) == pd . DataFrame : self . d = data elif type ( data ) == str : self . d = pd . read_csv ( data ) else : raise TypeError ( \"data field not in the correct type \\n data must be one of the following type: \\n - numpy.ndarray \\n - pandas.DataFrame \\n - .csv path\" ) # Columns name handling if vars is not None : self . d . columns = list ( vars ) self . orig_features = self . features self . orig_pretty_features = self . pretty_features self . orig_N = self . N self . orig_T = len ( self . d ) # Filling NaNs if fill_nan : if self . d . isnull () . values . any (): self . d . fillna ( inplace = True , method = \"ffill\" ) self . d . fillna ( inplace = True , method = \"bfill\" ) # Subsampling data if subsampling is not None : subsampler = Subsampler ( self . d , ss_method = subsampling ) self . d = pd . DataFrame ( subsampler . subsample (), columns = self . features ) if show_subsampling : subsampler . plot_subsampled_data () # Standardize data if stand : scaler = StandardScaler () scaler = scaler . fit ( self . d ) self . d = pd . DataFrame ( scaler . transform ( self . d ), columns = self . features ) @property def features ( self ): \"\"\" Returns list of features Returns: list(str): list of feature names \"\"\" return list ( self . d . columns ) @property def pretty_features ( self ): \"\"\" Returns list of features with LATEX symbols Returns: list(str): list of feature names \"\"\" return [ r '$' + str ( v ) + '$' for v in self . d . columns ] @property def N ( self ): \"\"\" Number of features Returns: (int): number of features \"\"\" return len ( self . d . columns ) @property def T ( self ): \"\"\" Dataframe length Returns: (int): dataframe length \"\"\" return len ( self . d ) def shrink ( self , selected_features ): \"\"\" Shrinks dataframe d and dependencies based on the selected features Args: selected_features (list(str)): features selected by the selector \"\"\" self . d = self . d [ selected_features ] def plot_timeseries ( self ): \"\"\" Plots timeseries data \"\"\" # Create grid gs = gridspec . GridSpec ( self . N , 1 ) # Time vector T = list ( range ( self . T )) plt . figure () for i in range ( 0 , self . d . shape [ 1 ]): ax = plt . subplot ( gs [ i , 0 ]) plt . plot ( T , self . d . values [:, i ], color = 'tab:red' ) plt . ylabel ( str ( self . pretty_features [ i ])) plt . show () N () property Number of features Returns: Type Description int number of features Source code in fpcmci/preprocessing/data.py 92 93 94 95 96 97 98 99 100 @property def N ( self ): \"\"\" Number of features Returns: (int): number of features \"\"\" return len ( self . d . columns ) T () property Dataframe length Returns: Type Description int dataframe length Source code in fpcmci/preprocessing/data.py 102 103 104 105 106 107 108 109 110 @property def T ( self ): \"\"\" Dataframe length Returns: (int): dataframe length \"\"\" return len ( self . d ) __init__ ( data , vars = None , fill_nan = True , stand = False , subsampling = None , show_subsampling = False ) Data class constructor Parameters: Name Type Description Default data str/DataFrame/np.array it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array required vars list ( str ) List containing variable names. If unset then, if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N] Defaults to None. None fill_nan bool Fill NaNs bit. Defaults to True. True stand bool Standardization bit. Defaults to False. False subsampling SubsamplingMethod Subsampling method. If None not active. Defaults to None. None show_subsampling bool If True shows subsampling result. Defaults to False. False Raises: Type Description TypeError if data is not str - DataFrame - ndarray Source code in fpcmci/preprocessing/data.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , data , vars = None , fill_nan = True , stand = False , subsampling : SubsamplingMethod = None , show_subsampling = False ): \"\"\" Data class constructor Args: data (str/DataFrame/np.array): it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array vars (list(str), optional): List containing variable names. If unset then, if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N] Defaults to None. fill_nan (bool, optional): Fill NaNs bit. Defaults to True. stand (bool, optional): Standardization bit. Defaults to False. subsampling (SubsamplingMethod, optional): Subsampling method. If None not active. Defaults to None. show_subsampling (bool, optional): If True shows subsampling result. Defaults to False. Raises: TypeError: if data is not str - DataFrame - ndarray \"\"\" # Data handling if type ( data ) == np . ndarray : self . d = pd . DataFrame ( data ) if vars is None : self . d . columns = list ([ 'X_' + str ( f ) for f in range ( len ( self . d . columns ))]) elif type ( data ) == pd . DataFrame : self . d = data elif type ( data ) == str : self . d = pd . read_csv ( data ) else : raise TypeError ( \"data field not in the correct type \\n data must be one of the following type: \\n - numpy.ndarray \\n - pandas.DataFrame \\n - .csv path\" ) # Columns name handling if vars is not None : self . d . columns = list ( vars ) self . orig_features = self . features self . orig_pretty_features = self . pretty_features self . orig_N = self . N self . orig_T = len ( self . d ) # Filling NaNs if fill_nan : if self . d . isnull () . values . any (): self . d . fillna ( inplace = True , method = \"ffill\" ) self . d . fillna ( inplace = True , method = \"bfill\" ) # Subsampling data if subsampling is not None : subsampler = Subsampler ( self . d , ss_method = subsampling ) self . d = pd . DataFrame ( subsampler . subsample (), columns = self . features ) if show_subsampling : subsampler . plot_subsampled_data () # Standardize data if stand : scaler = StandardScaler () scaler = scaler . fit ( self . d ) self . d = pd . DataFrame ( scaler . transform ( self . d ), columns = self . features ) features () property Returns list of features Returns: Name Type Description list str list of feature names Source code in fpcmci/preprocessing/data.py 72 73 74 75 76 77 78 79 80 @property def features ( self ): \"\"\" Returns list of features Returns: list(str): list of feature names \"\"\" return list ( self . d . columns ) plot_timeseries () Plots timeseries data Source code in fpcmci/preprocessing/data.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def plot_timeseries ( self ): \"\"\" Plots timeseries data \"\"\" # Create grid gs = gridspec . GridSpec ( self . N , 1 ) # Time vector T = list ( range ( self . T )) plt . figure () for i in range ( 0 , self . d . shape [ 1 ]): ax = plt . subplot ( gs [ i , 0 ]) plt . plot ( T , self . d . values [:, i ], color = 'tab:red' ) plt . ylabel ( str ( self . pretty_features [ i ])) plt . show () pretty_features () property Returns list of features with LATEX symbols Returns: Name Type Description list str list of feature names Source code in fpcmci/preprocessing/data.py 82 83 84 85 86 87 88 89 90 @property def pretty_features ( self ): \"\"\" Returns list of features with LATEX symbols Returns: list(str): list of feature names \"\"\" return [ r '$' + str ( v ) + '$' for v in self . d . columns ] shrink ( selected_features ) Shrinks dataframe d and dependencies based on the selected features Parameters: Name Type Description Default selected_features list(str features selected by the selector required Source code in fpcmci/preprocessing/data.py 113 114 115 116 117 118 119 120 def shrink ( self , selected_features ): \"\"\" Shrinks dataframe d and dependencies based on the selected features Args: selected_features (list(str)): features selected by the selector \"\"\" self . d = self . d [ selected_features ]","title":"Preprocessing"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data","text":"Data class manages the preprocess of the data before the causal analysis Source code in fpcmci/preprocessing/data.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 class Data (): \"\"\" Data class manages the preprocess of the data before the causal analysis \"\"\" def __init__ ( self , data , vars = None , fill_nan = True , stand = False , subsampling : SubsamplingMethod = None , show_subsampling = False ): \"\"\" Data class constructor Args: data (str/DataFrame/np.array): it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array vars (list(str), optional): List containing variable names. If unset then, if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N] Defaults to None. fill_nan (bool, optional): Fill NaNs bit. Defaults to True. stand (bool, optional): Standardization bit. Defaults to False. subsampling (SubsamplingMethod, optional): Subsampling method. If None not active. Defaults to None. show_subsampling (bool, optional): If True shows subsampling result. Defaults to False. Raises: TypeError: if data is not str - DataFrame - ndarray \"\"\" # Data handling if type ( data ) == np . ndarray : self . d = pd . DataFrame ( data ) if vars is None : self . d . columns = list ([ 'X_' + str ( f ) for f in range ( len ( self . d . columns ))]) elif type ( data ) == pd . DataFrame : self . d = data elif type ( data ) == str : self . d = pd . read_csv ( data ) else : raise TypeError ( \"data field not in the correct type \\n data must be one of the following type: \\n - numpy.ndarray \\n - pandas.DataFrame \\n - .csv path\" ) # Columns name handling if vars is not None : self . d . columns = list ( vars ) self . orig_features = self . features self . orig_pretty_features = self . pretty_features self . orig_N = self . N self . orig_T = len ( self . d ) # Filling NaNs if fill_nan : if self . d . isnull () . values . any (): self . d . fillna ( inplace = True , method = \"ffill\" ) self . d . fillna ( inplace = True , method = \"bfill\" ) # Subsampling data if subsampling is not None : subsampler = Subsampler ( self . d , ss_method = subsampling ) self . d = pd . DataFrame ( subsampler . subsample (), columns = self . features ) if show_subsampling : subsampler . plot_subsampled_data () # Standardize data if stand : scaler = StandardScaler () scaler = scaler . fit ( self . d ) self . d = pd . DataFrame ( scaler . transform ( self . d ), columns = self . features ) @property def features ( self ): \"\"\" Returns list of features Returns: list(str): list of feature names \"\"\" return list ( self . d . columns ) @property def pretty_features ( self ): \"\"\" Returns list of features with LATEX symbols Returns: list(str): list of feature names \"\"\" return [ r '$' + str ( v ) + '$' for v in self . d . columns ] @property def N ( self ): \"\"\" Number of features Returns: (int): number of features \"\"\" return len ( self . d . columns ) @property def T ( self ): \"\"\" Dataframe length Returns: (int): dataframe length \"\"\" return len ( self . d ) def shrink ( self , selected_features ): \"\"\" Shrinks dataframe d and dependencies based on the selected features Args: selected_features (list(str)): features selected by the selector \"\"\" self . d = self . d [ selected_features ] def plot_timeseries ( self ): \"\"\" Plots timeseries data \"\"\" # Create grid gs = gridspec . GridSpec ( self . N , 1 ) # Time vector T = list ( range ( self . T )) plt . figure () for i in range ( 0 , self . d . shape [ 1 ]): ax = plt . subplot ( gs [ i , 0 ]) plt . plot ( T , self . d . values [:, i ], color = 'tab:red' ) plt . ylabel ( str ( self . pretty_features [ i ])) plt . show ()","title":"Data"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.N","text":"Number of features Returns: Type Description int number of features Source code in fpcmci/preprocessing/data.py 92 93 94 95 96 97 98 99 100 @property def N ( self ): \"\"\" Number of features Returns: (int): number of features \"\"\" return len ( self . d . columns )","title":"N()"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.T","text":"Dataframe length Returns: Type Description int dataframe length Source code in fpcmci/preprocessing/data.py 102 103 104 105 106 107 108 109 110 @property def T ( self ): \"\"\" Dataframe length Returns: (int): dataframe length \"\"\" return len ( self . d )","title":"T()"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.__init__","text":"Data class constructor Parameters: Name Type Description Default data str/DataFrame/np.array it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array required vars list ( str ) List containing variable names. If unset then, if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N] Defaults to None. None fill_nan bool Fill NaNs bit. Defaults to True. True stand bool Standardization bit. Defaults to False. False subsampling SubsamplingMethod Subsampling method. If None not active. Defaults to None. None show_subsampling bool If True shows subsampling result. Defaults to False. False Raises: Type Description TypeError if data is not str - DataFrame - ndarray Source code in fpcmci/preprocessing/data.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def __init__ ( self , data , vars = None , fill_nan = True , stand = False , subsampling : SubsamplingMethod = None , show_subsampling = False ): \"\"\" Data class constructor Args: data (str/DataFrame/np.array): it can be a string specifing the path of a csv file to load/pandas.DataFrame/numpy.array vars (list(str), optional): List containing variable names. If unset then, if data = (str/DataFrame) vars = data columns name elif data = np.array vars = [X_0 .. X_N] Defaults to None. fill_nan (bool, optional): Fill NaNs bit. Defaults to True. stand (bool, optional): Standardization bit. Defaults to False. subsampling (SubsamplingMethod, optional): Subsampling method. If None not active. Defaults to None. show_subsampling (bool, optional): If True shows subsampling result. Defaults to False. Raises: TypeError: if data is not str - DataFrame - ndarray \"\"\" # Data handling if type ( data ) == np . ndarray : self . d = pd . DataFrame ( data ) if vars is None : self . d . columns = list ([ 'X_' + str ( f ) for f in range ( len ( self . d . columns ))]) elif type ( data ) == pd . DataFrame : self . d = data elif type ( data ) == str : self . d = pd . read_csv ( data ) else : raise TypeError ( \"data field not in the correct type \\n data must be one of the following type: \\n - numpy.ndarray \\n - pandas.DataFrame \\n - .csv path\" ) # Columns name handling if vars is not None : self . d . columns = list ( vars ) self . orig_features = self . features self . orig_pretty_features = self . pretty_features self . orig_N = self . N self . orig_T = len ( self . d ) # Filling NaNs if fill_nan : if self . d . isnull () . values . any (): self . d . fillna ( inplace = True , method = \"ffill\" ) self . d . fillna ( inplace = True , method = \"bfill\" ) # Subsampling data if subsampling is not None : subsampler = Subsampler ( self . d , ss_method = subsampling ) self . d = pd . DataFrame ( subsampler . subsample (), columns = self . features ) if show_subsampling : subsampler . plot_subsampled_data () # Standardize data if stand : scaler = StandardScaler () scaler = scaler . fit ( self . d ) self . d = pd . DataFrame ( scaler . transform ( self . d ), columns = self . features )","title":"__init__()"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.features","text":"Returns list of features Returns: Name Type Description list str list of feature names Source code in fpcmci/preprocessing/data.py 72 73 74 75 76 77 78 79 80 @property def features ( self ): \"\"\" Returns list of features Returns: list(str): list of feature names \"\"\" return list ( self . d . columns )","title":"features()"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.plot_timeseries","text":"Plots timeseries data Source code in fpcmci/preprocessing/data.py 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 def plot_timeseries ( self ): \"\"\" Plots timeseries data \"\"\" # Create grid gs = gridspec . GridSpec ( self . N , 1 ) # Time vector T = list ( range ( self . T )) plt . figure () for i in range ( 0 , self . d . shape [ 1 ]): ax = plt . subplot ( gs [ i , 0 ]) plt . plot ( T , self . d . values [:, i ], color = 'tab:red' ) plt . ylabel ( str ( self . pretty_features [ i ])) plt . show ()","title":"plot_timeseries()"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.pretty_features","text":"Returns list of features with LATEX symbols Returns: Name Type Description list str list of feature names Source code in fpcmci/preprocessing/data.py 82 83 84 85 86 87 88 89 90 @property def pretty_features ( self ): \"\"\" Returns list of features with LATEX symbols Returns: list(str): list of feature names \"\"\" return [ r '$' + str ( v ) + '$' for v in self . d . columns ]","title":"pretty_features()"},{"location":"preprocessing/#fpcmci.preprocessing.data.Data.shrink","text":"Shrinks dataframe d and dependencies based on the selected features Parameters: Name Type Description Default selected_features list(str features selected by the selector required Source code in fpcmci/preprocessing/data.py 113 114 115 116 117 118 119 120 def shrink ( self , selected_features ): \"\"\" Shrinks dataframe d and dependencies based on the selected features Args: selected_features (list(str)): features selected by the selector \"\"\" self . d = self . d [ selected_features ]","title":"shrink()"},{"location":"subsampling_method/","text":"Subsampler Subsampler class. It subsamples the data by using a subsampling method chosen among Static - subsamples data by taking one sample each step-samples WSDynamic - entropy based method with dynamic window size computed by breakpoint analysis WSFFTStatic - entropy based method with fixed window size computed by FFT analysis WSStatic - entropy base method with predefined window size Source code in fpcmci/preprocessing/Subsampler.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class Subsampler (): \"\"\" Subsampler class. It subsamples the data by using a subsampling method chosen among: - Static - subsamples data by taking one sample each step-samples - WSDynamic - entropy based method with dynamic window size computed by breakpoint analysis - WSFFTStatic - entropy based method with fixed window size computed by FFT analysis - WSStatic - entropy base method with predefined window size \"\"\" def __init__ ( self , df : pd . DataFrame , ss_method : SubsamplingMethod ): \"\"\" Subsampler class constructor Args: df (pd.DataFrame): dataframe to subsample ss_method (SubsamplingMethod): subsampling method \"\"\" self . df = df self . ss_method = ss_method self . ss_method . initialise ( df ) def subsample ( self ): \"\"\" Runs the subsampling algorithm and returns the subsapled ndarray Returns: (ndarray): Subsampled dataframe value \"\"\" self . result = self . ss_method . run () return self . df . values [ self . result , :] def plot_subsampled_data ( self , dpi = 100 , show = True ): \"\"\" Plot dataframe sub-sampled data Args: dpi (int, optional): image dpi. Defaults to 100. show (bool, optional): if True it shows the figure and block the process. Defaults to True. \"\"\" n_plot = self . df . shape [ 1 ] # Create grid gs = gridspec . GridSpec ( n_plot , 1 ) # Time vector T = list ( range ( 0 , self . df . shape [ 0 ])) pl . figure ( dpi = dpi ) for i in range ( 0 , n_plot ): ax = pl . subplot ( gs [ i , 0 ]) pl . plot ( T , self . df . values [:, i ], color = 'tab:red' ) pl . scatter ( np . array ( T )[ self . result ], self . df . values [ self . result , i ], s = 80 , facecolors = 'none' , edgecolors = 'b' ) pl . gca () . set ( ylabel = r '$' + str ( self . df . columns . values [ i ]) + '$' ) if show : pl . show () __init__ ( df , ss_method ) Subsampler class constructor Parameters: Name Type Description Default df pd . DataFrame dataframe to subsample required ss_method SubsamplingMethod subsampling method required Source code in fpcmci/preprocessing/Subsampler.py 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , df : pd . DataFrame , ss_method : SubsamplingMethod ): \"\"\" Subsampler class constructor Args: df (pd.DataFrame): dataframe to subsample ss_method (SubsamplingMethod): subsampling method \"\"\" self . df = df self . ss_method = ss_method self . ss_method . initialise ( df ) plot_subsampled_data ( dpi = 100 , show = True ) Plot dataframe sub-sampled data Parameters: Name Type Description Default dpi int image dpi. Defaults to 100. 100 show bool if True it shows the figure and block the process. Defaults to True. True Source code in fpcmci/preprocessing/Subsampler.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def plot_subsampled_data ( self , dpi = 100 , show = True ): \"\"\" Plot dataframe sub-sampled data Args: dpi (int, optional): image dpi. Defaults to 100. show (bool, optional): if True it shows the figure and block the process. Defaults to True. \"\"\" n_plot = self . df . shape [ 1 ] # Create grid gs = gridspec . GridSpec ( n_plot , 1 ) # Time vector T = list ( range ( 0 , self . df . shape [ 0 ])) pl . figure ( dpi = dpi ) for i in range ( 0 , n_plot ): ax = pl . subplot ( gs [ i , 0 ]) pl . plot ( T , self . df . values [:, i ], color = 'tab:red' ) pl . scatter ( np . array ( T )[ self . result ], self . df . values [ self . result , i ], s = 80 , facecolors = 'none' , edgecolors = 'b' ) pl . gca () . set ( ylabel = r '$' + str ( self . df . columns . values [ i ]) + '$' ) if show : pl . show () subsample () Runs the subsampling algorithm and returns the subsapled ndarray Returns: Type Description ndarray Subsampled dataframe value Source code in fpcmci/preprocessing/Subsampler.py 33 34 35 36 37 38 39 40 41 def subsample ( self ): \"\"\" Runs the subsampling algorithm and returns the subsapled ndarray Returns: (ndarray): Subsampled dataframe value \"\"\" self . result = self . ss_method . run () return self . df . values [ self . result , :] SubsamplingMethod Bases: ABC SubsamplingMethod abstract class Source code in fpcmci/preprocessing/subsampling_methods/SubsamplingMethod.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class SubsamplingMethod ( ABC ): \"\"\" SubsamplingMethod abstract class \"\"\" def __init__ ( self , ssmode : SSMode ): self . ssmode = ssmode self . df = None def initialise ( self , dataframe : pd . DataFrame ): \"\"\" Initialise class by setting the dataframe to subsample Args: dataframe (pd.DataFrame): _description_ \"\"\" self . df = dataframe @abstractmethod def run ( self ): \"\"\" Run subsampler \"\"\" pass initialise ( dataframe ) Initialise class by setting the dataframe to subsample Parameters: Name Type Description Default dataframe pd . DataFrame description required Source code in fpcmci/preprocessing/subsampling_methods/SubsamplingMethod.py 22 23 24 25 26 27 28 29 def initialise ( self , dataframe : pd . DataFrame ): \"\"\" Initialise class by setting the dataframe to subsample Args: dataframe (pd.DataFrame): _description_ \"\"\" self . df = dataframe run () abstractmethod Run subsampler Source code in fpcmci/preprocessing/subsampling_methods/SubsamplingMethod.py 32 33 34 35 36 37 @abstractmethod def run ( self ): \"\"\" Run subsampler \"\"\" pass EntropyBasedMethod Bases: ABC EntropyBasedMethod abstract class Source code in fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class EntropyBasedMethod ( ABC ): \"\"\" EntropyBasedMethod abstract class \"\"\" def __init__ ( self , threshold ): self . windows = list () self . segments = list () self . threshold = threshold def create_rounded_copy ( self ): \"\"\" Create deepcopy of the dataframe but with rounded values Returns: (pd.DataFrame): rounded dataframe \"\"\" de = deepcopy ( self . df ) de = de . round ( 1 ) return de def __normalization ( self ): \"\"\" Normalize entropy for each moving window \"\"\" max_e = max ([ mw . entropy for mw in self . windows ]) for mw in self . windows : mw . entropy = mw . entropy / max_e def moving_window_analysis ( self ): \"\"\" Compute dataframe entropy on moving windows \"\"\" de = self . create_rounded_copy () for ll , rl in self . segments : # Create moving window mw_df = de . values [ ll : rl ] # Build a Moving Window mw = MovingWindow ( mw_df ) # Compute entropy mw . get_entropy () # Compute optimal number of samples mw . optimal_sampling ( self . threshold ) # Collect result in a list self . windows . append ( mw ) # Entropy normalization self . __normalization () # def extract_data(self): # \"\"\" # Extract plottable data from moving window analysis # \"\"\" # # Entropies and samples numbers list # self.__entropy_list = [mw.entropy for mw in self.__window_list] # self.__sample_number_list = [mw.opt_size for mw in self.__window_list] # self.__original_size = [mw.T for mw in self.__window_list] # self.num_samples = sum(self.__sample_number_list) # # Make entropy and sample array plottable # self.__pretty_signals() # def __pretty_signals(self): # \"\"\" # Make entropy list and sample number list plottable # \"\"\" # _pretty_entropy = [] # _pretty_sample_number = [] # _pretty_original_size = [] # for i, mw in enumerate(self.__window_list): # _pretty_entropy += np.repeat(self.__entropy_list[i], mw.T).tolist() # _pretty_sample_number += np.repeat(self.__sample_number_list[i], mw.T).tolist() # _pretty_original_size += np.repeat(self.__original_size[i], mw.T).tolist() # self.__entropy_list = _pretty_entropy # self.__sample_number_list = _pretty_sample_number # self.__original_size = _pretty_original_size # _diff = self.df.shape[0] - len(self.__entropy_list) # if _diff != 0: # self.__entropy_list = np.append(self.__entropy_list, [self.__entropy_list[-1]] * _diff) # self.__sample_number_list = np.append(self.__sample_number_list, [self.__sample_number_list[-1]] * _diff) def extract_indexes ( self ): \"\"\" Extract a list of indexes corresponding to the samples selected by the subsampling procedure \"\"\" _sample_index_list = list () for i , mw in enumerate ( self . windows ): sum_ws = sum ([ wind . T for wind in self . windows [: i ]]) sample_index = [ si + sum_ws for si in mw . opt_samples_index ] _sample_index_list += sample_index return _sample_index_list @abstractmethod def dataset_segmentation ( self ): \"\"\" abstract method \"\"\" pass __normalization () Normalize entropy for each moving window Source code in fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py 28 29 30 31 32 33 34 def __normalization ( self ): \"\"\" Normalize entropy for each moving window \"\"\" max_e = max ([ mw . entropy for mw in self . windows ]) for mw in self . windows : mw . entropy = mw . entropy / max_e create_rounded_copy () Create deepcopy of the dataframe but with rounded values Returns: Type Description pd . DataFrame rounded dataframe Source code in fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py 16 17 18 19 20 21 22 23 24 25 def create_rounded_copy ( self ): \"\"\" Create deepcopy of the dataframe but with rounded values Returns: (pd.DataFrame): rounded dataframe \"\"\" de = deepcopy ( self . df ) de = de . round ( 1 ) return de dataset_segmentation () abstractmethod abstract method Source code in fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py 111 112 113 114 115 116 @abstractmethod def dataset_segmentation ( self ): \"\"\" abstract method \"\"\" pass extract_indexes () Extract a list of indexes corresponding to the samples selected by the subsampling procedure Source code in fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py 98 99 100 101 102 103 104 105 106 107 108 def extract_indexes ( self ): \"\"\" Extract a list of indexes corresponding to the samples selected by the subsampling procedure \"\"\" _sample_index_list = list () for i , mw in enumerate ( self . windows ): sum_ws = sum ([ wind . T for wind in self . windows [: i ]]) sample_index = [ si + sum_ws for si in mw . opt_samples_index ] _sample_index_list += sample_index return _sample_index_list moving_window_analysis () Compute dataframe entropy on moving windows Source code in fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def moving_window_analysis ( self ): \"\"\" Compute dataframe entropy on moving windows \"\"\" de = self . create_rounded_copy () for ll , rl in self . segments : # Create moving window mw_df = de . values [ ll : rl ] # Build a Moving Window mw = MovingWindow ( mw_df ) # Compute entropy mw . get_entropy () # Compute optimal number of samples mw . optimal_sampling ( self . threshold ) # Collect result in a list self . windows . append ( mw ) # Entropy normalization self . __normalization () WSDynamic Bases: SubsamplingMethod , EntropyBasedMethod Subsampling method with dynamic window size based on entropy analysis Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class WSDynamic ( SubsamplingMethod , EntropyBasedMethod ): \"\"\" Subsampling method with dynamic window size based on entropy analysis \"\"\" def __init__ ( self , window_min_size , entropy_threshold ): \"\"\" WSDynamic class constructor Args: window_min_size (int): minimun window size entropy_threshold (float): entropy threshold Raises: ValueError: if window_min_size == None \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSDynamic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) if window_min_size is None : raise ValueError ( \"window_type = DYNAMIC but window_min_size not specified\" ) self . wms = window_min_size self . ws = None def dataset_segmentation ( self ): \"\"\" Segments dataset based on breakpoint analysis and a min window size \"\"\" de = self . create_rounded_copy () algo = rpt . Pelt ( model = \"l2\" , min_size = self . wms ) . fit ( de ) seg_res = algo . predict ( pen = 10 ) self . segments = [( seg_res [ i - 1 ], seg_res [ i ]) for i in range ( 1 , len ( seg_res ))] self . segments . insert ( 0 , ( 0 , seg_res [ 0 ])) def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs __init__ ( window_min_size , entropy_threshold ) WSDynamic class constructor Parameters: Name Type Description Default window_min_size int minimun window size required entropy_threshold float entropy threshold required Raises: Type Description ValueError if window_min_size == None Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , window_min_size , entropy_threshold ): \"\"\" WSDynamic class constructor Args: window_min_size (int): minimun window size entropy_threshold (float): entropy threshold Raises: ValueError: if window_min_size == None \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSDynamic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) if window_min_size is None : raise ValueError ( \"window_type = DYNAMIC but window_min_size not specified\" ) self . wms = window_min_size self . ws = None dataset_segmentation () Segments dataset based on breakpoint analysis and a min window size Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 28 29 30 31 32 33 34 35 36 def dataset_segmentation ( self ): \"\"\" Segments dataset based on breakpoint analysis and a min window size \"\"\" de = self . create_rounded_copy () algo = rpt . Pelt ( model = \"l2\" , min_size = self . wms ) . fit ( de ) seg_res = algo . predict ( pen = 10 ) self . segments = [( seg_res [ i - 1 ], seg_res [ i ]) for i in range ( 1 , len ( seg_res ))] self . segments . insert ( 0 , ( 0 , seg_res [ 0 ])) run () Run subsampler Returns: Type Description list [ int ] indexes of the remaining samples Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs WSDynamic Bases: SubsamplingMethod , EntropyBasedMethod Subsampling method with dynamic window size based on entropy analysis Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class WSDynamic ( SubsamplingMethod , EntropyBasedMethod ): \"\"\" Subsampling method with dynamic window size based on entropy analysis \"\"\" def __init__ ( self , window_min_size , entropy_threshold ): \"\"\" WSDynamic class constructor Args: window_min_size (int): minimun window size entropy_threshold (float): entropy threshold Raises: ValueError: if window_min_size == None \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSDynamic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) if window_min_size is None : raise ValueError ( \"window_type = DYNAMIC but window_min_size not specified\" ) self . wms = window_min_size self . ws = None def dataset_segmentation ( self ): \"\"\" Segments dataset based on breakpoint analysis and a min window size \"\"\" de = self . create_rounded_copy () algo = rpt . Pelt ( model = \"l2\" , min_size = self . wms ) . fit ( de ) seg_res = algo . predict ( pen = 10 ) self . segments = [( seg_res [ i - 1 ], seg_res [ i ]) for i in range ( 1 , len ( seg_res ))] self . segments . insert ( 0 , ( 0 , seg_res [ 0 ])) def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs __init__ ( window_min_size , entropy_threshold ) WSDynamic class constructor Parameters: Name Type Description Default window_min_size int minimun window size required entropy_threshold float entropy threshold required Raises: Type Description ValueError if window_min_size == None Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , window_min_size , entropy_threshold ): \"\"\" WSDynamic class constructor Args: window_min_size (int): minimun window size entropy_threshold (float): entropy threshold Raises: ValueError: if window_min_size == None \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSDynamic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) if window_min_size is None : raise ValueError ( \"window_type = DYNAMIC but window_min_size not specified\" ) self . wms = window_min_size self . ws = None dataset_segmentation () Segments dataset based on breakpoint analysis and a min window size Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 28 29 30 31 32 33 34 35 36 def dataset_segmentation ( self ): \"\"\" Segments dataset based on breakpoint analysis and a min window size \"\"\" de = self . create_rounded_copy () algo = rpt . Pelt ( model = \"l2\" , min_size = self . wms ) . fit ( de ) seg_res = algo . predict ( pen = 10 ) self . segments = [( seg_res [ i - 1 ], seg_res [ i ]) for i in range ( 1 , len ( seg_res ))] self . segments . insert ( 0 , ( 0 , seg_res [ 0 ])) run () Run subsampler Returns: Type Description list [ int ] indexes of the remaining samples Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs Static Bases: SubsamplingMethod Subsamples data by taking one sample each step-samples Source code in fpcmci/preprocessing/subsampling_methods/Static.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Static ( SubsamplingMethod ): \"\"\" Subsamples data by taking one sample each step-samples \"\"\" def __init__ ( self , step ): \"\"\" Static class constructor Args: step (int): integer subsampling step Raises: ValueError: if step == None \"\"\" super () . __init__ ( SSMode . Static ) if step is None : raise ValueError ( \"step not specified\" ) self . step = step def run ( self ): return range ( 0 , len ( self . df . values ), self . step ) __init__ ( step ) Static class constructor Parameters: Name Type Description Default step int integer subsampling step required Raises: Type Description ValueError if step == None Source code in fpcmci/preprocessing/subsampling_methods/Static.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , step ): \"\"\" Static class constructor Args: step (int): integer subsampling step Raises: ValueError: if step == None \"\"\" super () . __init__ ( SSMode . Static ) if step is None : raise ValueError ( \"step not specified\" ) self . step = step WSFFTStatic Bases: SubsamplingMethod , EntropyBasedMethod Subsampling method with static window size based on Fourier analysis Source code in fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class WSFFTStatic ( SubsamplingMethod , EntropyBasedMethod ): \"\"\" Subsampling method with static window size based on Fourier analysis \"\"\" def __init__ ( self , sampling_time , entropy_threshold ): \"\"\" WSFFTStatic class constructor Args: sampling_time (float): timeseries sampling time entropy_threshold (float): entropy threshold \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSFFTStatic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) self . sampling_time = sampling_time def __fourier_window ( self ): \"\"\" Compute window size based on Fourier analysis performed on dataframe Returns: (int): window size \"\"\" N , dim = self . df . shape xf = rfftfreq ( N , self . sampling_time ) w_array = list () for i in range ( 0 , dim ): yf = np . abs ( rfft ( self . df . values [:, i ])) peak_indices , _ = scipy . signal . find_peaks ( yf ) highest_peak_index = peak_indices [ np . argmax ( yf [ peak_indices ])] w_array . append ( ceil ( 1 / ( 2 * xf [ highest_peak_index ]) / self . sampling_time )) fig , ax = pl . subplots () ax . plot ( xf , yf ) ax . plot ( xf [ highest_peak_index ], np . abs ( yf [ highest_peak_index ]), \"x\" ) pl . show () return min ( w_array ) def dataset_segmentation ( self ): \"\"\" Segments dataset with a fixed window size \"\"\" seg_res = [ i for i in range ( 0 , len ( self . df . values ), self . ws )] self . segments = [( i , i + self . ws ) for i in range ( 0 , len ( self . df . values ) - self . ws , self . ws )] if not seg_res . __contains__ ( len ( self . df . values )): self . segments . append (( seg_res [ - 1 ], len ( self . df . values ))) seg_res . append ( len ( self . df . values )) def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # define window size self . ws = self . __fourier_window () # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs __fourier_window () Compute window size based on Fourier analysis performed on dataframe Returns: Type Description int window size Source code in fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __fourier_window ( self ): \"\"\" Compute window size based on Fourier analysis performed on dataframe Returns: (int): window size \"\"\" N , dim = self . df . shape xf = rfftfreq ( N , self . sampling_time ) w_array = list () for i in range ( 0 , dim ): yf = np . abs ( rfft ( self . df . values [:, i ])) peak_indices , _ = scipy . signal . find_peaks ( yf ) highest_peak_index = peak_indices [ np . argmax ( yf [ peak_indices ])] w_array . append ( ceil ( 1 / ( 2 * xf [ highest_peak_index ]) / self . sampling_time )) fig , ax = pl . subplots () ax . plot ( xf , yf ) ax . plot ( xf [ highest_peak_index ], np . abs ( yf [ highest_peak_index ]), \"x\" ) pl . show () return min ( w_array ) __init__ ( sampling_time , entropy_threshold ) WSFFTStatic class constructor Parameters: Name Type Description Default sampling_time float timeseries sampling time required entropy_threshold float entropy threshold required Source code in fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py 14 15 16 17 18 19 20 21 22 23 24 def __init__ ( self , sampling_time , entropy_threshold ): \"\"\" WSFFTStatic class constructor Args: sampling_time (float): timeseries sampling time entropy_threshold (float): entropy threshold \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSFFTStatic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) self . sampling_time = sampling_time dataset_segmentation () Segments dataset with a fixed window size Source code in fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py 50 51 52 53 54 55 56 57 58 def dataset_segmentation ( self ): \"\"\" Segments dataset with a fixed window size \"\"\" seg_res = [ i for i in range ( 0 , len ( self . df . values ), self . ws )] self . segments = [( i , i + self . ws ) for i in range ( 0 , len ( self . df . values ) - self . ws , self . ws )] if not seg_res . __contains__ ( len ( self . df . values )): self . segments . append (( seg_res [ - 1 ], len ( self . df . values ))) seg_res . append ( len ( self . df . values )) run () Run subsampler Returns: Type Description list [ int ] indexes of the remaining samples Source code in fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # define window size self . ws = self . __fourier_window () # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs WSStatic Bases: SubsamplingMethod , EntropyBasedMethod Entropy based subsampling method with static window size Source code in fpcmci/preprocessing/subsampling_methods/WSStatic.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class WSStatic ( SubsamplingMethod , EntropyBasedMethod ): \"\"\" Entropy based subsampling method with static window size \"\"\" def __init__ ( self , window_size , entropy_threshold ): \"\"\" WSStatic class constructor Args: window_size (int): minimun window size entropy_threshold (float): entropy threshold Raises: ValueError: if window_size == None \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSDynamic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) if window_size is None : raise ValueError ( \"window_type = STATIC but window_size not specified\" ) self . ws = window_size def dataset_segmentation ( self ): \"\"\" Segments dataset with a fixed window size \"\"\" seg_res = [ i for i in range ( 0 , len ( self . df . values ), self . ws )] self . segments = [( i , i + self . ws ) for i in range ( 0 , len ( self . df . values ) - self . ws , self . ws )] if not seg_res . __contains__ ( len ( self . df . values )): self . segments . append (( seg_res [ - 1 ], len ( self . df . values ))) seg_res . append ( len ( self . df . values )) def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs __init__ ( window_size , entropy_threshold ) WSStatic class constructor Parameters: Name Type Description Default window_size int minimun window size required entropy_threshold float entropy threshold required Raises: Type Description ValueError if window_size == None Source code in fpcmci/preprocessing/subsampling_methods/WSStatic.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def __init__ ( self , window_size , entropy_threshold ): \"\"\" WSStatic class constructor Args: window_size (int): minimun window size entropy_threshold (float): entropy threshold Raises: ValueError: if window_size == None \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSDynamic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) if window_size is None : raise ValueError ( \"window_type = STATIC but window_size not specified\" ) self . ws = window_size dataset_segmentation () Segments dataset with a fixed window size Source code in fpcmci/preprocessing/subsampling_methods/WSStatic.py 28 29 30 31 32 33 34 35 36 def dataset_segmentation ( self ): \"\"\" Segments dataset with a fixed window size \"\"\" seg_res = [ i for i in range ( 0 , len ( self . df . values ), self . ws )] self . segments = [( i , i + self . ws ) for i in range ( 0 , len ( self . df . values ) - self . ws , self . ws )] if not seg_res . __contains__ ( len ( self . df . values )): self . segments . append (( seg_res [ - 1 ], len ( self . df . values ))) seg_res . append ( len ( self . df . values )) run () Run subsampler Returns: Type Description list [ int ] indexes of the remaining samples Source code in fpcmci/preprocessing/subsampling_methods/WSStatic.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs","title":"Subsampling"},{"location":"subsampling_method/#fpcmci.preprocessing.Subsampler.Subsampler","text":"Subsampler class. It subsamples the data by using a subsampling method chosen among Static - subsamples data by taking one sample each step-samples WSDynamic - entropy based method with dynamic window size computed by breakpoint analysis WSFFTStatic - entropy based method with fixed window size computed by FFT analysis WSStatic - entropy base method with predefined window size Source code in fpcmci/preprocessing/Subsampler.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 class Subsampler (): \"\"\" Subsampler class. It subsamples the data by using a subsampling method chosen among: - Static - subsamples data by taking one sample each step-samples - WSDynamic - entropy based method with dynamic window size computed by breakpoint analysis - WSFFTStatic - entropy based method with fixed window size computed by FFT analysis - WSStatic - entropy base method with predefined window size \"\"\" def __init__ ( self , df : pd . DataFrame , ss_method : SubsamplingMethod ): \"\"\" Subsampler class constructor Args: df (pd.DataFrame): dataframe to subsample ss_method (SubsamplingMethod): subsampling method \"\"\" self . df = df self . ss_method = ss_method self . ss_method . initialise ( df ) def subsample ( self ): \"\"\" Runs the subsampling algorithm and returns the subsapled ndarray Returns: (ndarray): Subsampled dataframe value \"\"\" self . result = self . ss_method . run () return self . df . values [ self . result , :] def plot_subsampled_data ( self , dpi = 100 , show = True ): \"\"\" Plot dataframe sub-sampled data Args: dpi (int, optional): image dpi. Defaults to 100. show (bool, optional): if True it shows the figure and block the process. Defaults to True. \"\"\" n_plot = self . df . shape [ 1 ] # Create grid gs = gridspec . GridSpec ( n_plot , 1 ) # Time vector T = list ( range ( 0 , self . df . shape [ 0 ])) pl . figure ( dpi = dpi ) for i in range ( 0 , n_plot ): ax = pl . subplot ( gs [ i , 0 ]) pl . plot ( T , self . df . values [:, i ], color = 'tab:red' ) pl . scatter ( np . array ( T )[ self . result ], self . df . values [ self . result , i ], s = 80 , facecolors = 'none' , edgecolors = 'b' ) pl . gca () . set ( ylabel = r '$' + str ( self . df . columns . values [ i ]) + '$' ) if show : pl . show ()","title":"Subsampler"},{"location":"subsampling_method/#fpcmci.preprocessing.Subsampler.Subsampler.__init__","text":"Subsampler class constructor Parameters: Name Type Description Default df pd . DataFrame dataframe to subsample required ss_method SubsamplingMethod subsampling method required Source code in fpcmci/preprocessing/Subsampler.py 18 19 20 21 22 23 24 25 26 27 28 29 30 def __init__ ( self , df : pd . DataFrame , ss_method : SubsamplingMethod ): \"\"\" Subsampler class constructor Args: df (pd.DataFrame): dataframe to subsample ss_method (SubsamplingMethod): subsampling method \"\"\" self . df = df self . ss_method = ss_method self . ss_method . initialise ( df )","title":"__init__()"},{"location":"subsampling_method/#fpcmci.preprocessing.Subsampler.Subsampler.plot_subsampled_data","text":"Plot dataframe sub-sampled data Parameters: Name Type Description Default dpi int image dpi. Defaults to 100. 100 show bool if True it shows the figure and block the process. Defaults to True. True Source code in fpcmci/preprocessing/Subsampler.py 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def plot_subsampled_data ( self , dpi = 100 , show = True ): \"\"\" Plot dataframe sub-sampled data Args: dpi (int, optional): image dpi. Defaults to 100. show (bool, optional): if True it shows the figure and block the process. Defaults to True. \"\"\" n_plot = self . df . shape [ 1 ] # Create grid gs = gridspec . GridSpec ( n_plot , 1 ) # Time vector T = list ( range ( 0 , self . df . shape [ 0 ])) pl . figure ( dpi = dpi ) for i in range ( 0 , n_plot ): ax = pl . subplot ( gs [ i , 0 ]) pl . plot ( T , self . df . values [:, i ], color = 'tab:red' ) pl . scatter ( np . array ( T )[ self . result ], self . df . values [ self . result , i ], s = 80 , facecolors = 'none' , edgecolors = 'b' ) pl . gca () . set ( ylabel = r '$' + str ( self . df . columns . values [ i ]) + '$' ) if show : pl . show ()","title":"plot_subsampled_data()"},{"location":"subsampling_method/#fpcmci.preprocessing.Subsampler.Subsampler.subsample","text":"Runs the subsampling algorithm and returns the subsapled ndarray Returns: Type Description ndarray Subsampled dataframe value Source code in fpcmci/preprocessing/Subsampler.py 33 34 35 36 37 38 39 40 41 def subsample ( self ): \"\"\" Runs the subsampling algorithm and returns the subsapled ndarray Returns: (ndarray): Subsampled dataframe value \"\"\" self . result = self . ss_method . run () return self . df . values [ self . result , :]","title":"subsample()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.SubsamplingMethod.SubsamplingMethod","text":"Bases: ABC SubsamplingMethod abstract class Source code in fpcmci/preprocessing/subsampling_methods/SubsamplingMethod.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class SubsamplingMethod ( ABC ): \"\"\" SubsamplingMethod abstract class \"\"\" def __init__ ( self , ssmode : SSMode ): self . ssmode = ssmode self . df = None def initialise ( self , dataframe : pd . DataFrame ): \"\"\" Initialise class by setting the dataframe to subsample Args: dataframe (pd.DataFrame): _description_ \"\"\" self . df = dataframe @abstractmethod def run ( self ): \"\"\" Run subsampler \"\"\" pass","title":"SubsamplingMethod"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.SubsamplingMethod.SubsamplingMethod.initialise","text":"Initialise class by setting the dataframe to subsample Parameters: Name Type Description Default dataframe pd . DataFrame description required Source code in fpcmci/preprocessing/subsampling_methods/SubsamplingMethod.py 22 23 24 25 26 27 28 29 def initialise ( self , dataframe : pd . DataFrame ): \"\"\" Initialise class by setting the dataframe to subsample Args: dataframe (pd.DataFrame): _description_ \"\"\" self . df = dataframe","title":"initialise()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.SubsamplingMethod.SubsamplingMethod.run","text":"Run subsampler Source code in fpcmci/preprocessing/subsampling_methods/SubsamplingMethod.py 32 33 34 35 36 37 @abstractmethod def run ( self ): \"\"\" Run subsampler \"\"\" pass","title":"run()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod","text":"Bases: ABC EntropyBasedMethod abstract class Source code in fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class EntropyBasedMethod ( ABC ): \"\"\" EntropyBasedMethod abstract class \"\"\" def __init__ ( self , threshold ): self . windows = list () self . segments = list () self . threshold = threshold def create_rounded_copy ( self ): \"\"\" Create deepcopy of the dataframe but with rounded values Returns: (pd.DataFrame): rounded dataframe \"\"\" de = deepcopy ( self . df ) de = de . round ( 1 ) return de def __normalization ( self ): \"\"\" Normalize entropy for each moving window \"\"\" max_e = max ([ mw . entropy for mw in self . windows ]) for mw in self . windows : mw . entropy = mw . entropy / max_e def moving_window_analysis ( self ): \"\"\" Compute dataframe entropy on moving windows \"\"\" de = self . create_rounded_copy () for ll , rl in self . segments : # Create moving window mw_df = de . values [ ll : rl ] # Build a Moving Window mw = MovingWindow ( mw_df ) # Compute entropy mw . get_entropy () # Compute optimal number of samples mw . optimal_sampling ( self . threshold ) # Collect result in a list self . windows . append ( mw ) # Entropy normalization self . __normalization () # def extract_data(self): # \"\"\" # Extract plottable data from moving window analysis # \"\"\" # # Entropies and samples numbers list # self.__entropy_list = [mw.entropy for mw in self.__window_list] # self.__sample_number_list = [mw.opt_size for mw in self.__window_list] # self.__original_size = [mw.T for mw in self.__window_list] # self.num_samples = sum(self.__sample_number_list) # # Make entropy and sample array plottable # self.__pretty_signals() # def __pretty_signals(self): # \"\"\" # Make entropy list and sample number list plottable # \"\"\" # _pretty_entropy = [] # _pretty_sample_number = [] # _pretty_original_size = [] # for i, mw in enumerate(self.__window_list): # _pretty_entropy += np.repeat(self.__entropy_list[i], mw.T).tolist() # _pretty_sample_number += np.repeat(self.__sample_number_list[i], mw.T).tolist() # _pretty_original_size += np.repeat(self.__original_size[i], mw.T).tolist() # self.__entropy_list = _pretty_entropy # self.__sample_number_list = _pretty_sample_number # self.__original_size = _pretty_original_size # _diff = self.df.shape[0] - len(self.__entropy_list) # if _diff != 0: # self.__entropy_list = np.append(self.__entropy_list, [self.__entropy_list[-1]] * _diff) # self.__sample_number_list = np.append(self.__sample_number_list, [self.__sample_number_list[-1]] * _diff) def extract_indexes ( self ): \"\"\" Extract a list of indexes corresponding to the samples selected by the subsampling procedure \"\"\" _sample_index_list = list () for i , mw in enumerate ( self . windows ): sum_ws = sum ([ wind . T for wind in self . windows [: i ]]) sample_index = [ si + sum_ws for si in mw . opt_samples_index ] _sample_index_list += sample_index return _sample_index_list @abstractmethod def dataset_segmentation ( self ): \"\"\" abstract method \"\"\" pass","title":"EntropyBasedMethod"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.__normalization","text":"Normalize entropy for each moving window Source code in fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py 28 29 30 31 32 33 34 def __normalization ( self ): \"\"\" Normalize entropy for each moving window \"\"\" max_e = max ([ mw . entropy for mw in self . windows ]) for mw in self . windows : mw . entropy = mw . entropy / max_e","title":"__normalization()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.create_rounded_copy","text":"Create deepcopy of the dataframe but with rounded values Returns: Type Description pd . DataFrame rounded dataframe Source code in fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py 16 17 18 19 20 21 22 23 24 25 def create_rounded_copy ( self ): \"\"\" Create deepcopy of the dataframe but with rounded values Returns: (pd.DataFrame): rounded dataframe \"\"\" de = deepcopy ( self . df ) de = de . round ( 1 ) return de","title":"create_rounded_copy()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.dataset_segmentation","text":"abstract method Source code in fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py 111 112 113 114 115 116 @abstractmethod def dataset_segmentation ( self ): \"\"\" abstract method \"\"\" pass","title":"dataset_segmentation()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.extract_indexes","text":"Extract a list of indexes corresponding to the samples selected by the subsampling procedure Source code in fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py 98 99 100 101 102 103 104 105 106 107 108 def extract_indexes ( self ): \"\"\" Extract a list of indexes corresponding to the samples selected by the subsampling procedure \"\"\" _sample_index_list = list () for i , mw in enumerate ( self . windows ): sum_ws = sum ([ wind . T for wind in self . windows [: i ]]) sample_index = [ si + sum_ws for si in mw . opt_samples_index ] _sample_index_list += sample_index return _sample_index_list","title":"extract_indexes()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.EntropyBasedMethod.EntropyBasedMethod.moving_window_analysis","text":"Compute dataframe entropy on moving windows Source code in fpcmci/preprocessing/subsampling_methods/EntropyBasedMethod.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def moving_window_analysis ( self ): \"\"\" Compute dataframe entropy on moving windows \"\"\" de = self . create_rounded_copy () for ll , rl in self . segments : # Create moving window mw_df = de . values [ ll : rl ] # Build a Moving Window mw = MovingWindow ( mw_df ) # Compute entropy mw . get_entropy () # Compute optimal number of samples mw . optimal_sampling ( self . threshold ) # Collect result in a list self . windows . append ( mw ) # Entropy normalization self . __normalization ()","title":"moving_window_analysis()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic","text":"Bases: SubsamplingMethod , EntropyBasedMethod Subsampling method with dynamic window size based on entropy analysis Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class WSDynamic ( SubsamplingMethod , EntropyBasedMethod ): \"\"\" Subsampling method with dynamic window size based on entropy analysis \"\"\" def __init__ ( self , window_min_size , entropy_threshold ): \"\"\" WSDynamic class constructor Args: window_min_size (int): minimun window size entropy_threshold (float): entropy threshold Raises: ValueError: if window_min_size == None \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSDynamic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) if window_min_size is None : raise ValueError ( \"window_type = DYNAMIC but window_min_size not specified\" ) self . wms = window_min_size self . ws = None def dataset_segmentation ( self ): \"\"\" Segments dataset based on breakpoint analysis and a min window size \"\"\" de = self . create_rounded_copy () algo = rpt . Pelt ( model = \"l2\" , min_size = self . wms ) . fit ( de ) seg_res = algo . predict ( pen = 10 ) self . segments = [( seg_res [ i - 1 ], seg_res [ i ]) for i in range ( 1 , len ( seg_res ))] self . segments . insert ( 0 , ( 0 , seg_res [ 0 ])) def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs","title":"WSDynamic"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.__init__","text":"WSDynamic class constructor Parameters: Name Type Description Default window_min_size int minimun window size required entropy_threshold float entropy threshold required Raises: Type Description ValueError if window_min_size == None Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , window_min_size , entropy_threshold ): \"\"\" WSDynamic class constructor Args: window_min_size (int): minimun window size entropy_threshold (float): entropy threshold Raises: ValueError: if window_min_size == None \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSDynamic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) if window_min_size is None : raise ValueError ( \"window_type = DYNAMIC but window_min_size not specified\" ) self . wms = window_min_size self . ws = None","title":"__init__()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.dataset_segmentation","text":"Segments dataset based on breakpoint analysis and a min window size Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 28 29 30 31 32 33 34 35 36 def dataset_segmentation ( self ): \"\"\" Segments dataset based on breakpoint analysis and a min window size \"\"\" de = self . create_rounded_copy () algo = rpt . Pelt ( model = \"l2\" , min_size = self . wms ) . fit ( de ) seg_res = algo . predict ( pen = 10 ) self . segments = [( seg_res [ i - 1 ], seg_res [ i ]) for i in range ( 1 , len ( seg_res ))] self . segments . insert ( 0 , ( 0 , seg_res [ 0 ]))","title":"dataset_segmentation()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.run","text":"Run subsampler Returns: Type Description list [ int ] indexes of the remaining samples Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs","title":"run()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic","text":"Bases: SubsamplingMethod , EntropyBasedMethod Subsampling method with dynamic window size based on entropy analysis Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class WSDynamic ( SubsamplingMethod , EntropyBasedMethod ): \"\"\" Subsampling method with dynamic window size based on entropy analysis \"\"\" def __init__ ( self , window_min_size , entropy_threshold ): \"\"\" WSDynamic class constructor Args: window_min_size (int): minimun window size entropy_threshold (float): entropy threshold Raises: ValueError: if window_min_size == None \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSDynamic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) if window_min_size is None : raise ValueError ( \"window_type = DYNAMIC but window_min_size not specified\" ) self . wms = window_min_size self . ws = None def dataset_segmentation ( self ): \"\"\" Segments dataset based on breakpoint analysis and a min window size \"\"\" de = self . create_rounded_copy () algo = rpt . Pelt ( model = \"l2\" , min_size = self . wms ) . fit ( de ) seg_res = algo . predict ( pen = 10 ) self . segments = [( seg_res [ i - 1 ], seg_res [ i ]) for i in range ( 1 , len ( seg_res ))] self . segments . insert ( 0 , ( 0 , seg_res [ 0 ])) def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs","title":"WSDynamic"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.__init__","text":"WSDynamic class constructor Parameters: Name Type Description Default window_min_size int minimun window size required entropy_threshold float entropy threshold required Raises: Type Description ValueError if window_min_size == None Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def __init__ ( self , window_min_size , entropy_threshold ): \"\"\" WSDynamic class constructor Args: window_min_size (int): minimun window size entropy_threshold (float): entropy threshold Raises: ValueError: if window_min_size == None \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSDynamic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) if window_min_size is None : raise ValueError ( \"window_type = DYNAMIC but window_min_size not specified\" ) self . wms = window_min_size self . ws = None","title":"__init__()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.dataset_segmentation","text":"Segments dataset based on breakpoint analysis and a min window size Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 28 29 30 31 32 33 34 35 36 def dataset_segmentation ( self ): \"\"\" Segments dataset based on breakpoint analysis and a min window size \"\"\" de = self . create_rounded_copy () algo = rpt . Pelt ( model = \"l2\" , min_size = self . wms ) . fit ( de ) seg_res = algo . predict ( pen = 10 ) self . segments = [( seg_res [ i - 1 ], seg_res [ i ]) for i in range ( 1 , len ( seg_res ))] self . segments . insert ( 0 , ( 0 , seg_res [ 0 ]))","title":"dataset_segmentation()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSDynamic.WSDynamic.run","text":"Run subsampler Returns: Type Description list [ int ] indexes of the remaining samples Source code in fpcmci/preprocessing/subsampling_methods/WSDynamic.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs","title":"run()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.Static.Static","text":"Bases: SubsamplingMethod Subsamples data by taking one sample each step-samples Source code in fpcmci/preprocessing/subsampling_methods/Static.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 class Static ( SubsamplingMethod ): \"\"\" Subsamples data by taking one sample each step-samples \"\"\" def __init__ ( self , step ): \"\"\" Static class constructor Args: step (int): integer subsampling step Raises: ValueError: if step == None \"\"\" super () . __init__ ( SSMode . Static ) if step is None : raise ValueError ( \"step not specified\" ) self . step = step def run ( self ): return range ( 0 , len ( self . df . values ), self . step )","title":"Static"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.Static.Static.__init__","text":"Static class constructor Parameters: Name Type Description Default step int integer subsampling step required Raises: Type Description ValueError if step == None Source code in fpcmci/preprocessing/subsampling_methods/Static.py 8 9 10 11 12 13 14 15 16 17 18 19 20 21 def __init__ ( self , step ): \"\"\" Static class constructor Args: step (int): integer subsampling step Raises: ValueError: if step == None \"\"\" super () . __init__ ( SSMode . Static ) if step is None : raise ValueError ( \"step not specified\" ) self . step = step","title":"__init__()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic","text":"Bases: SubsamplingMethod , EntropyBasedMethod Subsampling method with static window size based on Fourier analysis Source code in fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 class WSFFTStatic ( SubsamplingMethod , EntropyBasedMethod ): \"\"\" Subsampling method with static window size based on Fourier analysis \"\"\" def __init__ ( self , sampling_time , entropy_threshold ): \"\"\" WSFFTStatic class constructor Args: sampling_time (float): timeseries sampling time entropy_threshold (float): entropy threshold \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSFFTStatic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) self . sampling_time = sampling_time def __fourier_window ( self ): \"\"\" Compute window size based on Fourier analysis performed on dataframe Returns: (int): window size \"\"\" N , dim = self . df . shape xf = rfftfreq ( N , self . sampling_time ) w_array = list () for i in range ( 0 , dim ): yf = np . abs ( rfft ( self . df . values [:, i ])) peak_indices , _ = scipy . signal . find_peaks ( yf ) highest_peak_index = peak_indices [ np . argmax ( yf [ peak_indices ])] w_array . append ( ceil ( 1 / ( 2 * xf [ highest_peak_index ]) / self . sampling_time )) fig , ax = pl . subplots () ax . plot ( xf , yf ) ax . plot ( xf [ highest_peak_index ], np . abs ( yf [ highest_peak_index ]), \"x\" ) pl . show () return min ( w_array ) def dataset_segmentation ( self ): \"\"\" Segments dataset with a fixed window size \"\"\" seg_res = [ i for i in range ( 0 , len ( self . df . values ), self . ws )] self . segments = [( i , i + self . ws ) for i in range ( 0 , len ( self . df . values ) - self . ws , self . ws )] if not seg_res . __contains__ ( len ( self . df . values )): self . segments . append (( seg_res [ - 1 ], len ( self . df . values ))) seg_res . append ( len ( self . df . values )) def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # define window size self . ws = self . __fourier_window () # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs","title":"WSFFTStatic"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.__fourier_window","text":"Compute window size based on Fourier analysis performed on dataframe Returns: Type Description int window size Source code in fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def __fourier_window ( self ): \"\"\" Compute window size based on Fourier analysis performed on dataframe Returns: (int): window size \"\"\" N , dim = self . df . shape xf = rfftfreq ( N , self . sampling_time ) w_array = list () for i in range ( 0 , dim ): yf = np . abs ( rfft ( self . df . values [:, i ])) peak_indices , _ = scipy . signal . find_peaks ( yf ) highest_peak_index = peak_indices [ np . argmax ( yf [ peak_indices ])] w_array . append ( ceil ( 1 / ( 2 * xf [ highest_peak_index ]) / self . sampling_time )) fig , ax = pl . subplots () ax . plot ( xf , yf ) ax . plot ( xf [ highest_peak_index ], np . abs ( yf [ highest_peak_index ]), \"x\" ) pl . show () return min ( w_array )","title":"__fourier_window()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.__init__","text":"WSFFTStatic class constructor Parameters: Name Type Description Default sampling_time float timeseries sampling time required entropy_threshold float entropy threshold required Source code in fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py 14 15 16 17 18 19 20 21 22 23 24 def __init__ ( self , sampling_time , entropy_threshold ): \"\"\" WSFFTStatic class constructor Args: sampling_time (float): timeseries sampling time entropy_threshold (float): entropy threshold \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSFFTStatic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) self . sampling_time = sampling_time","title":"__init__()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.dataset_segmentation","text":"Segments dataset with a fixed window size Source code in fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py 50 51 52 53 54 55 56 57 58 def dataset_segmentation ( self ): \"\"\" Segments dataset with a fixed window size \"\"\" seg_res = [ i for i in range ( 0 , len ( self . df . values ), self . ws )] self . segments = [( i , i + self . ws ) for i in range ( 0 , len ( self . df . values ) - self . ws , self . ws )] if not seg_res . __contains__ ( len ( self . df . values )): self . segments . append (( seg_res [ - 1 ], len ( self . df . values ))) seg_res . append ( len ( self . df . values ))","title":"dataset_segmentation()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSFFTStatic.WSFFTStatic.run","text":"Run subsampler Returns: Type Description list [ int ] indexes of the remaining samples Source code in fpcmci/preprocessing/subsampling_methods/WSFFTStatic.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # define window size self . ws = self . __fourier_window () # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs","title":"run()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSStatic.WSStatic","text":"Bases: SubsamplingMethod , EntropyBasedMethod Entropy based subsampling method with static window size Source code in fpcmci/preprocessing/subsampling_methods/WSStatic.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 class WSStatic ( SubsamplingMethod , EntropyBasedMethod ): \"\"\" Entropy based subsampling method with static window size \"\"\" def __init__ ( self , window_size , entropy_threshold ): \"\"\" WSStatic class constructor Args: window_size (int): minimun window size entropy_threshold (float): entropy threshold Raises: ValueError: if window_size == None \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSDynamic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) if window_size is None : raise ValueError ( \"window_type = STATIC but window_size not specified\" ) self . ws = window_size def dataset_segmentation ( self ): \"\"\" Segments dataset with a fixed window size \"\"\" seg_res = [ i for i in range ( 0 , len ( self . df . values ), self . ws )] self . segments = [( i , i + self . ws ) for i in range ( 0 , len ( self . df . values ) - self . ws , self . ws )] if not seg_res . __contains__ ( len ( self . df . values )): self . segments . append (( seg_res [ - 1 ], len ( self . df . values ))) seg_res . append ( len ( self . df . values )) def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs","title":"WSStatic"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSStatic.WSStatic.__init__","text":"WSStatic class constructor Parameters: Name Type Description Default window_size int minimun window size required entropy_threshold float entropy threshold required Raises: Type Description ValueError if window_size == None Source code in fpcmci/preprocessing/subsampling_methods/WSStatic.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 def __init__ ( self , window_size , entropy_threshold ): \"\"\" WSStatic class constructor Args: window_size (int): minimun window size entropy_threshold (float): entropy threshold Raises: ValueError: if window_size == None \"\"\" SubsamplingMethod . __init__ ( self , SSMode . WSDynamic ) EntropyBasedMethod . __init__ ( self , entropy_threshold ) if window_size is None : raise ValueError ( \"window_type = STATIC but window_size not specified\" ) self . ws = window_size","title":"__init__()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSStatic.WSStatic.dataset_segmentation","text":"Segments dataset with a fixed window size Source code in fpcmci/preprocessing/subsampling_methods/WSStatic.py 28 29 30 31 32 33 34 35 36 def dataset_segmentation ( self ): \"\"\" Segments dataset with a fixed window size \"\"\" seg_res = [ i for i in range ( 0 , len ( self . df . values ), self . ws )] self . segments = [( i , i + self . ws ) for i in range ( 0 , len ( self . df . values ) - self . ws , self . ws )] if not seg_res . __contains__ ( len ( self . df . values )): self . segments . append (( seg_res [ - 1 ], len ( self . df . values ))) seg_res . append ( len ( self . df . values ))","title":"dataset_segmentation()"},{"location":"subsampling_method/#fpcmci.preprocessing.subsampling_methods.WSStatic.WSStatic.run","text":"Run subsampler Returns: Type Description list [ int ] indexes of the remaining samples Source code in fpcmci/preprocessing/subsampling_methods/WSStatic.py 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def run ( self ): \"\"\" Run subsampler Returns: (list[int]): indexes of the remaining samples \"\"\" # build list of segment self . dataset_segmentation () # compute entropy moving window self . moving_window_analysis () # extracting subsampling procedure results idxs = self . extract_indexes () return idxs","title":"run()"}]}